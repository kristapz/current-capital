{
    "Topic 6": [
        "Copyright 2001, 2005 by Princeton University Press Published by Princeton University Press, 41 William Street, Princeton, New Jersey 08540 In the United Kingdom: Princeton University Press, 3 Market Place, Woodstock, Oxfordshire OX20 1SY All Rights Reserved First Edition, 2001 Revised Edition, 2005 Library of Congress Cataloging in Publication Data Cochrane, John H. John Howland Asset pricing John H. Cochrane. Rev. ed. p. cm. Includes bibliographical references and index. ISBN 0 691 12137 0 cl : alk. paper 1. Capital assets pricing model. 2. Securities. I. Title. HG4636.C56 2005 332.6 dc22 2004050561 British Library Cataloging in Publication Data is available This book has been composed in New Baskerville Printed on acid free paper. pup.princeton.edu Printed in the United States of America 10 9 8 7 6 5 4 3 2 1",
        "Acknowledgments This book owes an enormous intellectual debt to Lars Hansen and Gene Fama. Most of the ideas in the book developed from long discussions with each of them, and from trying to make sense of what each was saying in the language of the other. I am also grateful to all my colleagues in Finance and Economics at the University of Chicago, and to George Constantinides especially, for many discussions about the ideas in this book. I thank all of the many people who have taken the time to give me comments, and espe cially George Constantinides, Andrea Eisfeldt, Gene Fama, Wayne Ferson, Michael Johannes, Owen Lamont, Anthony Lynch, Dan Nelson, Monika Piazzesi, Alberto Pozzolo, Michael Roberts, Juha Seppala, Mike Stutzer, Pietro Veronesi, an anonymous reviewer, and several generations of Ph.D. students at the University of Chicago. I am grateful to the many people who have pointed out typos and other mistakes in the first printing, and espe cially to Rodrigo Bueno, Samir Dutt, Tom Engsted, John van der Hoek, Karl Ludwig Keiber, Jonathan Lewellen, Claus Munk, Sergey Mityakov, Beat Naef, Alan Neal, and Denis Sokolov. Peter Dougherty was instrumental in shep herding this book to publication. I thank the National Science Foundation and the Graduate School of Business for research support. You will find typos, corrections, and additional materials, as they develop, on my website http: gsbwww.uchicago.edu fac john.cochrane research Papers v",
        "Acknowledgments Preface Part I. Contents v xiii Asset Pricing Theory 1 1 Consumption Based Model and Overview 3 1.1 BasicPricingEquation.............................. 4 1.2 Marginal Rate of Substitution Stochastic Discount Factor 6 1.3 Prices,Payoffs,andNotation......................... 8 1.4 ClassicIssuesinFinance ............................ 10 1.5 DiscountFactorsinContinuousTime ................. 25 Problems......................................... 31 2 Applying the Basic Model 35 2.1 AssumptionsandApplicability ....................... 35 2.2 GeneralEquilibrium ............................... 37 2.3 Consumption BasedModelinPractice ................ 41 2.4 AlternativeAssetPricingModels:Overview............. 43 Problems......................................... 45 3 Contingent Claims Markets 49 3.1 ContingentClaims ................................. 49 3.2 Risk NeutralProbabilities ........................... 51 3.3 InvestorsAgain.................................... 53 3.4 RiskSharing ...................................... 54 3.5 StateDiagramandPriceFunction .................... 56 4 The Discount Factor 61 4.1 Law of One Price and Existence of a Discount Factor . . . . 62 4.2 NoArbitrageandPositiveDiscountFactors ............ 67 vii",
        "Preface Asset pricing theory tries to understand the prices or values of claims to uncertain payments. A low price implies a high rate of return, so one can also think of the theory as explaining why some assets pay higher average returns than others. To value an asset, we have to account for the delay and for the risk of its payments. The effects of time are not too difficult to work out. However, corrections for risk are much more important determinants of many assets values. For example, over the last 50 years U.S. stocks have given a real return of about 9 on average. Of this, only about 1 is due to interest rates; the remaining 8 is a premium earned for holding risk. Uncertainty, or corrections for risk make asset pricing interesting and challenging. Asset pricing theory shares the positive versus normative tension present in the rest of economics. Does it describe the way the world does work, or the way the world should work? We observe the prices or returns of many assets. We can use the theory positively, to try to understand why prices or returns are what they are. If the world does not obey a model s predic tions, we can decide that the model needs improvement. However, we can also decide that the world is wrong, that some assets are mis priced and present trading opportunities for the shrewd investor. This latter use of asset pricing theory accounts for much of its popularity and practical application. Also, and perhaps most importantly, the prices of many assets or claims to uncertain cash flows are not observed, such as potential public or private investment projects, new financial securities, buyout prospects, and com plex derivatives. We can apply the theory to establish what the prices of these claims should be as well; the answers are important guides to public and private decisions. Asset pricing theory all stems from one simple concept, presented in the first page of the first chapter of this book: price equals expected discounted payoff. The rest is elaboration, special cases, and a closet full of tricks that make the central equation useful for one or another application. xiii",
        "xiv Preface There are two polar approaches to this elaboration. I call them absolute pricing and relative pricing. In absolute pricing, we price each asset by refer ence to its exposure to fundamental sources of macroeconomic risk. The consumption based and general equilibrium models are the purest exam ples of this approach. The absolute approach is most common in academic settings, in which we use asset pricing theory positively to give an economic explanation for why prices are what they are, or in order to predict how prices might change if policy or economic structure changed. In relative pricing, we ask a less ambitious question. We ask what we can learn about an asset s value given the prices of some other assets. We do not ask where the prices of the other assets came from, and we use as little information about fundamental risk factors as possible. Black Scholes option pricing is the classic example of this approach. While limited in scope, this approach offers precision in many applications. Asset pricing problems are solved by judiciously choosing how much absolute and how much relative pricing one will do, depending on the assets in question and the purpose of the calculation. Almost no problems are solved by the pure extremes. For example, the CAPM and its successor factor models are paradigms of the absolute approach. Yet in applications, they price assets relative to the market or other risk factors, without answer ing what determines the market or factor risk premia and betas. The latter are treated as free parameters. On the other end of the spectrum, even the most practical financial engineering questions usually involve assump tions beyond pure lack of arbitrage, assumptions about equilibrium market prices of risk. The central and unfinished task of absolute asset pricing is to under stand and measure the sources of aggregate or macroeconomic risk that drive asset prices. Of course, this is also the central question of macro economics, and this is a particularly exciting time for researchers who want to answer these fundamental questions in macroeconomics and finance. A lot of empirical work has documented tantalizing stylized facts and links between macroeconomics and finance. For example, expected returns vary across time and across assets in ways that are linked to macroeconomic variables, or variables that also forecast macroeconomic events; a wide class of models suggests that a recession or financial distress factor lies behind many asset prices. Yet theory lags behind; we do not yet have a well described model that explains these interesting correlations. In turn, I think that what we are learning about finance must feed back on macroeconomics. To take a simple example, we have learned that the risk premium on stocks the expected stock return less interest rates is much larger than the interest rate, and varies a good deal more than interest rates. This means that attempts to line investment up with interest rates are pretty hopeless most variation in the cost of capital comes from the",
        "xvi Preface They are not yet common in textbooks, and that is the niche that this book tries to fill. I also diverge from the usual order of presentation. Most books are structured following the history of thought: portfolio theory, mean variance frontiers, spanning theorems, CAPM, ICAPM, APT, option pricing, and finally consumption based model. Contingent claims are an esoteric extension of option pricing theory. I go the other way around: contingent claims and the consumption based model are the basic and simplest models around; the others are specializations. Just because they were discovered in the opposite order is no reason to present them that way. I also try to unify the treatment of empirical methods. A wide variety of methods are popular, including time series and cross sectional regres sions, and methods based on generalized method of moments GMM and maximum likelihood. However, in the end all of these apparently different approaches do the same thing: they pick free parameters of the model to make it fit best, which usually means to minimize pricing errors; and they evaluate the model by examining how big those pricing errors are. As with the theory, I do not attempt an encyclopedic compilation of empirical procedures. The literature on econometric methods contains lots of methods and special cases likelihood ratio analogues of common Wald tests; cases with and without risk free assets and when factors do and do not span the mean variance frontier, etc. that are seldom used in practice. I try to focus on the basic ideas and on methods that are actually used in practice. The accent in this book is on understanding statements of theory, and working with that theory to applications, rather than rigorous or general proofs. Also, I skip very lightly over many parts of asset pricing theory that have faded from current applications, although they occupied large amounts of the attention in the past. Some examples are portfolio separa tion theorems, properties of various distributions, or asymptotic APT. While portfolio theory is still interesting and useful, it is no longer a cornerstone of pricing. Rather than use portfolio theory to find a demand curve for assets, which intersected with a supply curve gives prices, we now go to prices directly. One can then find optimal portfolios, but it is a side issue for the asset pricing question. My presentation is consciously informal. I like to see an idea in its sim plest form and learn to use it before going back and understanding all the foundations of the ideas. I have organized the book for similarly minded readers. If you are hungry for more formal definitions and background, keep going, they usually show up later on. Again, my organizing principle is that everything can be traced back to specializations of the basic pricing equation p E mx . Therefore, after reading the first chapter, one can pretty much skip around and read topics",
        "Preface xvii in as much depth or order as one likes. Each major subject always starts back at the same pricing equation. The target audience for this book is economics and finance Ph.D. students, advanced MBA students, or professionals with similar background. I hope the book will also be useful to fellow researchers and finance pro fessionals, by clarifying, relating, and simplifying the set of tools we have all learned in a hodgepodge manner. I presume some exposure to undergrad uate economics and statistics. A reader should have seen a utility function, a random variable, a standard error, and a time series, should have some basic linear algebra and calculus, and should have solved a maximum problem by setting derivatives to zero. The hurdles in asset pricing are really conceptual rather than mathematical.",
        "4 1. Consumption Based Model and Overview positively with a large index such as the market portfolio. This is a Capital Asset Pricing Model. We will see a wide variety of additional indicators for marginal utility, things against which to compute a covariance in order to predict the risk adjustment for prices. 1.1 Basic Pricing Equation Our basic objective is to figure out the value of any stream of uncertain cash flows. I start with an apparently simple case, which turns out to capture very general situations. Let us find the value at time t of a payoff xt 1. If you buy a stock today, the payoff next period is the stock price plus dividend, xt 1 pt 1 dt 1. xt 1 is a random variable: an investor does not know exactly how much he will get from his investment, but he can assess the probability of various possible outcomes. Do not confuse the payoff xt 1 with the profit or return; xt 1 is the value of the investment at time t 1, without subtracting or dividing by the cost of the investment. We find the value of this payoff by asking what it is worth to a typical investor. To do this, we need a convenient mathematical formalism to cap ture what an investor wants. We model investors by a utility function defined over current and future values of consumption, U ct,ct 1 u ct Et u ct 1 , where ct denotes consumption at date t. We often use a convenient power utility form, u c 1 c1 . t 1 t Thelimitas 1is1 1 To think about this limit precisely, add a constant to the utility function and write it as model, u ct 1 u ct An investor s first order conditions give the basic consumption based pt Et  xt 1 . u c ln c . u ct t 1  . c1  1",
        "1.1. Basic Pricing Equation 5 The utility function captures the fundamental desire for more consumption, rather than posit a desire for intermediate objectives such as mean and variance of portfolio returns. Consumption ct 1 is also random; the investor does not know his wealth tomorrow, and hence how much he will decide to consume tomorrow. The period utility function u is increas ing, reflecting a desire for more consumption, and concave, reflecting the declining marginal value of additional consumption. The last bite is never as satisfying as the first. This formalism captures investors impatience and their aversion to risk, so we can quantitatively correct for the risk and delay of cash flows. Discount ing the future by  captures impatience, and  is called the subjective discount factor. The curvature of the utility function generates aversion to risk and to intertemporal substitution: The investor prefers a consumption stream that is steady over time and across states of nature. Now, assume that the investor can freely buy or sell as much of the payoff xt 1 as he wishes, at a price pt . How much will he buy or sell? To find the answer, denote by e the original consumption level if the investor bought none of the asset , and denote by  the amount of the asset he chooses to buy. Then, his problem is max u ct Et u ct 1 s.t.  ct et pt, ct 1 et 1 xt 1. Substituting the constraints into the objective, and setting the derivative with respect to  equal to zero, we obtain the first order condition for an optimal consumption and portfolio choice, or pt u ct Et u ct 1 xt 1 , u ct 1 u ct 1.1 1.2 pt Et  xt 1 . The investor buys more or less of the asset until this first order condition holds. Equation 1.1 expresses the standard marginal condition for an opti mum: p u c is the loss in utility if the investor buys another unit of the asset; tt Et u ct 1 xt 1 is the increase in discounted, expected utility he obtains from the extra payoff at t 1. The investor continues to buy or sell the asset until the marginal loss equals the marginal gain.",
        "6 1. Consumption Based Model and Overview Equation 1.2 is the central asset pricing formula. Given the payoff xt 1 and given the investor s consumption choice ct,ct 1, it tells you what market price pt to expect. Its economic content is simply the first order conditions for optimal consumption and portfolio formation. Most of the theory of asset pricing just consists of specializations and manipulations of this formula. We have stopped short of a complete solution to the model, i.e., an expression with exogenous items on the right hand side. We relate one endogenous variable, price, to two other endogenous variables, consump tion and payoffs. One can continue to solve this model and derive the optimal consumption choice ct , ct 1 in terms of more fundamental givens of the model. In the model I have sketched so far, those givens are the income sequence et , et 1 and a specification of the full set of assets that the investor may buy and sell. We will in fact study such fuller solutions below. However, for many purposes one can stop short of specifying possibly wrongly all this extra structure, and obtain very useful predictions about asset prices from 1.2 , even though consumption is an endogenous variable. 1.2 Marginal Rate of Substitution Stochastic Discount Factor A convenient way to break up the basic pricing equation 1.2 is to define the stochastic discount factor mt 1 1.3 1.4 We break up the basic consumption based pricing equation into p E mx , u ct where mt 1 is the stochastic discount factor. u ct 1 m  , mt 1  u ct 1 u ct . Then, the basic pricing formula 1.2 can simply be expressed as pt Et mt 1xt 1 . When it is not necessary to be explicit about time subscripts or the dif ference between conditional and unconditional expectation, I will suppress the subscripts and just write p E mx . The price always comes at t, the payoff at t 1, and the expectation is conditional on time t information.",
        "8 1. Consumption Based Model and Overview 1.3 Prices, Payoffs, and Notation Thepricept givesrightstoapayoffxt 1.Inpractice,thisnotationcoversa variety of cases, including the following: Stock Return Price dividend ratio Excess return Managed portfolio Moment condition One period bond Risk free rate Option Price pt pt 1 pt dt Payoff xt 1 pt 1 dt 1 Rt 1 pt 1 1 dt 1 0 zt E pt pt 1 C Re t 1 Ra t 1 t 1 dt Rb Rf max ST K,0 ztRt 1 zt xt 1zt 1 dt 1 The price pt and payoff xt 1 seem like a very restrictive kind of security. In fact, this notation is quite general and allows us easily to accommodate many different asset pricing questions. In particular, we can cover stocks, bonds, and options and make clear that there is one theory for all asset pricing. For stocks, the one period payoff is of course the next price plus divi dend, xt 1 pt 1 dt 1. We frequently divide the payoff xt 1 by the price pt toobtainagrossreturn Rt 1 xt 1 . pt We can think of a return as a payoff with price one. If you pay one dol lar today, the return is how many dollars or units of consumption you get tomorrow. Thus, returns obey 1 E mR , which is by far the most important special case of the basic formula p E mx . I use capital letters to denote gross returns R , which have a numer ical value like 1.05. I use lowercase letters to denote net returns r R 1 or log continuously compounded returns r ln R , both of which have numerical values like 0.05. One may also quote percent returns 100 r . Returns are often used in empirical work because they are typically stationary over time. Stationary in the statistical sense; they do not have",
        "1.3. Prices, Payoffs, and Notation 9 trends and you can meaningfully take an average. Stationary does not mean constant. However, thinking in terms of returns takes us away from the central task of finding asset prices. Dividing by dividends and creating a payoff of the form xt 1 1 pt 1 dt 1 dt 1 dt correspondingtoapricept dt isawaytolookatpricesbutstilltoexamine stationary variables. Not everything can be reduced to a return. If you borrow a dollar at the interest rate Rf and invest it in an asset with return R, you pay no money out of pocket today, and get the payoff R R f . This is a payoff with a zero price, so you obviously cannot divide payoff by price to get a return. Zero price does not imply zero payoff. It is a bet in which the value of the chance of losing exactly balances the value of the chance of winning, so that no money changes hands when the bet is made. It is common to study equity strategies in which one short sells one stock or portfolio and invests the proceeds in another stock or portfolio, generating an excess return. I denote any such difference between returns as an excess return, R e . It is also called a zero cost portfolio. In fact, much asset pricing focuses on excess returns. Our economic understanding of interest rate variation turns out to have little to do with our understanding of risk premia, so it is convenient to separate the two phenomena by looking at interest rates and excess returns separately. We also want to think about the managed portfolios, in which one invests more or less in an asset according to some signal. The price of such a strategy is the amount invested at time t , say zt , and the payoff is zt R t 1 . For example, a market timing strategy might make an investment in stocks pro portional to the price dividend ratio, investing less when prices are higher. Wecouldrepresentsuchastrategyasapayoffusingzt a b pt dt . When we think about conditioning information below, we will think of objects like zt as instruments. Then we take an unconditional expectation of ptzt Et mt 1xt 1 zt, yielding E ptzt E mt 1xt 1zt . We can think of this operation as creating a security with payoff xt 1 zt , and price E pt zt represented with unconditional expectations. A one period bond is of course a claim to a unit payoff. Bonds, options, investment projects are all examples in which it is often more useful to think of prices and payoffs rather than returns. Prices and returns can be real denominated in goods or nominal denominated in dollars ; p E mx can refer to either case. The only difference is whether we use a real or nominal discount factor. If prices, returns, and payoffs are nominal, we should use a nominal discount factor. For example, if p and x denote nominal values, then we can create real",
        "10 prices and payoffs to write pt 1. Consumption Based Model and Overview  u ct 1 xt 1 u ct t 1 Et where denotes the price level cpi . Obviously, this is the same as defining t a nominal discount factor by , pt Et u ct 1 t xt 1 . u ct t 1 To accommodate all these cases, I will simply use the notation price pt andpayoffxt 1.Thesesymbolscandenote0,1,orzt andRte,Rt 1,orztRt 1, respectively, according to the case. Lots of other definitions of p and x are useful as well. 1.4 Classic Issues in Finance A few simple rearrangements and manipulations of the basic pricing equation p E mx give a lot of intuition and introduce some classic issues in finance, including determinants of the interest rate, risk corrections, idiosyncratic versus systematic risk, beta pricing models, and mean variance frontiers. Risk Free Rate The risk free rate is related to the discount factor by Rf 1 E m . I use simple manipulations of the basic pricing equation to introduce classic issues in finance: the economics of interest rates, risk adjustments, systematic versus idiosyncratic risk, expected return beta representations, the mean variance frontier, the slope of the mean variance frontier, time varying expected returns, and present value relations. With lognormal consumption growth and power utility, 2 rf  E lnc 2 lnc . t tt 12tt 1",
        "14 1. Consumption Based Model and Overview wealth and consumption would otherwise be low you get a check when your house burns down. For this reason, you are happy to hold insurance, even though you expect to lose money even though the price of insurance is greater than its expected payoff discounted at the risk free rate. To emphasize why the covariance of a payoff with the discount factor rather than its variance determines its riskiness, keep in mind that the investor cares about the volatility of consumption. He does not care about the volatility of his individual assets or of his portfolio, if he can keep a steady consumption. Consider then what happens to the volatility of consumption if the investor buys a little more  of payoff x. 2 c becomes 2 c x 2 c 2 cov c,x 22 x . For small marginal portfolio changes, the covariance between consumption and payoff determines the effect of adding a bit more of each payoff on the volatility of consumption. We use returns so often that it is worth restating the same intuition for the special case that the price is 1 and the payoff is a return. Start with the basic pricing equation for returns, 1 E mRi . I denote the return Ri to emphasize that the point of the theory is to distinguishthebehaviorofoneassetRi fromanotherRj. The asset pricing model says that, although expected returns can vary across time and assets, expected discounted returns should always be the same, 1. Applying the covariance decomposition, and, using Rf or 1 E m E Ri cov m,Ri 1 E m , E Ri Rf Rf cov m,Ri i f cov u ct 1 ,Rti 1 E R R . 1.11 1.12 1.13 All assets have an expected return equal to the risk free rate, plus a risk adjustment. Assets whose returns covary positively with consumption make consumption more volatile, and so must promise higher expected returns to induce investors to hold them. Conversely, assets that covary negatively with consumption, such as insurance, can offer expected rates of return that are lower than the risk free rate, or even negative net expected returns. E u ct 1",
        "1.4. Classic Issues in Finance 15 Much of finance focuses on expected returns. We think of expected returns increasing or decreasing to clear markets; we offer intuition that riskier securities must offer higher expected returns to get investors to hold them, rather than saying riskier securities trade for lower prices so that investors will hold them. Of course, a low initial price for a given payoff corresponds to a high expected return, so this is no more than a different language for the same phenomenon. Idiosyncratic Risk Does Not Affect Prices You might think that an asset with a volatile payoff is risky and thus should have a large risk correction. However, if the payoff is uncorrelated with the discount factor m, the asset receives no risk correction to its price, and pays an expected return equal to the risk free rate! In equations, if Only the component of a payoff perfectly correlated with the discount factor generates an extra return. Idiosyncratic risk, uncorrelated with the discount factor, generates no premium. cov m,x 0, E x p , no matter how large 2 x . This prediction holds even if the payoff x is highly volatile and investors are highly risk averse. The reason is simple: if you buy a little bit more of such an asset, it has no first order effect on the variance of your consumption stream. More generally, one gets no compensation or risk adjustment for hold ing idiosyncratic risk. Only systematic risk generates a risk correction. To give meaning to these words, we can decompose any payoff x into a part corre lated with the discount factor and an idiosyncratic part uncorrelated with the discount factor by running a regression, x proj x m . Then, the price of the residual or idiosyncratic risk  is zero, and the price of x is the same as the price of its projection on m. The projection of x on m is of course that part of x which is perfectly correlated with m. The idiosyncratic component of any payoff is that part uncorrelated with m. Thus only the systematic part of a payoff accounts for its price. then Rf",
        "16 1. Consumption Based Model and Overview Projection means linear regression without a constant, E mx E m2 You can verify that regression residuals are orthogonal to right hand vari ables E m 0 from this definition. E m 0 of course means that the price of  is zero, p proj x m p E mx E m2 m E m2 E mx E m2 E mx p x . proj x m m. The words systematic and idiosyncratic are defined differently in different contexts, which can lead to some confusion. In this decomposi tion, the residuals  can be correlated with each other, though they are not correlated with the discount factor. The APT starts with a factor analytic decomposition of the covariance of payoffs, and the word idiosyncratic there is reserved for the component of payoffs uncorrelated with all of the other payoffs. Expected Return Beta Representation We can express the expected return equation 1.12 , for a return R i , as or We can write p E mx as E Ri Rf i,mm. i f cov Ri,m var m E m E R R E Ri Rf i,mm, var m where i,m is the regression coefficient of the return Ri on m. This is a beta pricing model. It says that each expected return should be proportional to the regression coefficient, or beta, in a regression of that return on the discount factor m. Notice that the coefficient m is the same for all assets i, while the i,m varies from asset to asset. The m is often interpreted as the price of risk and the  as the quantity of risk in each asset. As you can see, the price of risk m depends on the volatility of the discount factor. Obviously, there is nothing deep about saying that expected returns are proportional to betas rather than to covariances. There is a long historical 1.14 1.15",
        "1.4. Classic Issues in Finance 17 tradition and some minor convenience in favor of betas. The betas refer to the projection of R on m that we studied above, so you see again a sense in which only the systematic component of risk matters. With m  ct 1 ct  , we can take a Taylor approximation of equation 1.14 to express betas in terms of a more concrete variable, consumption growth, rather than marginal utility. The result, which I derive more explicitly and conveniently in the continuous time limit 1.38 below, is E Ri Rf i, c c, 1.16  c  var c . Expected returns should increase linearly with their betas on consumption growth itself. In addition, though it is treated as a free parameter in many applications,thefactorriskpremium c isdeterminedbyriskaversionand the volatility of consumption. The more risk averse people are, or the riskier their environment, the larger an expected return premium one must pay to get investors to hold risky high beta assets. Mean Variance Frontier Asset pricing theory has focused a lot on the means and variances of asset returns. Interestingly, the set of means and variances of returns is limited. All assets priced by the discount factor m must obey 1.17 1.18 All asset returns lie inside a mean variance frontier. Assets on the frontier are perfectly correlated with each other and with the discount factor. Returns on the frontier can be generated as portfolios of any two frontier returns. We can construct a discount factor from any frontier return except R f , and an expected return beta representation holds using any frontier return except Rf as the factor. E Ri Rf To derive 1.17 write for a given asset return R i and hence  m E m  Ri . 1 E mRi E m E Ri   m E Ri Rf  i  Ri . m,R i  Ri  m m,R E m",
        "24 1. Consumption Based Model and Overview excess returns have to be explained by changing risk t ct 1 or chang ing risk aversion  . It is not plausible that risk or risk aversion change at daily frequencies, but fortunately returns are not predictable at daily fre quencies. It is much more plausible that risk and risk aversion change over the business cycle, and this is exactly the horizon at which we see predictable excess returns. Models that make this connection precise are a very active area of current research. Present Value Statement It is convenient to use only the two period valuation, thinking of a price pt andapayoffxt 1.Buttherearetimeswhenwewanttorelateapricetothe entire cash flow stream, rather than just to one dividend and next period s price. The most straightforward way to do this is to write out a longer term objective, j 0 Now suppose an investor can purchase a stream dt j at price pt . As with the two period model, his first order condition gives us the pricing formula directly, pt Et j 1 mt,t jdt j. Et j u ct j . j u ct j u ct Et You can see that if this equation holds at time t and time t 1, then we can derive the two period version pt Et mt 1 pt 1 dt 1 . 1.24 Thus, the infinite period and two period models are equivalent. Going in the other direction is a little tougher. If you chain together 1.24 , you get 1.23 plus an extra term. To get 1.23 you also need the transversality condition limj Et mt,t jpt j 0. This is an extra first order condition of the infinite period investor, which is not present with overlapping generations of two period investors. It rules out pt Et  j 1 dt j mt,t jdt j. 1.23 j 1",
        "1.5. Discount Factors in Continuous Time 25 bubbles in which prices grow so fast that people will buy now just to resell at higher prices later, even if there are no dividends. From 1.23 we can write a risk adjustment to prices, as we did with one period payoffs, where R f t,t j pt E m Et dt j covt dt j , mt,t j , Rf j 1 t,t j j 1 1 is the j period interest rate. Again, assets whose dividend streams covary negatively with marginal utility, and posi tively with consumption, have lower prices, since holding those assets gives the investor a more volatile consumption stream. It is common instead to write prices as a discounted value using a risk adjusted discount factor, t t,t j e.g., pi E di Ri j, but this approach is difficult to use correctly t j 1 t t j for multiperiod problems, especially when expected returns can vary over time. At a deeper level, the expectation in the two period formula p E mx sums over states of nature. Equation 1.23 just sums over time as well and is mathematically identical. 1.5 Discount Factors in Continuous Time Continuous time versions of the basic pricing equations. Discrete Continuous pt Et j u ct j Dt j E R Rf Rfcov m,R e su ct s Dt s ds mt 1  p E mx u ct 1 u ct ptu ct Et t e tu ct 0 Ddt Et d p E dp Ddt rfdt E d dp tppttp j 1 u ct s 0 It is often convenient to express asset pricing ideas in the language of continuous time stochastic differential equations rather than discrete time stochastic difference equations as I have done so far. The appendix",
        "30 1. Consumption Based Model and Overview it is really the same trick . With t e t u ct we have d d t e tu c dt e tu c dc 1e tu c dc2, t t tt2tt c u c dc 1 c2u c dc2 t dt t t t t t t. 1.37 u ct ct 2 u ct ct2 Denote the local curvature and third derivative of the utility function as ct u ct t t . For power utility, the former is the power coefficient  and the latter is t   1 . Using this formula we can quickly redo the relationship between interest rates and consumption growth, equation 1.7 , u ct , ct2u ct u ct 1 d 1 dc 1 1 dc2 rf Et  Et Et. t dt t t dt t c 2 t dt t c2 ttt We can also easily express asset prices in terms of consumption risk rather than discount factor risk, as in equation 1.16 . Using 1.37 in 1.35 , E dpt Dt dt rfdt E dct dpt . 1.38 tppttcp tttt Thus, assets whose returns covary more strongly with consumption get higher mean excess returns, and the constant relating covariance to mean return is the utility curvature coefficient  . Since correlations are less than 1, equation 1.38 implies that Sharpe ratios are related to utility curvature and consumption volatility directly; we do not need the ugly lognormal facts and an approximation that we needed in 1.20 .Using E dp p ;2 E dp p 2 ;2 E dc c 2 , ptttptttcttt  Dt dt rfdt ppt t c.  p",
        "Problems 1. a 31 Problems Chapter 1 The absolute risk aversion coefficient is u c . We scale by u c because expected utility is only defined up to linear transformations a bu c gives the same predictions as u c and this measure of the second derivative is invariant to linear transformations. Show that the utility function with constant absolute risk aversion is u c e c. b The coefficient of relative risk aversion in a one period model i.e., when consumption equals wealth is defined as c The elasticity of intertemporal substitution is defined in a non stochastic model with interest rate R as dR R Show that with power utility u c c  , the intertemporal substitution elasticity is equal to 1  . Hint: differentiate the first order conditions 2. Show that decomposition of a return into systematic and idiosyncratic components in Figure 1.1 has the following properties: the systematic component has the same mean return as the original asset, and the idiosyncratic component has zero mean return the line is hori zontal; the systematic component is a return, while the idiosyncratic is an excess return; the two components are uncorrelated, and the systematic component is perfectly correlated with the discount factor m. Hint: start witharegressionRi a bm . 3. a Suppose you have a mean variance efficient return Rmv and the risk free rate. Using the fact that Rmv is perfectly correlated with the discount factor, construct a discount factor m in terms of Rf and Rmv, with no free parameters. You have to find the constants a and b in m a bRmv. They will depend on things like E Rmv . It will be easier to parameterize m E m b Rmv E Rmv . rra cu c u c u c For power utility u c equals the power,  . c 1  1  . , show that the risk aversion coefficient I c2 c1d c1 c2  .",
        "32 1. Consumption Based Model and Overview b Using this result, and the beta model in terms of m, show that expected returns can be described in a single beta representation using any mean variance efficient return except the risk free rate . E Ri Rf i,mv E Rmv Rf . By drawing a line between two assets inside the mean variance frontier, it seems you can get arbitrarily high Sharpe ratios. Can the Sharpe ratio between two risky assets exceed the slope of the mean variance frontier? Thatis,ifRmv isonthefrontier,isitpossiblethat 4. E Ri E Rj E Rmv Rf ?  Ri Rj  Rmv 5. Show that if consumption growth is lognormal, then E Rmv Rf  ct 1 ct   e22 lnct 1 1  lnct 1 . E ct 1 ct  Rmv Start with 2 x E x2 E x 2 and the lognormal property E ez e Ez 1 2 2 z . 6. There are assets with mean return equal to the risk free rate, but sub stantial standard deviation of returns. Long term bonds are pretty close examples. Why would anyone hold such an asset? Wouldn t it be better to put your money in a mean variance frontier asset? 7. The first order conditions for an infinitely lived investor who can buy an asset with dividend stream dt are pt Et  dt j. 1.39 pt Et  t 1 pt 1 dt 1 u ct a Derive 1.40 from 1.39 . 1.40 j u ct j j 1 u ct The first order conditions for buying a security with price pt and payoff xt 1 dt 1 pt 1 are u c b Derive 1.39 from 1.40 . You need an extra condition. Show that this extra condition is a first order condition for maximization. To",
        "Problems 33 do this, think about what strategy the consumer could follow to improve utility if the condition did not hold, both if this is if the only security available and if the investor can trade all state and date contingent claims. 8. Suppose a consumer has a utility function that includes leisure. This could also be a second good, or a good produced in another country. Using the continuous time setup, show that expected returns will now depend on two covariances, the covariance of returns with leisure and the covariance of returns with consumption, so long as leisure enters nonseparably, i.e., u c , l cannot be written v c w l . This is a three line problem, but you need to apply Ito s lemma to . 9. From show that the negative of the mean log discount factor must be larger than any mean return, E lnm E lnR . How is it possible that E ln R is bounded what about returns of the form R 1  R f R m for arbitrarily large ? Hint: start by assuming m and R are lognormal. Then see if you can generalize the results using Jensen sinequality,E f x f E x forf convex.Thereturnthatsolves maxR E ln R is known as the growth optimal portfolio. 1 E mR",
        "2 Applying the Basic Model 2.1 Assumptions and Applicability Writing p E mx , we do not assume 1. Markets are complete, or there is a representative investor 2. Asset returns or payoffs are normally distributed no options , or indepen dent over time 3. Two period investors, quadratic utility, or separable utility 4. Investors have no human capital or labor income 5. The market has reached equilibrium, or individuals have bought all the securities they want to All of these assumptions come later, in various special cases, but we have not made them yet. We do assume that the investor can consider a small marginal investment or disinvestment. The theory of asset pricing contains lots of assumptions used to derive analytically convenient special cases and empirically useful representations. In writing p E mx or pu ct Et u ct 1 xt 1 , we have not made most of these assumptions. We have not assumed complete markets or a representative investor. These equations apply to each individual investor, for each asset to which he has access, independently of the presence or absence of other investors or other assets. Complete markets representative agent assumptions are used if one wants to use aggregate consumption data in u ct , or other specializations and simplifications of the model. We have not said anything about payoff or return distributions. In particular, we have not assumed that returns are normally distributed or that utility is quadratic. The basic pricing equation should hold for any asset, stock, bond, option, real investment opportunity, etc., and any mono tone and concave utility function. In particular, it is often thought that 35",
        "36 2. Applying the Basic Model mean variance analysis and beta pricing models require these kinds of limiting assumptions or quadratic utility, but that is not the case. A mean variance efficient return carries all pricing information no matter what the distribution of payoffs, utility function, etc. This is not a two period model. The fundamental pricing equation holds for any two periods of a multiperiod model, as we have seen. Really, everything involves conditional moments, so we have not assumed i.i.d. returns over time. I have written things down in terms of a time and state separable utility function and I have focused on the convenient power utility example. Nothing important lies in either choice. Just interpret u ct as the partial derivative of a general utility function with respect to consumption at time t. State or time nonseparable utility habit persistence, durability compli cates the relation between the discount factor and real variables, but does not change p E mx or any of the basic structure. We do not assume that investors have no nonmarketable human capital, or no outside sources of income. The first order conditions for purchase of an asset relative to consumption hold no matter what else is in the budget constraint. By contrast, the portfolio approach to asset pricing as in the CAPM and ICAPM relies heavily on the assumption that the investor has no nonasset income, and we will study these special cases below. For example, leisure in the utility function just means that marginal utility u c,l may depend on l as well as c. We do not even really need the assumption yet that the market is in equilibrium, that the investor has bought all of the asset that he wants to, or even that he can buy the asset at all. We can interpret p E mx as giving us the value, or willingness to pay for, a small amount of a payoff xt 1 that the investor does not yet have. Here is why: If the investor had a little moreofthepayoffxt 1 attimet 1,hisutilityu ct Etu ct 1 would increase by Et u ct 1 xt 1 u ct 1 Et u ct 1 xt 1 12u ct 1 xt 1 2 . If  is small, only the first term on the right matters. If the investor has to give up a small amount of money vt  at time t , that loss lowers his utility by u ct vt u ct u ct vt 12u ct vt 2 . Again, for small , only the first term matters. Therefore, in order to receive the small extra payoff xt 1, the investor is willing to pay the small",
        "2.2. General Equilibrium amount vt  , where 37 vt Et  xt 1 . u ct 1 u ct If this private valuation is higher than the market value pt , and if the investor can buy some more of the asset, he will. As he buys more, his consumption will change; it will be higher in states where xt 1 is higher, driving down u ct 1 in those states, until the value to the investor has declined to equal the market value. Thus, after an investor has reached his optimal portfolio, the market value should obey the basic pricing equation as well, using post trade or equilibrium consumption. But the formula can also be applied to generate the marginal private valuation, using pre trade consumption, or to value a potential, not yet traded security. We have calculated the value of a small or marginal portfolio change for the investor. For some investment projects, an investor cannot take a small diversified position. For example, a venture capitalist or entrepreneur must usually take all or nothing of a project with payoff j u ct j  j u ct j xt j . Once the project is taken, of course, ct j xt j stream xt .Thenthevalueofaprojectnotalreadytaken,E xt j u ct j , might be substantially different from its marginal counter part, E becomes ct j , so the marginal valuation still applies to the ex post consump tion stream. Analysts often forget this point and apply marginal diversified valuation models such as the CAPM to projects that must be bought in dis crete chunks. Also, we have abstracted from short sales and bid ask spreads; this modification changes p E mx from an equality to a set of inequalities. j j 2.2 General Equilibrium Asset returns and consumption: which is the chicken and which is the egg? I present the exogenous return model, the endowment economy model, and the argument that it does not matter for studying p E mx . So far, we have not said where the joint statistical properties of the payoff xt 1 and marginal utility mt 1 or consumption ct 1 come from. We have also not said anything about the fundamental exogenous shocks that drive the economy. The basic pricing equation p E mx tells us only what the price should be, given the joint distribution of consumption marginal utility, discount factor and the asset payoff. There is nothing that stops us from writing the basic pricing equation as u ct Et u ct 1 xt 1 pt .",
        "38 2. Applying the Basic Model We can think of this equation as determining today s consumption given asset prices and payoffs, rather than determining today s asset price in terms of consumption and payoffs. Thinking about the basic first order condition in this way gives the permanent income model of consumption. Which is the chicken and which is the egg? Which variable is exogenous and which is endogenous? The answer is, neither, and for many purposes, it does not matter. The first order conditions characterize any equilibrium; if you happen to know E mx , you can use them to determine p; if you happen to know p, you can use them to determine consumption and savings decisions. For most asset pricing applications we are interested in understanding a wide cross section of assets. Thus, it is interesting to contrast the cross sectional variation in asset prices expected returns with cross sectional variation in their second moments betas with a single discount factor. In most applications, the discount factor is a function of aggregate variables market return, aggregate consumption , so it is plausible to hold the prop erties of the discount factor constant as we compare one individual asset to another. Permanent income studies typically dramatically restrict the num ber of assets under consideration, often to just an interest rate, and study the time series evolution of aggregate or individual consumption. Nonetheless, it is an obvious next step to complete the solution of our model economy; to find c and p in terms of truly exogenous forces. The results will of course depend on what the rest of the economy looks like, in particular the production or intertemporal transformation technology and the set of markets. Figure 2.1 shows one possibility for a general equilibrium. Suppose that the production technologies are linear: the real, physical rate of return the rate of intertemporal transformation is not affected by how much is invested. Figure 2.1. Consumption adjusts when the rate of return is determined by a linear technology.",
        "2.2. General Equilibrium 39 Now consumption must adjust to these technologically given rates of return. If the rates of return on the intertemporal technologies were to change, the consumption process would have to change as well. This is, implicitly, how the permanent income model works. This is how many finance theories such as the CAPM and ICAPM and the Cox, Ingersoll, and Ross 1985 model of the term structure work as well. These models specify the return process, and then solve the consumer s portfolio and consumption rules. Figure 2.2 shows another extreme possibility for the production technol ogy. This is an endowment economy. Nondurable consumption appears or is produced by labor every period. There is nothing anyone can do to save, store, invest, or otherwise transform consumption goods this period to consumption goods next period. Hence, asset prices must adjust until people are just happy consuming the endowment process. In this case con sumption is exogenous and asset prices adjust. Lucas 1978 and Mehra and Prescott 1985 are two very famous applications of this sort of endowment economy. Which of these possibilities is correct? Well, neither, of course. The real economy and all serious general equilibrium models look something like Figure 2.3: one can save or transform consumption from one date to the next, but at a decreasing rate. As investment increases, rates of return decline. Does this observation invalidate the modeling we do with the linear technology CAPM, CIR, permanent income model, or the endowment economy model? No. Start at the equilibrium in Figure 2.3. Suppose we model this economy as a linear technology, but we happen to choose for the rate of return on the linear technologies exactly the same stochastic pro cess for returns that emerges from the general equilibrium. The resulting joint consumption asset return process is exactly the same as in the origi nal general equilibrium! Similarly, suppose we model this economy as an Figure 2.2. Asset prices adjust to consumption in an endowment economy.",
        "40 2. Applying the Basic Model Figure 2.3. General equilibrium. The solid lines represent the indifference curve and pro duction possibility set. The dashed straight line represents the equilibrium rate of return. The dashed box represents an endowment economy that predicts the same consumption and asset return process. endowment economy, but we happen to choose for the endowment pro cess exactly the stochastic process for consumption that emerges from the equilibrium with a concave technology. Again, the joint consumption asset return process is exactly the same. Therefore, there is nothing wrong in adopting one of the following strategies for empirical work: 1. Form a statistical model of bond and stock returns, solve the opti mal consumption portfolio decision. Use the equilibrium consumption values in p E mx . 2. Formastatisticalmodeloftheconsumptionprocess,calculateassetprices and returns directly from the basic pricing equation p E mx . 3. Form a completely correct general equilibrium model, including the production technology, utility function, and specification of the market structure. Derive the equilibrium consumption and asset price process, including p E mx as one of the equilibrium conditions. If the statistical models for consumption and or asset returns are right, i.e., if they coincide with the equilibrium consumption or return pro cess generated by the true economy, either of the first two approaches will give correct predictions for the joint consumption asset return process. Most finance models, developed from the 1950s through the early 1970s, take the return process as given, implicitly assuming linear technolo gies. The endowment economy approach, introduced by Lucas 1978 , is a breakthrough because it turns out to be much easier. It is much easier to evaluate p E mx for fixed m than it is to solve joint consumption portfolio",
        "2.3. Consumption Based Model in Practice 41 problems for given asset returns, all to derive the equilibrium consumption process. To solve a consumption portfolio problem we have to model the investor s entire environment: we have to specify all the assets to which he has access, what his labor income process looks like or wage rate process, and include a labor supply decision . Once we model the consumption stream directly, we can look at each asset in isolation, and the actual computation is almost trivial. This breakthrough accounts for the unusual structure of the presentation in this book. It is traditional to start with an extensive study of consumption portfolio problems. But by modeling consumption directly, we have been able to study pricing directly, and portfolio problems are an interesting side trip which we can defer. Most uses of p E mx do not require us to take any stand on exogene ity or endogeneity, or general equilibrium. This is a condition that must hold for any asset, for any production technology. Having a taste of the extra assumptions required for a general equilibrium model, you can now appreciate why people stop short of full solutions when they can address an application using only the first order conditions, using knowledge of E mx to make a prediction about p. It is enormously tempting to slide into an interpretation that E mx determines p. We routinely think of betas and factor risk prices components of E mx as determining expected returns. For example, we routinely say things like the expected return of a stock increased because the firm took on riskier projects, thereby increasing its beta. But the whole consumption process, discount factor, and factor risk premia change when the produc tion technology changes. Similarly, we are on thin ice if we say anything about the effects of policy interventions, new markets and so on. The equi librium consumption or asset return process one has modeled statistically may change in response to such changes in structure. For such questions one really needs to start thinking in general equilibrium terms. It may help to remember that there is an army of permanent income macroeconomists who make precisely the opposite assumption, taking our asset return pro cesses as exogenous and studying endogenous consumption and savings decisions. 2.3 Consumption Based Model in Practice The consumption based model is, in principle, a complete answer to all asset pricing questions, but works poorly in practice. This observation motivates other asset pricing models.",
        "42 2. Applying the Basic Model The model I have sketched so far can, in principle, give a complete answer to all the questions of the theory of valuation. It can be applied to any security bonds, stocks, options, futures, etc. or to any uncertain cash flow. All we need is a functional form for utility, numerical values for the parameters, and a statistical model for the conditional distribution of consumption and payoffs. To be specific, consider the standard power utility function u c c  . Then, excess returns should obey sition, expected excess returns should follow 2.1 Re . Taking unconditional expectations and applying the covariance decompo 0 E  ct 1 t ct t 1 2.2 ,Re . 2.3 t 1 E Re Rfcov  ct 1 t 1 ct  Given a value for , and data on consumption and returns, you can easily estimate the mean and covariance on the right hand side, and check whether actual expected returns are, in fact, in accordance with the formula. Similarly, the present value formula is pt Et N ct N ct  t 1, t N  pt Et j ct j j 1 ct dt j. 2.4 Given data on consumption and dividends or another stream of payoffs, you can estimate the right hand side and check it against prices on the left. Bonds and options do not require separate valuation theories. For exam ple, an N period default free nominal discount bond a U.S. Treasury strip isaclaimtoonedollarattimet N.Itspriceshouldbe  where price level good . A European option is a claim to the payoff max St T K,0 ,whereSt T stockpriceattimet T,K strikeprice. The option price should be pt Et T ct T ct  max St T K,0 .",
        "2.4. Alternative Asset Pricing Models: Overview 43 Figure 2.4. Mean excess returns of 10 CRSP size portfolios versus predictions of the power utility consumption based model. The predictions are generated by Rf cov m, Ri with m  ct 1 ct  .  0.98 and  241 are picked by first stage GMM to minimize thesumofsquaredpricingerrors deviationfrom45 line .Source:Cochrane 1996 . Again, we can use data on consumption, prices, and payoffs to check these predictions. Unfortunately, this specification of the consumption based model does not work very well. To give a flavor of some of the problems, Figure 2.4 presents the mean excess returns on the ten size ranked portfolios of NYSE stocks versus the predictions the right hand side of 2.3 of the consumption basedmodel.Ipickedtheutilitycurvatureparameter 241 to make the picture look as good as possible. The section on GMM esti mation below goes into detail on how to do this. The figure presents the first stage GMM estimate. As you can see, the model is not hopeless there is some correlation between sample average returns and the consumption based model predictions. But the model does not do very well. The pricing error actual expected return predicted expected return for each port folio is of the same order of magnitude as the spread in expected returns across the portfolios. 2.4 Alternative Asset Pricing Models: Overview I motivate exploration of different utility functions, general equilibrium models, and linear factor models such as the CAPM, APT, and ICAPM as ways to circumvent the empirical difficulties of the consumption based model.",
        "44 2. Applying the Basic Model The poor empirical performance of the consumption based model motivates a search for alternative asset pricing models alternative func tions m f data . All asset pricing models amount to different functions for m. I give here a bare sketch of some of the different approaches; we study each in detail in later chapters. 1 Different utility functions. Perhaps the problem with the consumption based model is simply the functional form we chose for utility. The natural response is to try different utility functions. Which variables determine marginal utility is a far more important question than the functional form. Perhaps the stock of durable goods influences the marginal utility of nondurable goods; perhaps leisure or yesterday s consumption affect today s marginal utility. These possibilities are all instances of nonseparabilities. One can also try to use micro data on indi vidual consumption of stockholders rather than aggregate consumption. Aggregation of heterogeneous investors can make variables such as the cross sectional variance of income appear in aggregate marginal utility. 2 General equilibrium models. Perhaps the problem is simply with the con sumption data. General equilibrium models deliver equilibrium decision rules linking consumption to other variables, such as income, invest ment, etc. Substituting the decision rules ct f yt , it , . . . in the consumption based model, we can link asset prices to other, hopefully better measured macroeconomic aggregates. In addition, true general equilibrium models completely describe the economy, including the stochastic process followed by all variables. They can answer questions such as why is the covariance beta of an asset payoff x with the discount factor m the value that it is, rather than take this covariance as a primitive. They can in principle answer struc tural questions, such as how asset prices might be affected by different government policies or the introduction of new securities. Neither kind of question can be answered by just manipulating investor first order conditions. 3 Factorpricingmodels.Anothersensibleresponsetobadconsumptiondata is to model marginal utility in terms of other variables directly. Factor pricing models follow this approach. They just specify that the discount factor is a linear function of a set of proxies, m a b fA b fB , 2.5 t 1 A t 1 B t 1 wherefi arefactorsanda,bi areparameters. Thisisadifferentsenseof the use of the word factor than discount factor or factor analysis. I did not invent the confusing terminology. By and large, the factors are just selected as plausible proxies for marginal utility: events that",
        "Problems 45 describe whether typical investors are happy or unhappy. Among others, 4 where R W is the rate of return on a claim to total wealth, often proxied by a broad based portfolio such as the value weighted NYSE portfolio. The Arbitrage Pricing Theory APT uses returns on broad based port folios derived from a factor analysis of the return covariance matrix. The Intertemporal Capital Asset Pricing Model ICAPM suggests macro economic variables such as GNP and inflation and variables that forecast macroeconomic variables or asset returns as factors. Term structure mod els such as the Cox Ingersoll Ross model specify that the discount factor is a function of a few term structure variables, for example the short rate of interest and a few interest rate spreads. Many factor pricing models are derived as general equilibrium mod els with linear technologies and no labor income; thus they also fall into the general idea of using general equilibrium relations from, admit tedly, very stylized general equilibrium models to substitute out for consumption. Arbitrage or near arbitrage pricing. The mere existence of a representation p E mx and the fact that marginal utility is positive m 0 these facts are discussed in the next chapter can often be used to deduce prices of one payoff in terms of the prices of other payoffs. The Black Scholes option pricing model is the paradigm of this approach: Since the option payoff can be replicated by a portfolio of stock and bond, any discount factor m that prices the stock and bond gives the price for the option. Recently, there have been several suggestions on how to use this idea in more general circumstances by using very weak further restrictions on m, and we will study these suggestions in Chapter 17. We return to a more detailed derivation and discussion of these alternative models of the discount factor m below. First, and with this brief overview in mind, we look at p E mx and what the discount factor m represents in a little more detail. Problems Chapter 2 The representative consumer maximizes a CRRA utility function, c1  Et jt j. 1  the Capital Asset Pricing Model CAPM is the model m a bRW, t 1 t 1 1.",
        "46 2. Applying the Basic Model Consumption is given by an endowment stream. a Show that with log utility, the price consumption ratio of the consumption stream is constant, no matter what the distribution of consumption growth. b Suppose there is news at time t that future consumption will be higher.For 1, 1,and 1,evaluatetheeffectofthisnewson the price. Make sense of your results. There is a real world interpretation here. It is often regarded as a puzzle that the market declines on good economic news. This is attributed to an expectation by the market that the Fed will respond to such news by raising interest rates. Note that  1 in this problem gives a completely real and frictionless interpretation to this phenomenon! I thank Pete Hecht for this nice problem. The linear quadratic permanent income model is a very useful general equilibrium model that we can solve in closed form. It specifies a production technology rather than fixed endowments, and it easily allows aggregation of disparate consumers. Hansen 1987 is a wonderful exposition of what one can do with this setup. The consumer maximizes 2. E t 1 ct c 2 t 0 2 subject to a linear technology kt 1 1 r kt it, it et ct. et is an exogenous endowment or labor income stream. Assume  1 1 r ; the discount rate equals the interest rate or marginal pro ductivity of capital. a Show that optimal consumption follows jEtet j, j 0 i.e., consumption equals permanent income, precisely defined, and consumption follows a random walk whose innovations are equal to innovations in permanent income. ct rkt r ct ct 1 Et Et 1 r 2.6 jet j, 2.7 j 0",
        "Problems 47 3. b Assume that the endowment et follows an AR 1 et et 1 t and specialize 2.6 and 2.7 . Calculate and interpret the result for  1 and  0. The result looks like a consumption function relating consumption to capital and current income, except that the slope of that function depends on the persistence of income shocks. Transitory shocks will have little effect on consumption, and permanent shocks a larger effect. c Calculate the one period interest rate it should come out to r , of course and the price of a claim to the consumption stream. Express the price in terms of ct . Interpret the price of the consumption stream as a risk neutral term and a risk premium, and explain the economics of the risk premium. This consumer gets more risk averse as consumption rises Consider again CRRA utility, to c . c is the bliss point, so at the bliss point there is no average return that can compensate the consumer for greater risk. E t j c1  . t j Consumption growth follows a two state Markov process. The states are ct ct ct 1 h,l, and a 2 2 matrix  governs the set of transition probabilities, i.e., pr ct 1 h ct l l h . This is the Mehra Prescott 1985 model, but it will be faster to do it than to look it up. It is a useful and simple endowment economy. a Find the risk free rate price of a certain real payoff of 1 in this economy. This price is generated by pb E m 1 . t t t,t 1 You are looking for two values, the price in the l state and the price in the h state. b Find the price of the consumption stream the price at t of ct 1,ct 2,... . To do this, guess that the price consumption ratio must be a function of state h , l , and find that function. From pc E m pc c t t t,t 1 t 1 t 1 find a recursive relation for p c c , and hence find the two values of p c c , tt tt one for the h state and one for the l state. c Pick  0.99 and try  0.5, 5. Try more values if you feel like it. Calibrate the consumption process to have a 1 mean and 1 standard",
        "3.3. Investors Again 53 3.3 Investors Again We look at an investor s first order conditions in a contingent claims market. The marginal rate of substitution equals the discount factor and the contingent claim price ratio. Though the focus of this chapter is on how to do without utility func tions, it is worth looking at the investor s first order conditions again in the contingent claim context. The investor starts with a pile of initial wealth y and a state contingent income y s . He may purchase contingent claims to each possible state in the second period. His problem is then max u c  s u c s s.t. c pc s c s y pc s y s . c, c s Introducing a Lagrange multiplier  on the budget constraint, the first order conditions are sss Eliminating the Lagrange multiplier , pc s  s u c s u c ,  s u c s pc s . or pc s  s u c u c s u c m s  . Coupled with p E mx , we obtain the consumption based model again. The investor s first order conditions say that the marginal rate of substitution between states tomorrow equals the relevant price ratio, m s1 u c s1 . m s2 u c s2 m s1 m s2 gives the rate at which the investor can give up consumption in state 2 in return for consumption in state 1 through purchase and sales of contingent claims. u c s1 u c s2 gives the rate at which the investor is willing to make this substitution. At an optimum, the marginal rate of substitution should equal the price ratio, as usual in economics. We learn that the discount factor m is the marginal rate of substitution between date and state contingent commodities. That is why it, like c s , is",
        "54 3. Contingent Claims Markets Figure 3.1. Indifference curve and contingent claim prices. a random variable. Also, scaling contingent claims prices by probabilities gives marginal utility, and so is not so artificial as it may have seemed above. Figure 3.1 gives the economics behind this approach to asset pricing. We observe the investor s choice of date or state contingent consumption. Once we know his utility function, we can calculate the contingent claim prices that must have led to the observed consumption choice, from the derivatives of the utility function. The relevant probabilities are the investors subjective probabilities over the various states. Asset prices are set, after all, by investors demands for assets, and those demands are set by investors subjective evaluations of the probabilities of various events. We often assume rational expectations, namely that subjective probabilities are equal to objective frequencies. But this is an additional assumption that we may not always want to make. 3.4 Risk Sharing We deduced that the marginal rate of substitution for any individual investor equals the contingent claim price ratio. But the prices are the same Risk sharing: In complete markets, individuals consumption moves together. Only aggregate risk matters for security markets.",
        "3.4. Risk Sharing 55 for all investors. Therefore, marginal utility growth should be the same for all investors, u ci u cj i t 1 j t 1   , 3.2 u ci u cj tt where i and j refer to different investors. If investors have the same homo thetic utility function for example, power utility , then consumption itself moves in lockstep, ci cj t 1 t 1. cti ctj More generally, shocks to consumption are perfectly correlated across individuals. This prediction is so radical, it is easy to misread it at first glance. It does not say that expected consumption growth is equal; it says that ex post consumption growth is equal. If my consumption goes up 10 , yours goes up exactly 10 as well, and so does everyone else s. In a complete con tingent claims market, all investors share all risks, so when any shock hits, it hits us all equally after insurance payments . It does not say the con sumption level is the same this is risk sharing, not socialism. The rich have higher levels of consumption, but rich and poor share the shocks equally. This risk sharing is Pareto optimal. Suppose a social planner wished to maximize everyone s utility given the available resources. For example, with two investors i and j, he would maximize maxE tu ci E tu cj s.t.ci cj ca, i tj tttt tt where ca is the total amount available and i and j are i and j s rela tive weights in the planner s objective. The first order condition to this problem is1 u ci u cj itjt and hence the same risk sharing that we see in a complete market, equation 3.2 . This simple fact has profound implications. First, it shows you why only aggregate shocks should matter for risk prices. Any idiosyncratic income risk will 1 People can have different utility functions, and these need not be functions of current consumption only, at the cost of more complex notation, i.e., iu i ci, ju j cj, tt",
        "56 3. Contingent Claims Markets be equally shared, and so 1 N of it becomes an aggregate shock. Then the stochastic discount factors m that determine asset prices are no longer affected by truly idiosyncratic risks. Much of this idea that only aggregate shocks matter stays with us in incomplete markets as well. Obviously, the real economy does not yet have complete markets or full risk sharing individual consumptions do not move in lockstep. However, this observation tells us much about the function of securities markets. Security markets state contingent claims bring individual consumptions closer together by allowing people to share some risks. In addition, better risk sharing is much of the force behind financial innovation. Many suc cessful new securities can be understood as devices to share risks more widely. 3.5 State Diagram and Price Function Think of the contingent claims price pc and asset payoffs x as vectors in RS , where each element gives the price or payoff to the corresponding state, pc pc 1 pc 2 pc S , x x 1 x 2 x S . Figure 3.2 is a graph of these vectors in RS . Next, I deduce the geometry of Figure 3.2. The contingent claims price vector pc points into the positive orthant. We saw in Section 3.3 that m s u c s u c . Now, marginal utility should always be positive people always want more , so the marginal rate of substitution and discount factor are always nonnegative, m 0 and pc 0. Do not forget, m and pc are vectors, or random variables. Thus, m 0 means the realization of m is positive in every state of nature, or, equivalently every element of the vector m is positive. The set of payoffs with any given price lie on a hyper plane perpendicular to the contingent claim price vector. We reasoned above that the price of the payoff x must be given by its contingent claim value 3.1 , s I introduce the state space diagram and inner product representation for prices, p x E mx m x. p x E mx implies that p x is a linear function. p x pc s x s . 3.3",
        "6.1. From Discount Factors to Beta Representations 101 Multiply and divide by var m , define  1 E m to get i cov m,Ri var m E R  var m  i,m m. E m As advertised, we have a single beta representation. For example, we can equivalently state the consumption based model as: mean asset returns should be linear in the regression betas of asset returns on ct 1 ct . Furthermore, the slope of this cross sectional relationship m is not a free parameter, though it is usually treated as such in empirical evaluationoffactorpricingmodels.m shouldequaltheratioofvarianceto mean of ct 1 ct  . The factor risk premium m for marginal utility growth is negative. Positive expected returns are associated with positive correlation with con sumption growth, and hence negative correlation with marginal utility growthandm.Thus,weexpectm 0.  Representation Using x and R It is often useful to express a pricing model in a way that the factor is a payoff rather than a real variable such as consumption growth. In applications, we can then avoid measurement difficulties of real data. We have already seen the idea of factor mimicking portfolios formed by projection: project Theorem:1 E mRi impliesanexpectedreturn betamodelwithx proj m X or R x E x 2 as factors, E Ri  i,x x and E Ri  i,R E R  . Proof: Recall that p E mx implies p E proj m X x , or p E x x . Then m on to X, and the resulting payoff x also serves as a discount factor. Unsurprisingly, x or the return R x E x 2 can also serve as factors in an expected return beta representation. When the factor is also a return, the model is particularly simple, since the factor risk premium is also the expected excess return. 1 E mRi E x Ri E x E Ri cov x ,Ri . Solving for the expected return, E R 6.1 i 1 cov x ,Ri E x 1 E x cov x ,Ri var x , E x var x E x",
        "8 Conditioning Information Asset pricing theory really describes prices in terms of conditional moments. The investor s first order conditions are pt u ct Et u ct 1 xt 1 , where Et means expectation conditional on the investor s time t information. Sensibly, the price at time t should be higher if there is information at time t that the discounted payoff is likely to be higher than usual at time t 1. The basic asset pricing equation should be pt Et mt 1xt 1 . Conditional expectation can also be written pt E mt 1xt 1 It when it is important to specify the information set It . If payoffs and discount factors were independent and identically dis tributed i.i.d. over time, then conditional expectations would be the same as unconditional expectations and we would not have to worry about the distinction between the two concepts. But stock price dividend ratios, and bond and option prices all change over time, which must reflect changing conditional moments of something on the right hand side. One approach is to specify and estimate explicit statistical models of conditional distributions of asset payoffs and discount factor variables e.g., consumption growth, market return . This approach is useful in some applications, but it is usually cumbersome. As we make the conditional mean, variance, covariance, and other parameters of the distribution of say N returns depend flexibly on M information variables, the number of required parameters can quickly exceed the number of observations. More importantly, this explicit approach typically requires us to assume that investors use the same model of conditioning information that we do. We obviously do not even observe all the conditioning information used 131",
        "9 Factor Pricing Models In Chapter 2, I noted that the consumption based model, while a complete answer to most asset pricing questions in principle, does not yet work well in practice. This observation motivates efforts to tie the discount factor m to other data. Linear factor pricing models are the most popular models of this sort in finance. They dominate discrete time empirical work. Factor pricing models replace the consumption based expression for marginal utility growth with a linear model of the form mt 1 a b ft 1. a and b are free parameters. As we saw in Chapter 6, this specification is equivalent to a multiple beta model E Rt 1   , where  are multiple regression coefficients of returns R on the factors f , and  and  are free parameters. The big question is, what should one use for factors ft 1? Factor pricing models look for variables that are good proxies for aggregate marginal utility growth, i.e., variables for which u ct 1 u ct is a sensible and economically interpretable approximation. More directly, the essence of asset pricing is that there are spe cial states of the world in which investors are especially concerned that their portfolios not do badly. They are willing to trade off some overall performance average return to make sure that portfolios do not do badly in these particular states of nature. The factors are variables that indicate that these bad states have occurred. 149  a b ft 1 9.1",
        "Factor Pricing Models 151 This view of factors as intuitively motivated proxies for marginal utility growth is sufficient to carry the reader through current empirical tests of factor models. The extra constraints that result from formal derivations in this chapter have not much constrained empirical specifications. The derivations all proceed in the way I have motivated factor models: One writes down a general equilibrium model, in particular a specification of the production technology by which real investment today results in real output tomorrow. This general equilibrium produces relations that express the determinants of consumption from exogenous variables, and relations linking consumption to other endogenous variables; equations of the form ct g ft . One then uses this kind of equation to substitute out for con sumption in the basic first order conditions. The CAPM and ICAPM are general equilibrium models with linear technologies, rates of return R that do not depend on the quantity invested. The derivations accomplish two things: they determine one particular list of factors that can proxy for marginal utility growth, and they prove that the relation should be linear. Some assumptions can often be substituted for others in the quest for these two features of a factor pricing model. This is a point worth remembering: all factor models are derived as special izations of the consumption based model. Many authors of factor model papers disparage the consumption based model, forgetting that their factor model is the consumption based model plus extra assumptions that allow one to proxy for marginal utility growth from some other variables. Constantinides 1989 is a good statement of this point. In Chapter 7, I argued that clear economic foundation was important for factor models, since it helps us to guard against fishing. Alas, the current state of factor pricing models is not a particularly good guard. One can call for better theories or derivations, more carefully aimed at limiting the list of potential factors and describing the fundamental macroeconomic sources of risk, and thus providing more discipline for empirical work. But the best minds in finance have been working on this problem for 40 years, so a ready solution is not immediately in sight. Furthermore, even current theory can provide more discipline than is commonly imposed in empirical work. So it s not clear that tighter theories will change practice. For example, the derivations of the CAPM and ICAPM make predictions for the risk free rate and for factor risk premia that are often ignored; these quantities are typically estimated as free parameters. The ICAPM gives tighter restrictions on state variables than are commonly checked: State variables should forecast something. The derivations also show how special and unrealistic are the general equilibrium setups necessary to derive popular specifications such as CAPM and ICAPM. This observation motivates a more serious look at real general equilibrium models.",
        "152 9. Factor Pricing Models 9.1 Capital Asset Pricing Model CAPM The CAPM, credited to Sharpe 1964 and Lintner 1965a, b , is the first, most famous, and so far most widely used model in asset pricing. It ties the discount factor m to the return on the wealth portfolio. The function is linear, m a bRW . t 1 t 1 a and b are free parameters. One can find theoretical values for the param eters a and b by requiring the discount factor m to price any two assets, such as the wealth portfolio return and risk free rate, 1 E mRW and 1 E m R f . For example, see equation 8.3 . In empirical applications, we can also pick a and b to best price larger cross sections of assets. We do not have good data on, or even a good empirical definition for, the return on total wealth. It is conventional to proxy R W by the return on a broad based stock portfolio such as the value or equally weighted NYSE, S P500, etc. The CAPM is, of course, most frequently stated in equivalent expected return beta language, E Ri   W E RW . i,R This section briefly describes some classic derivations of the CAPM. Again, we need to find assumptions that defend which factors proxy for marginalutility RW here ,andassumptionstodefendthelinearitybetween m and the factor. I present several derivations of the same model. Many of these derivations use classic and widely applicable modeling assumptions. You can also see that various sets of assumptions can often be used to get to the same place. By seeing several derivations, you can see how one assumption can be traded for another. For example, the CAPM does not require normal distributions, if one is willing to swallow quadratic utility instead. The CAPM is the model m a bRw; Rw wealth portfolio return. I derive it from the consumption based model by 1 two period quadratic utility; 2 two periods, exponential utility, and normal returns; 3 infinite horizon, quadratic utility, and i.i.d. returns; 4 log utility.",
        "9.1. Capital Asset Pricing Model CAPM 153 Two Period Quadratic Utility Investors have quadratic preferences and only live two periods, 11 U ct,ct 1 c ct 2 E c ct 1 2 . 9.2 22 Their marginal rate of substitution is thus R W i 1 i 1 is the rate of return on total wealth. Two period investors with no labor income and quadratic utility imply the CAPM. mt 1  u ct 1 u ct  c ct 1 c ct . The quadratic utility assumption means marginal utility is linear in consump tion. Thus, we have achieved one target of the derivation, linearity. InvestorsarebornwithwealthWt inthefirstperiodandearnnolabor income. They can invest in N assets with prices pti and payoffs xti 1 , or, to keep the notation simple, returns R ti 1 . They choose how much to consume at the two dates, ct and ct 1, and portfolio weights wi. Thus, the budget constraint is ct 1 Wt 1, W RW W c, t 1 t 1 t t N RW wRi , 9.3 N t 1 it 1 i The two period assumption means that investors consume everything in the second period, by constraint 9.3 . This fact allows us to substitute wealth and the return on wealth for consumption, achieving the second goal of the derivation, naming the factor that proxies for consumption or marginal utility: c RW W c c  W c m  t 1 t t t t RW , 9.4 w 1. i.e., t 1 c ct c ct c ct t 1 m a bRW. t 1 t tt 1",
        "154 9. Factor Pricing Models Exponential Utility, Normal Distributions The combination of exponential utility and normal distributions is another set of assumptions that deliver the CAPM in a one or two period model. This structure has a particularly convenient analytical form. Since it gives rise to linear demand curves, it is very widely used in models that complicate the trading structure, by introducing incomplete markets or asymmetric information. Grossman and Stiglitz 1980 is a very famous example. I present a model with consumption only in the last period. You can do the quadratic utility model of the last section this way as well. Utility is E u c E e c .  is known as the coefficient of absolute risk aversion. If consumption is normally distributed, we have Eu c e E c 2 2 2 c . Suppose this investor has initial wealth W which can be split between a risk free asset paying R f and a set of risky assets paying return R . Let y denote the amount of this wealth W amount, not fraction invested in each security. Then, the budget constraint is c yfRf y R, W yf y 1. Plugging the first constraint into the utility function, we obtain Eu c e  yf R f y E R 2 2 y y. 9.5 As with quadratic utility, the two period model is what allows us to set con sumption to wealth and then substitute the return on the wealth portfolio for consumption growth in the discount factor. Maximizing 9.5 with respect to y, yf , we obtain the first order condition describing the optimal amount to be invested in the risky asset, 1 E R R f y .  u c e c and a normally distributed set of returns also produces the CAPM.",
        "9.1. Capital Asset Pricing Model CAPM 155 Sensibly, the investor invests more in risky assets if their expected return is higher, less if his risk aversion coefficient is higher, and less if the assets are riskier. Notice that total wealth does not appear in this expression. With this setup, the amount invested in risky assets is independent of the level of wealth. This is why we say that this investor has constant absolute rather than relative to wealth risk aversion. Note also that these demands for the risky assets are linear in expected returns. Inverting the first order conditions, we obtain E R Rf  y cov R,RW . 9.6 The investor s total risky portfolio is y R. Hence, y gives the covariance of eachreturnwithy R,andalsowiththeinvestor soverallportfolioyf Rf y R. If all investors are identical, then the market portfolio is the same as the individual s portfolio so y also gives the correlation of each return with Rm yfRf y R. Ifinvestorsdifferinriskaversion,thesamethinggoes through but with an aggregate risk aversion coefficient. Thus, we have the CAPM. This version is especially interesting because it ties the market price of risk to the risk aversion coefficient. Applying 9.6 to the market return itself, we have E RW Rf 2 RW . Quadratic Value Function, Dynamic Programming The two period structure is unpalatable, since most investors live longer than two periods. It is natural to try to make the same basic ideas work with less restrictive assumptions. We can derive the CAPM in a multiperiod context by replacing the second period quadratic utility function with a quadratic value function. However, the quadratic value function requires the additional assump tion that returns are i.i.d. no shifts in the investment opportunity set . This observation, due to Fama 1970 , is also a nice introduction to dynamic programming, which is a powerful way to handle multiperiod problems by expressing them as two period problems. Finally, I think this derivation makes the CAPM more realistic, transparent, and intuitively compelling. We can let investors live forever in the quadratic utility CAPM so long as we assume that the environment is independent over time. Then the value function is quadratic, taking the place of the quadratic second period utility function. This case is a nice introduction to dynamic programming.",
        "156 9. Factor Pricing Models Buying stocks amounts to taking bets over wealth; really the fundamental assumption driving the CAPM is that marginal utility of wealth is linear in wealth and does not depend on other state variables. Let us start in a simple ad hoc manner by just writing down a utility function defined over this period s consumption and next period s wealth, U u ct EtV Wt 1 . This is a reasonable objective for an investor, and does not require us to make the very artificial assumption that he will die tomorrow. If an investor with this utility function can buy an asset at price pt with payoff xt 1, his first order condition buy a little more, then x contributes to wealth next period is pt u ct Et V Wt 1 xt 1 . Thus, the discount factor uses next period s marginal value of wealth in place of the more familiar marginal utility of consumption V Wt 1 u ct The envelope condition states that, at the optimum, a penny saved has the same value as a penny consumed, u ct V Wt . We could also use this condition to express the denominator in terms of wealth. Now, suppose the value function were quadratic, Then, we would have mt 1  . V Wt 1  2 Wt 1 W 2. W W mt 1  t 1  RW Wt ct W t 1 u ct RW, u ct W  Wt ct u ct u ct t 1 or, once again, m a bRW, t 1 t tt 1 the CAPM! Let us be clear about the assumptions and what they do. 1 The value function only depends on wealth. If other variables entered the value function, then V W would depend on those other variables, and so would m. This assumption bought us the first objective of any derivation: the identity of the factors. The ICAPM, below, allows other variables in the value function, and obtains more factors. Actually, other variables could enter the value function so long as they do not affect the marginal value of wealth. The weather is an example: You, like I, might be happier on sunny",
        "9.1. Capital Asset Pricing Model CAPM 157 days, but you do not value additional wealth more on sunny than on rainy days. Hence, covariance with weather does not affect how you value stocks. 2 The value function is quadratic. We wanted the marginal value function V W to be linear, to buy us the second objective, showing m is linear in the factor. Quadratic utility and value functions deliver a globally linear marginal value function V W . Why is the Value Function Quadratic? You might think we are done. But good economists are unhappy about a utility function that has wealth in it. Few of us are like Disney s Uncle Scrooge, who got pure enjoyment out of a daily swim in the coins in his vault. Wealth is only valuable because it gives us access to more consumption. Utility functions should always be written over consumption. One of the few real rules in economics to keep our theories from being vacuous is that ad hoc utility functions over other objects like wealth or means and variances of portfolio returns should eventually be defended as arising from a more fundamental desire for consumption or leisure. More practically, being careful about the derivation makes clear that the superficially plausible assumption that the value function is only a func tion of wealth derives from the much less plausible, in fact certainly false, assumption that interest rates are constant, the distribution of returns is i.i.d., and that the investor has no risky labor income. So, let us see what it takes to defend the quadratic value function in terms of some utility function. Suppose investors last forever, and have the standard sort of utility function j 0 Again, investors start with wealth W0 which earns a random return RW and they have no other source of income. In addition, suppose that interest rates are constant, and stock returns are i.i.d. over time. Define the value function as the maximized value of the utility function in this environment. Thus, define V W as1 9.7 1 There is also a transversality condition or a lower limit on wealth Wt W in the budget constraints. This keeps the consumer from consuming a bit more and rolling over more and more debt, and it means we can write the budget constraint in present value form. U Et ju ct j . j u ct j s.t. W RW W c ; RW w R; w 1 1. V Wt max Et ct , ct 1, ct 2, ..., wt , wt 1, ... t 1 t 1 t t t t t t j 0",
        "158 9. Factor Pricing Models I use vector notation to simplify the statement of the portfolio problem; R R1 R2 RN , etc. The value function is the total level of utility the investor can achieve, given how much wealth he has, and any other vari ables constraining him. This is where the assumptions of no labor income, a constant interest rate, and i.i.d. returns come in. Without these assump tions, the value function as defined above might depend on these other characteristics of the investor s environment. For example, if there were some variable, say, D P that indicated returns would be high or low for a while, then the investor might be happier, and have a high value, when D P is high, for a given level of wealth. Thus, we would have to write V Wt , D Pt . Value functions allow you to express an infinite period problem as a two period problem. Break up the maximization into the first period and all the remaining periods, as follows: or V Wt max u ct Et max Et 1 ju ct 1 j 9.8 ct , wt ct 1, ct 2,...,wt 1, wt 2,... V Wt max u ct EtV Wt 1 ct,wt j 0 Thus, we have defended the existence of a value function. Writing down a two period utility function over this period s consumption and next period s wealth is not as crazy as it might seem. The value function is also an attractive view of how people actually make decisions. People do not think If I buy an expensive lunch today, I will not be able to go out to dinner one night 20 years from now trading off goods directly as expressed by the utility function. They think I cannot afford an expensive lunch meaning that the decline in the value of wealth is not worth the increase in the marginal utility of consumption. Thus, the maximization in 9.8 describes people s psychological approach to utility maximization. The remaining question is, can the value function be quadratic? What utility function assumption leads to a quadratic value function? Here is the fun fact: A quadratic utility function leads to a quadratic value function in this environment. This fact is not a law of nature; it is not true that for any u c , V W has the same functional form. But it is true here and a few other special cases. The in this environment clause is not innocuous. The value function the achieved level of expected utility is a result of the utility function and the constraints. How could we show this fact? One way would be to try to calculate the value function by brute force from its definition, equation 9.7 . This approach is not fun, and it does not exploit the beauty of dynamic program ming, which is the reduction of an infinite period problem to a two period problem.",
        "160 9. Factor Pricing Models ThisisaquadraticfunctionofWt andct.Aquadraticfunctionofalinearfunc tion is a quadratic function, so the value function is a quadratic function of Wt . If you want to spend a pleasant few hours doing algebra, plug 9.10 into 9.11 , check that the result really is quadratic in Wt , and determine the coef ficients,W intermsoffundamentalparameters,c ,E RW ,E RW2 or  2 R W . The expressions for , W do not give much insight, so I do not do the algebra here. Log Utility The point of the CAPM is to avoid the use of consumption data, and to use wealth or the rate of return on wealth instead. Log utility is another special case that allows this substitution. Log utility is much more plausible than quadratic utility. Rubinstein 1976 introduced the log utility CAPM. Suppose that the investor has log utility u c ln c . Define the wealth portfolio as a claim to all future consumption. Then, with log utility, the price of the wealth portfolio is proportional to consumption itself, The return on the wealth portfolio is proportional to consumption growth, Log utility rather than quadratic utility also implies a CAPM. Log utility implies that consumption is proportional to wealth, allowing us to substitute the wealth return for consumption data. u ct j ct c E j c c.  t t u c t j t ct j 1 t pW E j j 1 t j 1 t j 1c 1 u c t 1 t 1 t. t 1 ptW Thus, the log utility discount factor equals the inverse of the wealth portfolio pW c RW t 1 t 1  1  1 c  1  ct  ct u ct 1 return, mt 1 1 . 9.12 RW t 1 Equation 9.12 could be used by itself: it attains the goal of replacing consumption data by some other variable. Brown and Gibbons 1985 test a CAPM in this form. Note that log utility is the only assumption so far. We",
        "9.1. Capital Asset Pricing Model CAPM 161 do not assume constant interest rates, i.i.d. returns, or the absence of labor income. Log utility has a special property that income effects offset substitution effects, or in an asset pricing context, that discount rate effects offset cashflow effects. News of higher consumption dividend should make the claim to consumption more valuable. However, through u c it also raises the discount rate, lowering the value of the claim to consumption. For log utility, these two effects exactly offset. Linearizing Any Model The twin goals of a linear factor model derivation are to derive what variables drive the discount factor, wealth in the case of the CAPM, and to derive a linear relation between the discount factor and these variables. The first goal is often easier than the second. For example, the log utility CAPM got us the right variable, the return on the market potfolio, but a nonlinear functional form. This section covers three standard tricks that are used to obtain a linear functional form. Section 9.3 considers whether linearity is still an important goal. Taylor Expansion The most obvious way to linearize the model is by a Taylor expansion, mt 1 g ft 1 at btft 1. Iwritethecoefficientsasat andbt becausethechosenexpansionpointmay well change over time, so that ft 1 does not stray too far from the expansion point. For example, a natural expansion point is the conditional mean of the factor. Then, mt 1 g Et ft 1 g Et ft 1 ft 1 Et ft 1 . Continuous Time We can often derive an exact linearization in continuous time. Then, if the discrete time interval is short enough, we can apply the continuous time result as an approximation. Write the nonlinear discount factor as t g ft,t Taylor expansions, the continuous time limit, and normal distributions canallturnanonlinearlinearmodelm g f intoalinearmodelm a bf .",
        "162 9. Factor Pricing Models so g g 1 2g2 dt dt dft dft. t f 2 f 2 Then, the basic pricing equation in continuous time for asset i reads Et dpti pti Dti f dt rt dt Et dpti d t pti t 1 g ft,t dpti Et idft, pti or, for a short discrete time interval, E Ri Rf cov Ri ,f 1 g ft,t t t 1 t t t 1 t 1  i , f ; t  ft . g f ,t f g f ,t f pt Working backward, we have a discrete time discount factor that is linear in f . Consumption based model. We used this trick in Chapter 1 to derive a linearized version of the consumption based model. With e t c  , we have and hence tt d dc dc2 t dt  t   1 t t ct ct2 Et dt rt dt Et pti pti t dpti pti Dti f dpti d t or, for a short discrete time interval, E Ri Rf cov Ri ,ct 1 portfolio is, in continuous time, tt 1t tt 1ct c. i, c;t t Log utility CAPM. The price of the consumption stream or wealth u c pW E t t t e su c c ds. t s t s 0 E dptidct t pti ct",
        "164 9. Factor Pricing Models Though it may belabor the point, this lemma allows us to derive a linear discount factor: or p E mx E g f x E g f E x cov g f ,x E g f E x E g f cov f ,x E E g f E g f f Ef x E E g f E g f E f E g f f x mt 1 Et g ft 1 Et g ft 1 Et ft 1 Et g ft 1 ft 1 at btft 1. Similarly, it allows us to derive an expected return beta model using the factors E Ri t t 1 Rf cov Ri ,m t t t 1 t 1 Rf E g f cov Ri ,f t t t 1 tt 1t 1 Rf  f. t i,f;t t 9.14 9.15 Two period CAPM. The classic use of Stein s lemma allows us to substitute a normal distribution assumption for the quadratic utility assumption in the two period CAPM. Repeating the analysis starting with equation 9.2 , using an arbitrary utility function, we have u c u RW Wt ct mt 1  t 1  t 1 . u ct u ct AssumingthatRW andRi arenormallydistributed,usingStein slemma,we have covRi ,m W c u RW W c E t t t 1 t t u ct covRi ,RW . t t 1 t 1 t t 1 t 1 Up to the technical assumption that the expectation exists, the trick is done. Log utility CAPM. Interestingly, Stein s lemma cannot be applied to the log utility CAPM because the market return cannot be normally distributed. It s easy to miss this fact, since in empirical applications we usually take the",
        "9.2. Intertemporal Capital Asset Pricing Model ICAPM 165 factor risk premium  in equation 9.15 as a free parameter. However, the term in equation 9.14 , applied to the log utility CAPM g f 1 R W is 1 If R W t 1 E Ri Rf E t 1 t t RW2 cov Ri ,RW . t t 1 t 1 t 1 is normally distributed, E 1 R W 2 does not exist. The Stein s lemma condition E g f is violated. This is not a little technical problem to be swept under the rug. If RW is normally distributed, it can range from to , including negative values. To get a non positive return on the wealth portfolio, price and con sumption must be non positive. Log utility and 1 c marginal utility blow up as consumption nears zero. A log utility consumer will never choose a consumption path with zero or negative consumption. The return on the wealth portfolio cannot be normally distributed in a log utility economy. This fact warns us not to apply the approximation derived from the continuous time model, 9.15 , to a long time horizon. We can derive a CAPM like relation for log returns, by assuming that the market and each asset return are jointly log normally distributed. The horizon should be short enough that the distinction between log and actual returns is small. Similarly, even though we can approximate a nonlinear discount factor model c c  or 1 RW by a linear discount factor model for short t 1 t t 1 time horizons, it would be a mistake to do so for longer time horizons, or to discount a stream of dividends. a bRW becomes a worse and worse approximation to 1 RW . In particular, the former can become negative while the latter does not. The point of Rubinstein 1976 , in fact, was not to derive a log utility CAPM, but to advocate the nonlinear model m 1 RW as a good way to use the CAPM for arbitrage free multiperiod discounting. 9.2 Intertemporal Capital Asset Pricing Model ICAPM The ICAPM generates linear discount factor models mt 1 a b ft 1 in which the factors are state variables for the investor s consumption portfolio decision. t 1 Any statevariable zt canbeafactor.TheICAPMisalinearfactormodel with wealth and state variables that forecast changes in the distribution of future returns or income.",
        "166 9. Factor Pricing Models The state variables are the variables that determine how well the investor can do in his maximization. Current wealth is obviously a state variable. Additional state variables describe the conditional distribution of asset returns the agent will face in the future or shifts in the investment opportunity set. In multiple good or international models, relative price changes are also state variables. Optimalconsumptionisafunctionofthestatevariables,ct g zt .We can use this fact once again to substitute out consumption, and write u g zt 1 mt 1  . From here, it is a simple linearization to deduce that the state variables zt 1 will be factors. Alternatively, the value function depends on the state variables V Wt 1, zt 1 , so we can write VW Wt 1, zt 1 VW Wt,zt mt 1  . u g zt The marginal value of a dollar must be the same in any use, so u ct VW Wt , zt . This completes the first step, naming the proxies. To obtain a linear relation, we can take a Taylor approximation, assume normality and use Stein s lemma, or, most conveniently, move to continuous time. Starting from we have t e tVW Wt,zt d t dt WtVWW Wt,zt dWt t VW Wt,zt Wt VWz Wt,zt dzt VW Wt,zt second derivative terms . The elasticity of marginal value with respect to wealth is often called the coefficient of relative risk aversion, rrat WVWW Wt , zt VW Wt,zt . It captures the investor s reluctance to take monetary or wealth bets.",
        "9.3. Comments on the CAPM and ICAPM 167 Substituting into the basic pricing equation, we obtain the ICAPM, which relates expected returns to the covariance of returns with wealth, and also with the other state variables, dpti Dti f E dt rtdt rratE From here, it is fairly straightforward to express the ICAPM in terms of betas rather than covariances, or as a linear discount factor model. Most empirical work occurs in discrete time; we often simply approximate the continuous time result as ERi Rf rracovRi ,W W covRi ,z . t t 1 t t t t 1 t 1 t zt t t 1 t 1 We can substitute covariance with the wealth portfolio in place of covariance with wealth shocks to the two are the same and we can use factor mimicking portfolios for the other factors dz as well. The factor mimicking portfolios are interesting for portfolio advice as well, as they give the purest way of hedging against or profiting from state variable risk exposure. This short presentation does not do justice to the beauty of Merton s portfolio theory and ICAPM. What remains is to actually state the consumer s problem and prove that the value function depends on W and z, the state variables for future investment opportunities, and that the optimal port folio holds the market and hedge portfolios for the investment opportunity variables. Working this out is not vacuous. For example, we saw that the log utility CAPM holds even with time varying investment opportunities. Thus the ICAPM will only work if the utility curvature parameter is not equal to one. 9.3 Comments on the CAPM and ICAPM A look at the formal derivations of the models should allow us to under stand their empirical application and to understand, if not settle, common controversies. dWt dpti VWz,t dpti Edzt pti pti Wtpti VW,t pti . Conditional vs. unconditional models. Do they price options? Why bother linearizing? The wealth portfolio. The implicit consumption based model, and ignored predictions. Portfolio intuition and recession state variables.",
        "168 9. Factor Pricing Models Is the CAPM Conditional or Unconditional? Is the CAPM a conditional or an unconditional factor model? That is, are the parameters a and b in m a bR W constants, or do they change at each time period, as conditioning information changes? We saw in Chapter 8 that a conditional CAPM does not imply an unconditional CAPM. If con ditional, additional steps must be taken to say anything about observed average returns and empirical failures might just come from conditioning information. The two period quadratic utility based derivation results in a conditional CAPM,sincetheparametersat andbt inequation 9.4 ,changeovertime. Also we know from equation 8.3 that at and bt must vary over time if the conditional moments of R W , R f vary over time. This two period investor chooses a portfolio on the conditional mean variance frontier, which is not on the unconditional frontier. The multiperiod quadratic utility CAPM only holds if returns are i.i.d. so it only holds if there is no difference between conditional and unconditional models. On the other hand, the log utility CAPM expressed with the inverse market return holds both conditionally and unconditionally. There are no free parameters that can change with conditioning information: 1 E1R 1 E1R. t RW t 1 RW t 1 t 1 t 1 However, it makes additional predictions that can be quickly rejected, which I will detail below. Furthermore, when we linearize the log utility CAPM, all the coefficients are again time varying. In sum, alternative assumptions give different answers. Whether the CAPM can be rescued by more careful treatment of conditioning informa tion remains an empirical question. Should the CAPM Price Options? You may hear the statement the CAPM is not designed to price derivative securities. This statement also depends on which derivation one has in mind. The quadratic utility CAPM and the log utility CAPM should apply to all payoffs: stocks, bonds, options, contingent claims, etc. Rubinstein 1976 shows that the log utility CAPM delivers the Black Scholes option pricing formula. However, if we assume normal return distributions to obtain a linear CAPM in discrete time, we can no longer hope to price options, since option returns are nonnormally distributed. Even the normal dis tribution for regular returns is a questionable assumption. Again, having looked at the derivations, we see that theory is not decisive on this point.",
        "9.3. Comments on the CAPM and ICAPM 169 Why Linearize? Why bother linearizing a model? Why take the log utility model m 1 R W whichshouldpriceanyasset,andturnitintom a bRW thatlosesthe t 1 t t t 1 clean conditioning down property, cannot price nonnormally distributed payoffs and must be applied at short horizons? The tricks were developed when it was hard to estimate nonlinear models. It is clear how to estimate a  and a  by regressions, but estimating nonlinear models used to be a big headache. Now, GMM has made it easy to estimate and evaluate nonlinear models. Thus, in my opinion, linearization is not that important anymore. If the nonlinear model makes important predictions or simplifications that are lost on linearizing, there is no reason to lose those features. What about the Wealth Portfolio? The log utility derivation makes clear just how expansive is the concept of the wealth portfolio. To own a share of the consumption stream, you have to own not only all stocks, but all bonds, real estate, privately held capital, publicly held capital roads, parks, etc. , and human capital a nice word for people. Clearly, the CAPM is a poor defense of common proxies such as the value weighted NYSE portfolio. And keep in mind that since it is easy to find ex post mean variance efficient portfolios of any subset of assets like stocks out there, taking the theory seriously is our only guard against fishing. Implicit Consumption Based Models Many users of alternative models clearly are motivated by a belief that the consumption based model does not work, no matter how well measured consumption might be. This view is not totally unreasonable; perhaps trans actions costs de link consumption and asset returns at high frequencies, and the perfect risk sharing behind the use of aggregate consumption has always seemed extreme. However, the derivations make clear that the CAPM and ICAPM are not alternatives to the consumption based model; they are special cases of that model. In each case mt 1 u ct 1 u ct still operates. We just added assumptions that allowed us to substitute other variables in place of ct . You cannot adopt the CAPM on the belief that the consumption based model is wrong. If you think the consumption based model is fundamen tally wrong, the economic justification for the alternative factor models evaporates as well. Now that we have seen the derivations, the only consistent motivation for factor models is a belief that consumption data are unsatisfactory. However, while asset return data are well measured, it is not obvious that the S P500 or other portfolio returns are terrific measures of the return to total wealth.",
        "170 9. Factor Pricing Models Macro factors used by Chen, Roll, and Ross 1986 and others are distant proxies for the quantities they want to measure, and macro factors based on other NIPA aggregates investment, output, etc. suffer from the same measurement problems as aggregate consumption. In large part, the better performance of the CAPM and ICAPM rel ative to consumption based models comes from throwing away content. Again, mt 1 u ct 1 u ct is there in any CAPM or ICAPM. The CAPM and ICAPM do make predictions concerning consumption data, and these predictions are often wildly implausible, not only of admittedly poorly mea sured aggregate consumption data but of any imaginable perfectly measured individual consumption data as well. For example, the log utility CAPM predicts RW 1ct 1. 9.16  ct Equation 9.16 implies that the standard deviation of the wealth port folio return equals the standard deviation of consumption growth. The standard deviation of stock returns is about 16 , that of measured con sumption growth is only 1 , and it is inconceivable that perfectly measured consumption varies 16 times more. Worse, equation 9.16 links consumption with returns ex post as well as ex ante. The wealth portfolio return is high, ex post, when consumption is high. This holds at every frequency: If stocks go up between 12:00 and 1:00, it must be because on average we all decided to have a big lunch. This seems silly. Aggregate consumption and asset returns are likely to be de linked at high frequencies, but how high quarterly? and by what mechanism are important questions to be answered. In any case, this is another implication of the log utility CAPM that is just thrown out. All of the models make further predictions, including the size of the factor risk premia , the magnitude of the risk free rate, or predictions about prices p c constant for the log utility CAPM that are conventionally ignored when testing them. In sum, the poor performance of the consumption based model is an important nut to chew on, not just a blind alley or failed attempt that we can safely disregard and go on about our business. Identity of State Variables The ICAPM does not tell us the identity of the state variables zt , and many authors use the ICAPM as an obligatory citation to theory on the way to using factors composed of ad hoc portfolios, leading Fama 1991 to character ize the ICAPM as a fishing license. The ICAPM really is not quite such an expansive license. One could do a lot to insist that the factor mimicking port folios actually are the projections of some identifiable state variables on the t 1",
        "9.3. Comments on the CAPM and ICAPM 171 space of returns, and one could do a lot to make sure the candidate state vari ables really are plausible state variables for an explicitly stated optimization problem. For example, one could check that investment opportunity set state variables actually do forecast something. The fishing license comes as much from habits of applying the theory as from the theory itself. On the other hand, the conventions of empirical work may be healthy. The CAPM and multiple factor models are obviously very artificial. Their central place really comes from a long string of empirical successes rather than theoretical purity. Perhaps researchers are wise to pick and choose implications of what are, after all, stylized quantitative parables. Portfolio Intuition and Recession State Variables I have derived the factor models as instances of the consumption based model, tricks for substituting consumption out of the discount factor. The more traditional portfolio approach to multifactor models gives a lot of useful intuition, which helps to explain why the CAPM and successor fac tor models have been so compelling for so long despite the artificiality of these formal derivations. The traditional intuition looks past consumption to think directly about its determinants in sources of income or news. Start at a portfolio R W . Now, think about changing the port folio adding  more R i and  less R f . This modification raises the mean portfolio return by E R i R f . This change also raises the variance of the portfolio return to 2 RW  Ri Rf 2 RW 2cov RW,Ri 2var Ri . Thus, for small  we re really taking a derivative here , this modification raises the portfolio variance by 2cov R W , R i . Thisisthecentralinsight.Thecovariance orbeta ofRi withRW measures how much a marginal increase in Ri affects the portfolio variance. I highlight portfolio variance. Modern asset pricing starts when we realize that investors care about portfolio returns, not about the behavior of specific assets. The benefit of the portfolio change is the increased portfolio mean return, E Ri Rf . The cost of the change is the increased portfolio variance, 2cov R W , R i . At an optimum, the cost benefit tradeoff must be the same for each asset. Thus, at an optimum, mean excess returns must be proportional to the covariance of returns with the investor s portfolio, or beta. The ICAPM adds long investment horizons and time varying investment opportunities to this picture. An investor with a long horizon and utility more curved than log is unhappy when news comes that future returns are lower, because his long term wealth or consumption will be lower. He",
        "Problems 183 utility basis seems not to have had much impact on practice. In practice, we just test models m b f and rarely worry about derivations. The best evidence for this view lies in the introductions of famous papers. Chen, Roll, and Ross 1986 describe one of the earliest popular multifactor mod els, using industrial production and inflation as some of the main factors. They do not even present a factor decomposition of test asset returns, or the time series regressions. A reader might well categorize the paper as a macroeconomic factor model or perhaps an ICAPM, but their introduction calls it an APT. Fama and French 1993 describe the currently most popular multifactor model, and their introduction describes it as an ICAPM in which the factors proxy for state variables. But the factors are portfolios of assets sorted on size and book market just like the test assets, the time series R2 are all above 90 , and much of the explanation involves common move ment in test assets captured by the factors. A reader might well categorize the model as much closer to an APT. In the preface, I made a distinction between relative pricing and absolute pricing. In the former, we price one security given the prices of others, while in the latter, we price each security by reference to fundamental sources of risk. The factor pricing stories are interesting in that they start with a nice absolute pricing model, the consumption based model, and throw out enough information to end up with relative models. The CAPM prices Ri given the market, but throws out the consumption based model s description of where the market return came from. Though the derivation says the CAPM should price any asset as an absolute model, everyone knows better than to test it, say, on options data. The APT is a true relative pricing model. It pretends to do no more than extend the prices of factor portfolios to nearby securities. Problems Chapter 9 1. Suppose the investor only has a one period horizon. He invests wealth W at date zero, and only consumes with expected utility Eu c Eu W in period 1. Derive the quadratic utility CAPM in this case. This is an even simpler derivation. The Lagrange multiplier on initial wealth W now becomes the denominator of m in place of u c0 . 2. Figure 9.1 suggests that m 0 is enough to establish a well behaved approximate APT. The text claims this is not true. Which is right? 3. In this problem, we ll explore the Arbitrage Pricing Theory. Re is a N dimensionalvectorofexcessreturns,Ref isaK dimensionalvectorofexcess",
        "21.2. New Models 469 differently in different states by specifying a square root type process rather than a simple AR 1 , 1 S 1 2 st s 1,  st S  . 21.10 21.11  1  The extra complication of 21.9 rather than 21.7 means consumption is always above habit, since S es 0. Other habit models in endowment economies can give consumption below habit which leads to infinite or imaginary marginal utility. St becomes the single state variable in this economy. Time varying expected returns, price dividend ratios, etc. are all functions of this state variable. Marginal utility is u C,X C X  S C . ctttttt The model assumes an external habit each individual s habit is determined by everyone else s consumption. This simplification, allows us to ignore terms by which current consumption affect future habits. With marginal utility, we now have a discount factor:  Since we have a stochastic process for S and C , and each is lognormal, we can evaluate the conditional mean of the discount factor to evaluate the risk free rate, uc Ct 1,Xt 1 St 1 Ct 1 Mt 1   . uc Ct,Xt St Ct 1 rf lnE M ln  g  1  . 21.12 ttt 1 2 We gave up on analytic solutions and evaluated the price dividend ratio as a function of the state variable by iteration on a grid: Pt s E M Ct 1 1 Pt 1 s . Ct t t 1C C t 1 t tt 1 With price dividend ratios, we can calculate returns, expected returns, etc.",
        "21.2. New Models 475 Now, yt 1 is specified so that people suffer a high cross sectional variance of consumption growth on dates of a low market return Rt 1, C 2 yt 1  ln it 1 Rt 1  lnRt 1. Cit   1 21.14 Given this structure, the individual is exactly happy to consume Cit withoutfurthertradinginthestock. WecancallCit incomeIit,andprove the optimal decision rule is to consume income Cit Iit . His first order condition for an optimal consumption portfolio decision 1 Eexp   y 1y2 lnR . t it 1 t 1 2 t 1 t 1 Since  is independent of everything else, we can use E f y E E f y y . Now, with  normal 0,1 ,  Rt 1 To prove this assertion, just substitute in for Cit 1 Cit and take the t 2 t 1 2 t 1 t 1 1 Et e  Cit 1 Cit holds, exactly. expectation: 122 E exp it 1yt 1 yt 1 exp  yt 1 . 2 Therefore, we have 1 E exp  12y2 1y2 lnR . Substituting in from 21.14 , 12 1 Etexp    1  lnRt 1 lnRt 1 2   1 1! The General Model In the general model, Constantinides and Duffie define yt 1 2 lnmt 1  lnCt 1,   1 Ct 21.15",
        "Problems 485 max E t 1  jCt j, Problems Chapter 21 1. Suppose habit accumulation is linear, and there is a constant risk free rate or linear technology equal to the discount rate, Rf 1 . The consumer s problem is then tet W0, Ct Xt 1  s.t.E tCt E t 0 t t Xt  j 1 where et is a stochastic endowment. In an internal habit specification, the consumer considers all the effects that current consumption has on future utility through Xt j . In an external habit specification, the consumer ignores such terms. Show that the two specifications give identical asset pricing predictions in this simple model, by showing that internal habit marginal utility is proportional to external habit marginal utility, state by state. 2. Suppose a consumer has quadratic utility with a constant interest rate equal to the subjective discount rate, but a habit or durable consumption good, so that utility is u ct ct 1 1 c ct ct 1 2. 2 Show that external habit persistence  0 implies positive serial correlation in consumption changes. Show that the same solution holds for internal habits, or durability. Show that durability leads to negative serial correlation in consumption changes. 3. Many models predict too much variation in the conditional mean dis count factor, or too much interest rate variation. This problem guides you through a simple example. Introduce a simple form of external habit formation, u Ct Ct 1 1  , and suppose consumption growth Ct 1 Ct is i.i.d. Show that interest rates still vary despite i.i.d. consumption growth. 4. We showed that if m satisfies the Hansen Jagannathan bound, then proj m X should also do so. Hansen and Jagannathan also compute bounds",
        "486 21. Equity Premium Puzzle and Consumption Based Models with positivity, solutions to min m s.t.p E mx , m 0, E m . Does proj m X also lie in the same bound? 5. One most often compares consumption based models to Hansen Jagannathan bounds. Can you compare the CAPM discount factor m a bRem tothebound?Totheboundwithpositivity?"
    ],
    "Topic 5": [
        "viii Contents 4.3 An Alternative Formula, and x in Continuous Time . . . .. 72 Problems....................................... .. 75 5 Mean Variance Frontier and Beta Representations 77 5.1 ExpectedReturn BetaRepresentations ................ 78 5.2 Mean Variance Frontier: Intuition and Lagrangian Characterization................................... 81 5.3 An Orthogonal Characterization of the Mean Variance Frontier.......................................... 84 5.4 SpanningtheMean VarianceFrontier................. 88 5.5 ACompilationofPropertiesofR ,Re ,andx .......... 89 5.6 Mean Variance Frontiers for Discount Factors: The Hansen JagannathanBounds........................ 92 Problems......................................... 97 6 Relation between Discount Factors, Betas, and Mean Variance Frontiers 99 6.1 From Discount Factors to Beta Representations . . . . . . . . . 100 6.2 From Mean Variance Frontier to a Discount Factor and BetaRepresentation................................ 103 6.3 FactorModelsandDiscountFactors .................. 106 6.4 Discount Factors and Beta Models to Mean Variance Frontier.......................................... 110 6.5 ThreeRisk FreeRateAnalogues...................... 111 6.6 Mean Variance Special Cases with No Risk Free Rate . . . . . 117 Problems......................................... 120 7 Implications of Existence and Equivalence Theorems 121 8 Conditioning Information 131 8.1 ScaledPayoffs..................................... 132 8.2 SufficiencyofAddingScaledReturns.................. 134 8.3 ConditionalandUnconditionalModels ............... 136 8.4 ScaledFactors:APartialSolution..................... 144 8.5 Summary......................................... 145 Problems......................................... 146 9 Factor Pricing Models 149 9.1 CapitalAssetPricingModel CAPM .................. 152 9.2 Intertemporal Capital Asset Pricing Model ICAPM . . . . . 165 9.3 CommentsontheCAPMandICAPM ................. 167 9.4 ArbitragePricingTheory APT ...................... 173",
        "18 1. Consumption Based Model and Overview Correlation coefficients cannot be greater than 1 in magnitude, leading to 1.17 . This simple calculation has many interesting and classic implications. 1. Meansandvariancesofassetreturnsmustlieinthewedge shapedregion illustrated in Figure 1.1. The boundary of the mean variance region in which assets can lie is called the mean variance frontier. It answers a nat urally interesting question, how much mean return can you get for a given level of variance? 2. All returns on the frontier are perfectly correlated with the discount factor: the frontier is generated by  i 1. Returns on the upper m,R part of the frontier are perfectly negatively correlated with the discount factor and hence positively correlated with consumption. They are max imally risky and thus get the highest expected returns. Returns on the lower part of the frontier are perfectly positively correlated with the dis count factor and hence negatively correlated with consumption. They thus provide the best insurance against consumption fluctuations. 3. We can go beyond perfect correlation. Consider a payoff m E m2 . Its price is E m2 E m2 1, so it is a return. It is on the mean variance frontier. Thus, if we know m, we can construct a mean variance effi cient return. We will expand on this theme in Chapter 5, in an explicitly incomplete market. 4. All frontier returns are also perfectly correlated with each other, since they are all perfectly correlated with the discount factor. This fact implies that we can span or synthesize any frontier return from two such returns. For example, if you pick any single frontier return R m , then all frontier Figure 1.1. Mean variance frontier. The mean and standard deviation of all assets priced by a discount factor m must lie in the wedge shaped region.",
        "1.4. Classic Issues in Finance returns Rmv must be expressible as 19 Rmv Rf a Rm Rf for some number a. 5. Since each point on the mean variance frontier is perfectly correlated with the discount factor, we must be able to pick constants a , b , d , e such that m a bR mv , Rmv d em. Thus, any mean variance efficient return carries all pricing information. Given a mean variance efficient return and the risk free rate, we can find a discount factor that prices all assets and vice versa. 6. Given a discount factor, we can also construct a single beta representa tion, so expected returns can be described in a single beta representation using any mean variance efficient return except the risk free rate , E Ri Rf i,mv E Rmv Rf . The essence of the beta pricing model is that, even though the means and standard deviations of returns fill out the space inside the mean variance frontier, a graph of mean returns versus betas should yield a straight line. Since the beta model applies to every return including Rmv itself, and Rmv has a beta of 1 on itself, we can identify the factor risk premiumas E Rmv Rf . The last two points suggest an intimate relationship between discount factors, beta models, and mean variance frontiers. I explore this relation in detail in Chapter 6. A problem at the end of this chapter guides you through the algebra to demonstrate points 5 and 6 explicitly. 7. Wecanplotthedecompositionofareturnintoa priced or systematic component and a residual, or idiosyncratic component as shown in Figure 1.1. The priced part is perfectly correlated with the discount factor, and hence perfectly correlated with any frontier return. The residual or idiosyncratic part generates no expected return, so it lies flat as shown in the figure, and it is uncorrelated with the discount factor or any frontier return. Assets inside the frontier or even on the lower portion of the frontier are not worse than assets on the frontier. The frontier and its internal region characterize equilibrium asset returns, with rational investors happy to hold all assets. You would not want to put your whole portfolio in one inefficient asset, but you are happy to put some wealth in such assets.",
        "4.3. An Alternative Formula, and x in Continuous Time 73 a convenient alternative formula is where x E x p E x E x 1 x E x , 4.2 E x E x x E x denotes the covariance matrix of the x payoffs. We could just substitute E xx E x E x in 4.1 , but the inverse of this sum is not very useful. We can derive this formula by postulating a discount factor that is a linear function of the shocks to the payoffs, x E x x E x b, and then finding b to ensure that x p E xx E x E x Ex x Ex b, prices the assets x: so If a risk free rate is traded, then we know E x 1 Rf . If a risk free b 1 p E x E x . rate is not traded if 1 is not in X then this formula does not necessarily risk free or zero beta rate is not a problem. This formula is particularly useful when the payoff space consists solely of excess returns or price zero payoffs. In that case, x p E xx 1x gives x 0. x 0 is in fact the only discount factor in X that prices all the assets, but in this case it is more interesting and avoids 1 0 difficulties when we want to transform to expected return beta or other representations to pick a discount factor not in X by picking a zero beta rate or price of the risk free payoff. In the case of excess returns, for arbitrarily chosen Rf , then, 4.2 gives us x 1 1E Re 1 Re E Re ; cov Re . Rf Rf This approach is due to Hansen and Jagannathan 1991 . Continuous Time The law of one price implies the existence of a discount factor process, and absence of arbitrage implies a positive discount factor process in continuous time as well as discrete time. At one level, this statement requires no new that is in X . In many applications, however, all that matters is producing some discount factor, and the arbitrariness of the produce a discount factor x",
        "5 Mean Variance Frontier and Beta Representations Much empirical work in asset pricing is written in terms of expected return beta representations and mean variance frontiers. This chapter introduces expected return beta representations and mean variance frontiers. I discuss here the beta representation of factor pricing models. Chapters 6 and 9 discuss where the factor models came from. Chapter 6 shows how an expected return beta model is equivalent to a linear model for the discount factor, i.e., m b f . Chapter 9 discusses the derivation of popular factor models such as the CAPM, ICAPM, and APT, i.e., under what assumptions thediscountfactorisalinearfunctionofothervariablesf suchasthemarket return. I summarize the classic Lagrangian approach to the mean variance frontier. I then introduce a powerful and useful representation of the mean variance frontier due to Hansen and Richard 1987 . This representation uses the state space geometry familiar from the existence theorems. It is also useful because it is valid in infinite dimensional payoff spaces, which we shall soon encounter when we add conditioning information, dynamic trading, or options. 77",
        "5.2. Mean Variance Frontier: Intuition and Lagrangian Characterization 81 5.2 Mean Variance Frontier: Intuition and Lagrangian Characterization The mean variance frontier of a given set of assets is the boundary of the set of means and variances of the returns on all portfolios of the given assets. One can find or define this boundary by minimizing return variance for a given mean return. Many asset pricing propositions and test statistics have interpretations in terms of the mean variance frontier. Figure 5.1 displays a typical mean variance frontier. As displayed in Figure 5.1, it is common to distinguish the mean variance frontier of all risky assets, graphed as the hyperbolic region, and the mean variance fron tier of all assets, i.e., including a risk free rate if there is one, which is the larger wedge shaped region. Some authors reserve the terminology mean variance frontier for the upper portion, calling the whole thing the minimum variance frontier. The risky asset frontier lies between two asymp totes, shown as dotted lines. The risk free rate is typically drawn below the intersection of the asymptotes and the vertical axis, or the point of mini mum variance on the risky frontier. If it were above this point, investors with a mean variance objective would try to short the risky assets, which cannot represent an equilibrium. In general, portfolios of two assets fill out a hyperbolic curve through the two assets. The curve is sharper the less correlated are the two assets, because the portfolio variance benefits from increasing diversification. Portfolios of a risky asset and risk free rate give rise to straight lines in mean standard deviation space. Figure 5.1. Mean variance frontier.",
        "82 5. Mean Variance Frontier and Beta Representations In Chapter 1, we derived a similar wedge shaped region as the set of means and variances of all assets that are priced by a given discount factor. This chapter is about incomplete markets, so we think of a mean variance frontier generated by a given set of assets, typically less than complete. When does the mean variance frontier exist? That is, when is the set of portfolio means and variances less than the whole E ,  space? We basically have to rule out a special case: two returns are perfectly correlated but have different means. In that case one could short one, buy the other, and achieve infinite expected returns with no risk. More formally, eliminate purely redundant securities from consideration, then Theorem: So long as the variance covariance matrix of returns is nonsingular, there is a mean variance frontier. To prove this theorem, just follow the construction below. This theorem should sound very familiar: Two perfectly correlated returns with different mean are a violation of the law of one price. Thus, the law of one price implies that there is a mean variance frontier as well as a discount factor. Lagrangian Approach to Mean Variance Frontier The standard definition and the computation of the mean variance frontier follow a brute force approach. Problem: Start with a vector of asset returns R . Denote by E the vector of mean returns, E E R , and denote by the variance covariance matrix E R E R E . A portfolio is defined by its weights w on the min w w w s.t. w E ; w 1 1. 5.6 Solution: Let A E 1E; B E 11; C 1 11. Then, for a given mean portfolio return , the minimum variance portfolio initial securities. The portfolio return is w R where the weights sum to one, w 1 1. The problem choose a portfolio to minimize variance for a given mean is then has variance varRp and is formed by portfolio weights C2 2B A 5.7 1 E C B 1 A B w . AC B2 AC B2",
        "5.2. Mean Variance Frontier: Intuition and Lagrangian Characterization 83 Equation 5.7 shows that the variance is a quadratic function of the mean. The square root of a parabola is a hyperbola, which is why we draw hyperbolic regions in mean standard deviation space. The minimum variance portfolio is interesting in its own right. It appears as a special case in many theorems and it appears in several test statistics. We canfinditbyminimizing 5.7 over,givingminvar B C.Theweightsof the minimum variance portfolio are thus 1 C , or w 11 1 11 . We can get to any point on the mean variance frontier by starting with two returns on the frontier and forming portfolios. The frontier is spanned by any two frontier returns. To see this fact, notice that w is a linear function of . Thus, if you take the portfolios corresponding to any two distinct mean returns 1 and 2, the weights on a third portfolio with mean 3 1 1  2 aregivenbyw3 w1 1  w2. Derivation: To derive the solution, introduce Lagrange multipliers 2 and 2 on the constraints. The first order conditions to 5.6 are then w E 1 0, w 1 E 1 . We find the Lagrange multipliers from the constraints, E w E 1 E 1 , 5.8 1 w 1 1 E 1 1, E 1E 1 1E AC B2 Plugging in to 5.8 , we get the portfolio weights and variance. or Hence, E 11   , 1 11  1 AB . BC1  C B, AC B2 A B  .",
        "84 5. Mean Variance Frontier and Beta Representations 5.3 An Orthogonal Characterization of the Mean Variance Frontier The Lagrangian approach to the mean variance frontier is straightfor ward but cumbersome. Our further manipulations will be easier if we follow an alternative approach due to Hansen and Richard 1987 . Technically, Hansen and Richard s approach is also valid when we cannot generate the EveryreturncanbeexpressedasRi R wiRe ni. The mean variance frontier is Rmv R wRe . R is defined as x p x . It is a return that represents prices. R e is defined as R e proj 1 R e . It represents mean excess returns, E Re E Re Re Re Re. payoff space by portfolios of a finite set of basis payoffs c x . This happens, for example, when we think about conditioning information in Chapter 8. Also, it is the natural geometric way to think about the mean variance frontier given that we have started to think of payoffs, discount factors, and other ran dom variables as vectors in the space of payoffs. Rather than write portfolios as combinations of basis assets, and pose and solve a minimization problem, we first describe any return by a three way orthogonal decomposition. The mean variance frontier then pops out easily without any algebra. Definitions of R , Re I start by defining two special returns. R is the return corresponding to the payoffx thatcanactasthediscountfactor.Thepriceofx is,likeanyother price, p x E x x . Thus, The definition of R is R x x . 5.9 5.10 p x E x 2 The definition of Re is Re proj 1 Re , Re space of excess returns x X s.t.p x 0 . Why Re ? We are heading towards a mean variance frontier, so it is natural to seek a special return that changes means. Re is an excess return that represents means on Re with an inner product in the same way that",
        "5.3. An Orthogonal Characterization of the Mean Variance Frontier 85 x is a payoff in X that represents prices with an inner product. As p x E mx E proj m X x E x x , so E Re E 1 Re E proj 1 Re Re E Re Re . If R and Re are still a bit mysterious at this point, they will make more sense as we use them, and discover their many interesting properties. Now we can state a beautiful orthogonal decomposition. Theorem: Every return Ri can be expressed as Ri R wiRe ni, where wi is a number, and ni is an excess return with the property E ni 0. The three components are orthogonal, E R Re E R ni E Re ni 0. This theorem quickly implies the characterization of the mean variance frontier which we are after: Theorem:Rmv isonthemean variancefrontierifandonlyif Rmv R wRe 5.11 for some real number w. As you vary the number w, you sweep out the mean variance frontier. E Re 0, so adding more w changes the mean and variance of Rmv unless the market is risk neutral, in which case Re 0 and the frontier collapses to a point . You can interpret 5.11 as a two fund theorem for the mean variance frontier. It expresses every frontier return as a portfolio of R Re , with varying weights on the latter. and As usual, first I will argue why the theorems are sensible, then I will offer a simple algebraic proof. Hansen and Richard 1987 give a much more careful algebraic proof. Graphical Construction Figure 5.2 illustrates the decomposition. Start at the origin 0 . Recall that the x vector is perpendicular to planes of constant price; thus the R vector lies perpendicular to the plane of returns as shown. Go to R .",
        "86 5. Mean Variance Frontier and Beta Representations Figure 5.2. Orthogonal decomposition and mean variance frontier. R e is the excess return that is closest to the vector 1; it lies at right angles to planes in R e of constant mean return, shown in the E 0, E 1 lines, Nowmove,againinanorthogonaldirection,byanamountni togetto the return Ri. We have thus expressed Ri R wiRe ni in a way that all three components are orthogonal. Returns with n 0, R wRe , are the mean variance frontier. Since E R2 2 R E R 2, we can define the mean variance frontier by min imizing second moment for a given mean. The length of each vector in Figure 5.2 is its second moment, so we want the shortest vector that is on the return plane for a given mean. The shortest vectors in the return plane with given mean are on the R wRe line. The graph also shows how Re represents means in the space of excess returns. Expectation is the inner product with 1. Planes of constant expected value in Figure 5.2 are perpendicular to the 1 vector, just as planes of constant side and no constant. Planes perpendicular to R e in R e are payoffs with just as the return R Re is an excess return, it is orthogonal to R . Proceed an amount wi in the direction of Re , getting as close to Ri as possible. lies at right angles to planes of constant price. Since price are perpendicular to the x or R vectors. I do not show the full extent of the constant expected payoff planes for clarity; I do show lines of constant expected excess return in R e , which are the intersection of constant expected payoff planes with the Re plane. Therefore, just as we found an x in X to represent prices in X by projecting m onto X, we find Re in Re by projecting 1 onto R e . Yes, this is a regression with one on the left hand constant mean, just as planes perpendicular to x same price. in X are payoffs with the",
        "5.3. An Orthogonal Characterization of the Mean Variance Frontier 87 Algebraic Argument Now, I present an algebraic proof of the decomposition and characterization of mean variance frontier. The algebra just represents statements about what is at right angles to what with second moments. Proof: Straightfromtheirdefinitions, 5.9 and 5.10 ,weknowthatRe is an excess return price zero , and hence that R and Re are orthogonal, 0. We define ni so that the decomposition adds up to Ri as claimed, and we define wi to make sure that ni is orthogonal to the other two components. Then we prove that E ni 0. Pick any wi and then define ni Ri R wiRe . ni is an excess return so already orthogonal to R , E R ni 0. To show E ni 0 and ni orthogonal to Re , we exploit the fact that since ni is an excess return, E ni E Re ni . Therefore,Re isorthogonaltoni ifandonlyifwepickwi sothatE ni 0. We do not have to explicitly calculate wi for the proof.2 Once we have constructed the decomposition, the frontier drops out. Since E ni 0 and the three components are orthogonal, E Ri E R wiE Re , 2 Ri 2 R wiRe 2 ni . Thus, for each desired value of the mean return, there is a unique wi. Returnswithni 0minimizevarianceforeachmean. P Decomposition in Mean Variance Space Figure 5.3 illustrates the decomposition in mean variance space rather than in state space. e E x Re E RR E x 2 2 Its value is not particularly enlightening. wi E Ri E R E Re",
        "88 5. Mean Variance Frontier and Beta Representations Figure 5.3. Orthogonal decomposition of a return Ri in mean standard deviation space. First, let us locate R . R is the minimum second moment return. One is the return closest to the origin, and thus the return with the smallest length which is sec can see this fact from the geometry of Figure 5.2: R ond moment. As with OLS regression, minimizing the length of R In mean standard deviation space, lines of constant second moment are and orthogonal to all excess returns are the same thing. One can also verify this property algebraically. Since any return can be expressed as R R wRe n,E R2 E R 2 w2E Re 2 E n2 .n 0andw 0 creating an R thus give the minimum second moment return. circles. Thus, the minimum second moment return R circle that intersects the set of all assets, which lie in the hyperbolic mean the most interesting return on the frontier! R or wealth portfolio, which typically lie on the upper portion of the frontier. Adding more Re moves one along the frontier. Adding n does not change mean but does change variance, so it is an idiosyncratic return that just moves an asset off the frontier as graphed. 5.4 Spanning the Mean Variance Frontier The characterization of the mean variance frontier in terms of R and Re is most natural in our setup. However, you can equivalently span the mean variance frontier with any two portfolios that are on the frontier any two distinct linear combinations of R and Re . In particular, take any return R R Re ,  0. 5.12 variance frontier. Notice that R the mean variance frontier. It is initially surprising that this is the location of is on the lower, or inefficient segment of is not the market portfolio is on the smallest",
        "5.5. A Compilation of Properties of R , Re , and x Using this return in place of Re , e R R R ,   you can express the mean variance frontier in terms of R and R : 89 5.13 R wRe R y R R 1 y R yR, where I have defined a new weight y w . The most common alternative approach is to use a risk free rate or a risky rate that somehow behaves like the risk free rate in place of Re to span the frontier. When there is a risk free rate, it is on the frontier with representation Rf R RfRe . I derive this expression in equation 5.20 below. Therefore, we can use 5.13 with R Rf . When there is no risk free rate, several risky returns that retain some properties of the risk free rate are often used. In Section 6.5 mimicking portfolio return, which is the return on the traded payoff closest to unity, R proj 1 X p proj 1 X , and the minimum variance return. Each of these returns is on the mean variance frontier, with form 5.12 , though with different values of  . Therefore, we can span the mean variance I present a zero beta return, which is uncorrelated with R , a constant frontier with R and any of these risk free rate proxies. 5.5 A Compilation of Properties of R , Re , and x The special returns R , Re that generate the mean variance frontier have lots of interesting and useful properties. Some I derived above, some I will derive and discuss below in more detail, and some will be useful tricks later on. Most properties and derivations are extremely obscure if you do not look at the pictures! 1 E R 2 1 . 5.14 E x 2 Toderivethisfact,multiplybothsidesofthedefinitionR x E x 2 by R , take expectations, and remember R is a return so 1 E x R . 2 We can reverse the definition and recover x from R via x R . 5.15 E R 2",
        "90 5. Mean Variance Frontier and Beta Representations To derive this formula, start with the definition R x E x 2 and substitute from 5.14 for E x 2 . 3 R can be used to represent prices just like x . This is not surprising, since they both point in the same direction, orthogonal to planes of constant price. Most obviously, from 5.15 , p x E x x For returns, we can nicely express this result as E R x x X. E R 2 E R 2 E R R R R. 5.16 5.17 5.18 If not, this gives a zero beta rate interpretation of the right hand expression. You can also derive this formula by applying 5.16 to R f . 6 Re and R are orthogonal, E R Re 0. is orthogonal to any excess return. This fact can also serve as an alternative defining property of R . 4 R e represents means on R e via an inner product in the same way that x represents prices on X via an inner product. R e is orthogonal to planes of constant mean in Re as x is orthogonal to planes of constant price. Algebraically, in analogy to p x E x x , we have This fact can serve as an alternative defining property of Re . E Re E Re Re Re Re. 5 If a risk free rate is traded, we can construct R f from R via f 1 E R 2 R . E x E R We proved this in Section 5.3; E R2 E R wRe n 2 E R 2 w2E Re 2 E n2 , and E n 0, so set n to zero. The conditional mean variance frontier allows w in the conditioning infor mation set. The unconditional mean variance frontier requires w to equal a constant Chapter 8 . More generally, R 7 The mean variance frontier is given by Rmv R wRe . 8 R is the minimum second moment return. Graphically, R is the return closest to the origin. To see this, use the decomposition in 7 , and set w2 and n to zero to minimize second moment Figure 5.3 .",
        "5.5. A Compilation of Properties of R , Re , and x 91 9 Re has the same first and second moment, E Re E Re 2 . Just apply fact 5.17 to Re itself. Therefore, var Re E Re 2 E Re 2 E Re 1 E Re . 10 If there is a risk free rate, then Re can also be defined as the residual in the projection of 1 on R : Re 1 proj 1 R 1 E R E R 2 1 R 1 R . 5.19 Rf Figure 5.2 makes the first equality obvious. To prove it analytically, note that since R and R e are orthogonal and together span X , 1 proj 1 Re proj 1 R Re proj 1 R . The last equality comes from equation 5.18 . You can also verify 5.19 analytically. Check that Re so defined is an excess return in X its price is zero and E Re Re E Re ; E R Re 0. 11 As a result of 5.19 , R f has the decomposition Rf R RfRe . 5.20 Since Rf 1 typically, this means that R Re is located on the lower portion of the mean variance frontier in mean variance space, just a bit to the right of R f . If the risk free rate were one, then the unit vector would lie in the return space, and we would have Rf R Re . Typically, the space of returns is a little bit above the unit vector. As you stretch the unit vector by the amount R f to arrive at the return R f , so you stretch the amount Re that you add to R to get to Rf . 12 If there is no risk free rate, then we can use proj 1 X proj proj 1 X Re proj proj 1 X R proj 1 Re proj 1 R to deduce an analogue to equation 5.19 , Re proj 1 X proj 1 R proj 1 X 13 Since we have a formula x p E xx 1x for constructing x from x p E xx 1x R . E R E R 2 R . 5.21 from basis assets see Section 4.1 , we can construct R in this case p x p E xx 1p p x E x x leading to the denominator.",
        "92 14 5. Mean Variance Frontier and Beta Representations We can construct Re from a set of basis assets as well. Following the definition to project one on the space of excess returns, Re E Re E ReRe 1Re, where Re is the vector of basis excess returns. You can always use Re R R to form excess returns. This construction obviously mirrors the way we constructed x in Section 4.1 as x p E xx 1x, and you can see the similarity in the result, with E in place of p, since Re represents means rather than prices. If there is a risk free rate, we can also use 5.19 to construct Re : Re 1 1 R 1 1 p E xx 1x. 5.22 Rf Rf p E xx 1p If there is no risk free rate, we can use 5.21 to construct Re . The central ingredient is proj 1 X E x E xx 1x. 5.6 Mean Variance Frontiers for Discount Factors: The Hansen Jagannathan Bounds The mean variance frontier of all discount factors that price a given set of assets is related to the mean variance frontier of asset returns by and hence  m E Re , E m  Re  m E Re min max . all m that price x X E m all excess returns Re in X  Re The discount factors on the frontier can be characterized analogously to the mean variance frontier of asset returns, m x we , e 1 proj 1 X proj 1 E 1 E x E xx 1x, E m x .",
        "5.6. Mean Variance Frontiers for Discount Factors 93 We derived in Chapter 1 a relation between the Sharpe ratio of an excess return and the volatility of discount factors necessary to price that return,  m E Re . 5.23 Quickly, E m  Re 0 E mR e E m E R e m , R e  m  R e , and  1. If we had a risk free rate, then we know in addition E m 1 Rf . Hansen and Jagannathan 1991 had the brilliant insight to read this equation as a restriction on the set of discount factors that can price a given set of returns, as well as a restriction on the set of returns we will see given a specific discount factor. This calculation teaches us that we need very volatile discount factors with a mean near one to understand stock returns. This and more general related calculations turn out to be a central tool in understanding and surmounting the equity premium puzzle, surveyed in Chapter 21. We would like to derive a bound that uses a large number of assets, and that is valid if there is no risk free rate. What is the set of E m , m consistent with a given set of asset prices and payoffs? What is the mean variance frontier for discount factors? Obviously from 5.23 the higher the Sharpe ratio, the tighter the bound on  m . This suggests a way to construct the frontier we are after. For any hypothetical risk free rate, find the highest Sharpe ratio. That is, of course, the tangency portfolio. Then the slope to the tangency portfolio gives the ratio  m E m . Figure 5.4 illustrates. As we increase 1 E m , the slope to the tangency becomes lower, and the Hansen Jagannathan bound declines. At the mean return corresponding Figure 5.4. Graphical construction of the Hansen Jagannathan bound.",
        "94 5. Mean Variance Frontier and Beta Representations to the minimum variance point, the HJ bound attains its minimum. As we increase 1 E m further, the tangency point touches the lower part of the frontier, the Sharpe ratio rises again, and so does the bound. If there were a risk free rate, then we know E m , the return frontier is a V shape, and the HJ bound is purely a bound on variance. This discussion implies a beautiful duality between discount factor volatility and Sharpe ratios:  m E Re min max . 5.24 all m that price x X E m all excess returns Re in X  Re We need formulas for an explicit calculation. Following the same logic we used to derive equation 4.2 , we can find a representation for the set of discount factors that price a given set of asset returns that satisfy p E mx : m E m p E m E x 1 x E x , 5.25 where cov x,x and E  0, E x 0. You can think of this as a regression or projection of any discount factor on the space of payoffs, plus an error. Since 2  0, this representation leads immediately to an explicit expression for the Hansen Jagannathan bound, 2 m p E m E x 1 p E m E x . 5.26 As all asset returns must lie in a hyperbolic region in E R ,  R space, all discount factors must lie in a hyperbolic region in E m , m space, as illustrated in the right hand panel of Figure 5.4. We would like an expression for the discount factors on the bound, as we wanted an expression for the returns on the mean variance frontier instead of just a formula for the means and variances. As we found a three way decomposition of all returns, in which two elements generated the mean variance frontier of returns, so we can find a three way decomposition of discount factors, in which two elements generate the mean variance frontier of discount factors 5.26 . I illustrate the construction in Figure 5.5. Any discount factor m must lie in the plane marked M , perpendicular toX throughx .Anym mustbeoftheform m x we n. Here, I have just broken up the residual  in the familiar representation m x  into two components. e is defined as the residual from the projection of 1 onto X or, equivalently the projection of 1 on the space E of excess m s, random variables of the form m x . e 1 proj 1 X proj 1 E .",
        "5.6. Mean Variance Frontiers for Discount Factors 95 Figure 5.5. Decomposition of any discount factor m x we n. e generates means of m just as Re did for returns: E m x E 1 m x E proj 1 E m x . Finally n, defined as the leftovers, has mean zero since it is orthogonal to 1 and is orthogonal to X . As with returns, then, the mean variance frontier of discount factors is given by m x we . 5.27 If the unit payoff is in the payoff space, then we know E m , and the frontier and bound are just m x , 2 m 2 x . This is exactly like the case of risk neutrality for return mean variance frontiers, in which the the Hansen Jagannathan bound for the finite dimensional cases discussed above. It is more general, since it can be used in infinite dimensional payoff spaces as well. Along with the corresponding return formula Rmv R wRe ,weseeinChapter8thatitextendsmoreeasilytothecalcu lation of conditional versus unconditional mean variance frontiers Gallant, Hansen, and Tauchen 1990 . It will make construction 5.27 come alive if we find equations for its frontier reduces to the single point R . The construction 5.27 can be used to derive the formula 5.26 for components. We find x as before; it is the portfolio c x in X that prices x: x p E xx 1x.",
        "96 5. Mean Variance Frontier and Beta Representations After a while you get used to the idea of running regressions with 1 on the left hand side and random variables on the right hand side! Thus, e 1 E x E xx 1x. Similarly, let us find e . The projection of 1 on X is proj 1 X E x E xx 1x. Again, you can construct time series of x and e from these definitions. Finally, we now can construct the variance minimizing discount factors m x we p E xx 1x w 1 E x E xx 1x m w p wE x E xx 1x. 5.28 2 m p wE x cov xx 1 p wE x . As you can see, Hansen Jagannathan frontiers are equivalent to mean variance frontiers. For example, an obvious exercise is to see how much the addition of assets raises the Hansen Jagannathan bound. This is exactly the same as asking how much those assets expand the mean variance fron tier. Chen and Knez 1996 and De Santis 1993 test for mean variance efficiency using Hansen Jagannathan bounds. Hansen Jagannathan bounds have the potential to do more than mean variance frontiers. Hansen and Jagannathan show how to solve the problem min2 m s.t. p E mx , m 0, E m fixed. This is the Hansen Jagannathan bound with positivity. It is strictly tighter than the Hansen Jagannathan bound since there is an extra restriction. It allows you to impose no arbitrage conditions. In stock applications, this extra bound ended up not being that informative. However, in the option application of this idea of Chapter 18, positivity is really important. That chapter shows how to solve for a bound with positivity. Hansen, Heaton, and Luttmer 1995 develop a distribution theory for the bounds. Luttmer 1996 develops bounds with market frictions such as short sales constraints and bid ask spreads, to account for ludi crously high apparent Sharpe ratios and bounds in short term bond data. or on the frontier with varying E m w p wE x E xx 1E x , As w varies, we trace out discount factors m means and variances.",
        "Problems 97 Cochrane and Hansen 1992 survey a variety of bounds, including bounds that incorporate information that discount factors are poorly correlated with stock returns the HJ bounds use the extreme  1 , bounds on condi tional moments that illustrate how many models imply excessive interest rate variation, bounds with short sales constraints and market frictions, etc. Chapter 21 discusses the results of Hansen Jagannathan bound calcu lations and what they mean for discount factors that can price stock and bond return data. Problems Chapter 5 1. Prove that Re lies at right angles to planes in Re of constant mean return, as shown in Figure 5.2. Hint: Check whether p x is greater or less than 1. 3. Show that if there is a risk free rate if the unit payoff is in the payoff spaceX thenRe Rf R Rf. 4. If no risk free rate is traded, can you construct Re from knowledge of risk neutral? a If a risk free rate is traded? b If no risk free rate is traded? Hint: make a drawing or think about the case that payoffs are generated by an N dimensional vector of basis assets x . 6. x proj m X . Is R proj m R ? 7. Prove 5.24 . To do this, you have to find returns and m for which the inequality is tight. Do the case with a risk free rate first. Then try it with no risk free rate. 2. Should we draw x a risk free economy b a risky, but risk neutral economy c our economy withmarketSharperatioE R Rf  R Rf 0.5andRf .01onan annual basis. above, below, or on the plane of returns? Consider a m,x ,orR ? 5. What happens to R , R e , and the mean variance frontier if investors are",
        "6 Relation between Discount Factors, Betas, and Mean Variance Frontiers In this chapter, I draw the connection between discount factors, mean variance frontiers, and beta representations. In the first chapter, I showed how mean variance and a beta representation follow from p E mx in a complete markets setting. Here, I discuss the connections in both directions and in incomplete markets, drawing on the representations studied in the last chapter. The central theme of the chapter is that all three representations are equivalent. Figure 6.1 summarizes the ways one can go from one representa tion to another. A discount factor, a reference variable for betas the thing you put on the right hand side in the regressions that give betas and a return on the mean variance frontier all carry the same information, and given any one of them, you can find the others. More specifically, 1. p E mx . Given m such that p E mx , then m,x ,R , or R wR e all can serve as reference variables for betas. 2. p E mx mean variance frontier. You can construct R from x proj m X ,R x E x 2 .ThenR ,R wRe areonthemean variance frontier. 3. Mean variance frontier p E mx . If R mv is on the mean variance frontier, then m a bR mv linear in that return is a discount factor; it satisfies p E mx . 4.  p E mx . If we have an expected return beta model with factors f , then m b f linear in the factors satisfies p E mx . 5. If a return is on the mean variance frontier, then there is an expected return beta model with that return as reference variable. The following subsections discuss the mechanics of going from one representation to the other in detail. The last section of the chapter collects some special cases when there is no risk free rate. The next chapter discusses 99",
        "100 6. Discount Factors, Betas, and Mean Variance Frontiers Figure 6.1. Relation between three views of asset pricing. some of the implications of these equivalence theorems, and why they are important. Roll 1977 pointed out the connection between mean variance fron tiers and beta pricing. Ross 1978 and Dybvig and Ingersoll 1982 pointed out the connection between linear discount factors and beta pricing. Hansen and Richard 1987 pointed out the connection between a discount factor and the mean variance frontier. 6.1 From Discount Factors to Beta Representations Beta Representation Using m p E mx impliesE Ri  i,mm.Startwith Thus, i 1 cov m,Ri E R . m , x , and R can all be the single factor in a single beta representation. 1 E mRi E m E Ri cov m,Ri . E m E m",
        "102 6. Discount Factors, Betas, and Mean Variance Frontiers which we can write as the desired single beta model, E Ri  i,x x . Notice that the zero beta rate 1 E x appears when there is no risk free rate. To derive a single beta representation with R , recall the definition, R x . E x 2 Substituting R a return R for x , equation 6.1 implies that we can in fact construct from m that acts as the single factor in a beta model, i E R 2 cov R ,Ri E R 2 cov R ,Ri var R E R E R E R E R var R or, defining Greek letters in the obvious way, E R i   i  . E R 6.2 is also a return, its expected excess return over the zero beta rate gives the factor risk premium R . Applying equation 6.2 to R Since the factor R itself, var R E R  . 6.3 So we can write the beta model in an even more traditional form R,RR E R E Ri   i E R  . 6.4 P R,R Special Cases A footnote to these constructions is that E m , E x , or E R cannot be zero, or you could not divide by them. This is a pathological case: E m 0 implies a zero price for the risk free asset, and an infinite risk free rate. If a risk free rate is traded, we can simply observe that it is not infinite and verify the fact. Also, in a complete market, E m cannot be zero since, by absence of arbitrage, m 0. We will see similar special cases in the remaining theorems: the manipulations only work for discount factor choices that do not imply zero or infinite risk free rates. I discuss the issue in Section 6.6. Recall that R portionofthemean variancefrontier.ThisiswhyR hasanunusualnegative expectedexcessreturnorfactorriskpremium,R var R E R 0.  is the zero beta rate on R . is the minimum second moment frontier, on the lower",
        "6.2. From Mean Variance Frontier to Discount Factor 103 The manipulation from expected return covariance to expected return beta breaks down if var m , var x , or var R is zero. This is the case of pure risk neutrality. In this case, all expected returns become the same as the risk free rate. 6.2 From Mean Variance Frontier to a Discount Factor and Beta Representation Rmv is on mean variance frontier m a bRmv; E Ri  i E Rmv  . We have seen that p E mx implies a single beta model with a mean varianceefficientreferencereturn,namelyR .Theconverseisalsotrue:for almost any return on the mean variance frontier, we can derive a discount factor m that is a linear function of the mean variance efficient return. Also, expected returns mechanically follow a single beta representation using the mean variance efficient return as reference. I start with the discount factor. Theorem: There is a discount factor of the form m a bRmv if and only if Rmv is on the mean variance frontier, and Rmv is not the risk free rate. When there is no risk free rate, if Rmv is not the constant mimicking portfolio return. Graphical Argument The basic idea is very simple, and Figure 6.2 shows the geometry for the complete markets case. The discount factor m x is proportional to R . The mean variance frontier is R wRe . Pick a vector Rmv on the mean variance frontier as shown in Figure 6.2. Then stretch it bRmv and then subtract some of the 1 vector a . Since Re is generated by the unit vector, we can get rid of the Re component and get back to the discount factor x if we pick the right a and b. If the original return vector were not on the mean variance frontier, then any linear combination a bRmv with b 0 would point in some of the n direction, which R and x do not. If b 0, though, just stretching up and down the 1 vector will not get us to x . Thus, we can only get a discount factor of the form a bRmv if Rmv is on the frontier. You may remember that x factors are of the form m x  with E x 0. Perhaps a bR gives one of these discount factors, when R is not on the mean variance frontier? This is not the only discount factor all discount",
        "104 6. Discount Factors, Betas, and Mean Variance Frontiers Figure 6.2. There is a discount factor m a bRmv if and only if Rmv is on the mean variance frontier and not the risk free rate. does not work, however; n is still in the payoff space X while, by definition,  is orthogonal to this space. If the mean variance efficient return Rmv that we start with happens to lie right on the intersection of the stretched unit vector and the frontier, then stretching the Rmv vector and adding some unit vector are the same thing,soweagaincannotgetbacktox bystretchingandaddingsomeunit vector. The stretched unit payoff is the risk free rate, so the theorem rules out the risk free rate. When there is no risk free rate, we have to rule out the constant mimicking portfolio return. I treat this case in Section 6.6. Algebraic Proof Now, an algebraic proof that captures the same ideas. Proof. For an arbitrary R, try the discount factor model m a bR a b R wRe n . 6.5 We show that this discount factor prices an arbitrary payoff if and only if n 0, and except for the w choice that makes R the risk free rate or the constant mimicking portfolio return if there is no risk free rate . We can determine a and b by forcing m to price any two assets. I find a and b to make the model price R and Re : 1 E mR aE R bE R 2 , 0 E mRe aE Re bwE Re 2 a bw E Re .",
        "6.2. From Mean Variance Frontier to Discount Factor 105 Solving for a and b, a w, wE R E R 2 b 1. wE R E R 2 Thus, if it is to price R and Re , the discount factor must be w R wRe n m . 6.6 decomposed as xi yiR wiRe ni. SeeFigure5.2ifthisisnotobvious. Thepriceofxi isyi,sincebothRe and ni are zero price excess return payoffs. Therefore, we want E mxi yi. Does it? w R wRe n yiR wiRe ni E mxi E wE R E R 2 . Using the orthogonality of R , Re ,n; E n 0 and E Re 2 E Re to simplify the product, wE R E R 2 Now, let us see if m prices an arbitrary payoff xi. Any xi X can also be E mxi wyiE R yiE R 2 E nni wE R E R 2 yi E nni wE R E R 2 . To get p xi yi E mxi , we need E nni 0. The only way to guarantee this condition for every payoff xi X is to insist that n 0. Obviously, this construction cannot work if the denominator of 6.6 is zero, i.e., if w E R 2 E R 1 E x . If there is a risk free rate, then Rf 1 E x , so we are ruling out the case Rmv R Rf Re , which is the risk free rate. If there is no risk free rate, I interpret R R E R 2 E R Re as a constant mimicking portfolio return in Section 6.6. P We can generalize the theorem somewhat. Nothing is special about returns; any payoff of the form yR wRe or yx wRe can be used to price assets; such payoffs have minimum variance among all payoffs with given mean and price. Of course, we proved existence, not uniqueness: m a bRmv ,E x 0alsopriceassetsasalways. To get from the mean variance frontier to a beta pricing model, we can just chain this theorem and the theorem of the last section together. There",
        "106 6. Discount Factors, Betas, and Mean Variance Frontiers is a slight subtlety about special cases when there is no risk free rate, but since it is not important for the basic points I relegate the direct connection and the special cases to Section 6.6. 6.3 Factor Models and Discount Factors We have shown that p E mx implies a single beta representation You can write a linear factor model most compactly as m b f , letting one of the factors be a constant. However, since we want a connection to the beta representation based on covariances rather than second moments, it is easiest to fold means of the factors in to the constant, and write m a b f with E f 0 and hence E m a. The connection is easiest to see in the special case that all the test assets are excess returns. Then 0 E mRe does not identify the mean of m, and we can normalize a arbitrarily. I find it convenient to normalize to E m 1, orm 1 b f E f .Then, Beta pricing models are equivalent to linear models for the discount factor m, E Ri   i m a b f. using m,x , or R as factors. Let us ask the converse question: suppose we have an expected return beta model such as CAPM, APT, ICAPM, etc. What discount factor does this model imply? I show that an expected return beta model is equivalent to a discount factor that is a linear function of the factors in the beta model. This is an important and central result. It gives the connection between the discount factor formulation emphasized in this book and the expected return beta, factor model formulation common in empirical work. Theorem: Given the model m 1 f E f b, 0 E mRe , one can find  such that E Re  , where  are the multiple regression coefficients of excess returns Re on the factors. Conversely, given  in 6.8 , we can find b such that 6.7 holds. 6.7 6.8",
        "6.3. Factor Models and Discount Factors 107 Proof: From 6.7 , 0 E mRe E Re cov Re , f b, E Re cov Re,f b. From covariance to beta is quick, E Re cov Re,f var f 1var f b  . Thus,  and b are related by  var f b. P When the test assets are returns, the same idea works just as well, but gets a little more drowned in algebra since we have to keep track of the constant in m and the zero beta rate in the beta model. Theorem: Given the model m a b f , 1 E mR i , 6.9 one can find  and  such that E Ri   i, 6.10 wherei arethemultipleregressioncoefficientsofRi onf withaconstant.Conversely, given  and  in a factor model of the form 6.10 , one can find a, b such that 6.9 holds. Proof: We just have to construct the relation between  ,  and a , b and show that it works. Start with m a b f , 1 E mR , and hence, still folding the mean of the factors in a so E f 0, 1 cov m,R 1 E Rf b E R . 6.11 E m E m a a i is the vector of the appropriate regression coefficients, i E ff 1E fRi , so to get  in the formula, continue with E R 1 E Rf E ff 1E ff b 1  E ff b. aaaa",
        "108 6. Discount Factors, Betas, and Mean Variance Frontiers Now, define  and  to make it work,  1 1, E m a  1cov f,f b E mf . 6.12 a Using 6.12 , we can just as easily go backwards from the expected return beta representation to m a b f . As always, we have to worry about a special case of zero or infinite risk free rates. We rule out E m E a b f 0 to keep 6.11 from exploding, and we rule out  0 and cov ff singular to go from ,, in 6.12 back to m. P Given either model, there is a model of the other form. They are not unique. We can add to m any random variable orthogonal to returns, and we can add spurious risk factors with zero  and or , leaving pric ing implications unchanged. We can also express the multiple beta model as a single beta model with m a b f as the single factor, or use its corresponding R . Equation 6.12 shows that the factor risk premium  can be interpreted as the price of the factor; a test of  0 is often called a test of whether the factor is priced. More precisely,  captures the price E mf of the de meaned factors brought forward at the risk free rate. If we start with underlyingfactorsf suchthatthede meanedfactorsaref f E f , E f  p f E f  p f  .  represents the price of the factors less their risk neutral valuation, i.e., the factor risk premium. If the factors are not traded,  is the model s predicted price rather than a market price. Low prices are high risk premia, resulting in the negative sign. If the factors are returns with price one, then the factor risk premium is the expected return of the factor, less  ,  E f  . Note that the factors need not be returns though they may be ; they need not be orthogonal, and they need not be serially uncorrelated or conditionally or unconditionally mean zero. Such properties may occur as natural special cases, or as part of the economic derivation of specific factor models, but they are not required for the existence of a factor pricing representation. For example, if the risk free rate is constant, then Et mt 1 is constant and at least the sum b ft 1 should be uncorrelated over time. But if the risk free rate is not constant, then Et mt 1 Et b ft 1 should vary over time.",
        "6.3. Factor Models and Discount Factors 109 Factor Mimicking Portfolios It is often convenient to use factor mimicking payoffs f proj f X , factor mimicking returns p proj f X or factor mimicking excess returns f proj f Re in place of true factors. These payoffs carry the same pricing information as the original factors, and can serve as reference variables in expected return beta representations. proj f X f , When the factors are not already returns or excess returns, it is con venient to express a beta pricing model in terms of its factor mimicking portfolios rather than the factors themselves. Recall that x proj m X carries all of m s pricing implications on X; p x E mx E x x . The factor mimicking portfolios are just the same idea using the individual factors. Define the payoffs f by f proj f X . Then, m b f carries the same pricing implications on X as does m b f : p E mx E b f x E b projf X x E b f x . 6.13 I include the constant as one of the factors. The factor mimicking portfolios also form a beta representation. Just go from 6.13 back to an expected return beta representation E Ri    , 6.14 and find  ,  using 6.12 . The  are the regression coefficients of the returns Ri on the factor mimicking portfolios, not on the factors, as they should be.",
        "110 6. Discount Factors, Betas, and Mean Variance Frontiers It is more traditional to use the returns or excess returns on the factor mimicking portfolios rather than payoffs as I have done so far. To generate returns, divide each payoff by its price, i proj fi X f . p proj fi X The resulting bi will be scaled down by the price of the factor mimicking payoff, and the model is the same. Note that you project on the space of payoffs, not of returns. Returns R are not a space, since they do not contain zero. If the test assets are all excess returns, you can even more easily project the factors on the set of excess returns, which are a space since they do include zero. If we define f proj f Re , then of course the excess returns f the factors f for a set of excess returns; m b f satisfies 0 E mRei and carry the same pricing implications as E Rei i,f  i,f E f . 6.4 Discount Factors and Beta Models to Mean Variance Frontier It is easy to show that, given m , we can find a return on the mean variance frontier. Given m, construct x proj m X and R x E x 2 . R is the minimum second moment return, and hence on the mean variance frontier. Similarly, if you have a set of factors f for a beta model, then a lin ear combination of the factor mimicking portfolios is on the mean variance frontier.Abetamodelisthesameasm b f.Sincemislinearinf,x is linear in f proj f X , so R is linear in the factor mimicking payoffs f or their returns f p f . mimicking portfolio returns is on the mean variance frontier. which is on the mean variance frontier If a beta pricing model holds, then a linear combination of the factor From m, we can construct R and one other return, a risk free rate or a risk free rate proxy. Thus, any frontier return is a linear function Any frontier return is a combination of R of the factor mimicking returns plus a risk free rate proxy.",
        "6.5. Three Risk Free Rate Analogues 111 Section 5.4 showed how we can span the mean variance frontier with R and a risk free rate, if there is one, or the zero beta, minimum variance, or constant mimicking portfolio return R proj 1 X p proj 1 X if there is no risk free rate. The latter is particularly nice in the case of a linear factor model, since we may consider the constant as a factor, so the frontier is entirely generated by factor mimicking portfolio returns. 6.5 Three Risk Free Rate Analogues I introduce three counterparts to the risk free rate that show up in asset pricing formulas when there is no risk free rate. The three returns are the zero beta return, the minimum variance return, and the constant mimicking portfolio return. Three different generalizations of the risk free rate are useful when a risk free rate or unit payoff is not in the set of payoffs. These are the zero beta return, the minimum variance return, and the constant mimicking portfolio return. I introduce the returns in this section, and I use them in the next section to state some special cases involving the mean variance frontier. Each of these returns maintains one property of the risk free rate in a market in which there is no risk free rate. The zero beta return is a mean variance effi cient return that is uncorrelated with another given mean variance efficient return. The minimum variance return is just that. The constant mimicking portfolio return is the return on the payoff closest to the unit payoff. Each of these returns has a representation in the standard form R wR e with slightly different w. In addition, the expected returns of these risky assets are used in some asset pricing representations. For example, the zero beta rate is often used to refer to the expected value of the zero beta return. Each of these risk free rate analogues is mean variance efficient. Thus, I characterize each one by finding its weight w in a representation of the form R wRe . We derived such a representation above for the risk free rate as equation 5.20 , Rf R RfRe . 6.15 In the last subsection, I show how each risk free rate analogue reduces to the risk free rate when there is one.",
        "112 6. Discount Factors, Betas, and Mean Variance Frontiers Zero Beta Return for R  The zero beta return for R , denoted R , is the mean variance efficient return uncorrelated with R . Its expected return is the zero beta rate  E R  . This zero beta return has representation var R E R E Re Re , 1  E R The zero beta rate is found graphically in mean standard deviation space by R R and the corresponding zero beta rate is  E R 2 . E R E x extending the tangency at R to the vertical axis. It is also the inverse of the price that x and R assign to the unit payoff. The risk free rate Rf is of course uncorrelated with R . Risky returns uncorrelated with R there is one, so such returns might take the place of Rf when Rf does not exist. For any return R that is uncorrelated with R we have E R R E R E R , so earn the same average return as the risk free rate if  E R R E R 2 1  E R . E R E R E x I call  the zero beta rate, and R the zero beta return. There is no risk free rate, so there is no security that just pays  . As you can see from the formula, the zero beta rate is the inverse of ral generalization of the risk free rate. It is called the zero beta rate because the price that R and x assign to the unit payoff, which is another natu cov R , R 0 implies that the regression beta of R on R is zero. More precisely, one might call it the zero beta rate on R , since one can calcu portfolio will generally be different from the zero beta rate on R . I draw  in Figure 6.3 as the intersection of the tangency and the verti cal axis. This is a property of any return on the mean variance frontier: The expected return on an asset uncorrelated with the mean variance efficient asset a zero beta asset lies at the point so constructed. To check this geom etry, use similar triangles: The length of R in Figure 6.3 is E R 2 , and its vertical extent is E R . Therefore,  E R 2 E R 2 E R , or late zero beta rates for returns other than R and they are not the same as the zero beta rate for R . In particular, the zero beta rate on the market",
        "6.5. Three Risk Free Rate Analogues 113 Figure 6.3. Zero beta rate  and zero beta return R for R .  E R 2 E R . Since R is on the lower portion of the mean variance frontier, this zero beta rate  is above the minimum variance return. Note that in general  1 E m . Projecting m on X preserves asset pricing implications on X but not for payoffs not in X . Thus if a risk free with mean equal to the zero beta rate, shown as R to characterize this return in R wR e form. To do this, we want to find w such that E R 2 E R rate is not traded, x rate as they do for other nontraded assets. and m may differ in their predictions for the risk free The zero beta return is the rate of return on the mean variance frontier E R Solving, the answer is  E R wE Re . in Figure 6.3. We want E R 2 E R 2 var R w . E R E Re E R E Re Thus, the zero beta return is R R var R E R E Re Re . Note that the weight is not E R E R 2 E R , as Rf different. R Rf Re . When there is no risk free rate, the weight and the mean return are",
        "114 6. Discount Factors, Betas, and Mean Variance Frontiers Minimum Variance Return The risk free rate obviously is the minimum variance return when it exists. When there is no risk free rate, the minimum variance return is The minimum variance return has the representation E R 1 E Re Rmin. var. R Re . Rmin. var. R Taking expectations, E Rmin. var. E R E R 1 E Re Re . 6.16 E R 1 E Re E R 1 E Re E Re Thus, the minimum variance return retains the nice property of the risk free . just as Rf R Rf Re . When there is no risk free rate, the zero beta and minimum variance returns are not the same. You can see this fact clearly in Figure 6.3. We can derive expression 6.16 for the minimum variance return by brute force: choose w in R wR e to minimize variance: min var R wRe E R wRe 2 E R wRe 2 w E R 2 w2E Re E R 2 2wE R E Re w2E Re 2. The first order condition is 0 wE Re 1 E Re E R E Re , E R w . 1 E Re rate, that its weight on Re is the same as its mean, Rmin. var. R E Rmin. var. Re ,",
        "6.5. Three Risk Free Rate Analogues 115 Constant Mimicking Portfolio Return The constant mimicking portfolio return is defined as the return on the projection of the unit vector on the payoff space, proj 1 X R . Re . p proj 1 X It has the representation E R 2 R R E R When there is a risk free rate, it is the rate of return on a unit payoff, Rf 1 p 1 . When there is no risk free rate, we might define the rate of return on the mimicking portfolio for a unit payoff, proj 1 X R . p proj 1 X I call this object the constant mimicking portfolio return. The mean variance representation of the constant mimicking portfolio return is E R 2 Re . 6.17 R R Re R E R Note that the weight  equal to the zero beta rate creates the constant mimicking return, not the zero beta return. To show 6.17 , start with property 5.21 , E R E R 2 Re proj 1 X Take the price of both sides. Since the price of Re is zero and the price of Solving 6.18 for proj 1 X , dividing by 6.19 , we obtain the right hand side of 6.17 . R is one, we establish p proj 1 X . 6.19 E R E R 2 R . 6.18",
        "116 6. Discount Factors, Betas, and Mean Variance Frontiers Risk Free Rate Again, we derived in equation 5.20 that the risk free rate has the representation, Rf R RfRe . 6.20 Obviously, we should expect that the zero beta return, minimum variance return, and constant mimicking portfolio return reduce to the risk free rate when there is one. These other rates are The risk free rate has the mean variance representation Rf R RfRe . The zero beta, minimum variance, and constant mimicking portfolio returns reduce to this formula when there is a risk free rate. constant mimicking: minimum variance: R R Rmin. var. R 6.21 Re , 6.22 E R 2 Re , E R 1 E Re f E R 2 E R var R R . 6.24 E R R R To establish that these are all the same when there is a risk free rate, we need to show that equality, take expectations of 6.15 , Rf E R Rf E Re 6.25 and solve for Rf . To derive the third equality, use the first equality from 6.24 in 6.25 , zero beta: var R E R E Re Re . 6.23 E R 1 E Re E R E Re We derived the first equality above as equation 5.18 . To derive the second Solving for Rf , E R 2 E R E R Rf E Re . E R 2 E R 2 var R R . f E R E Re E R E Re",
        "6.6. Mean Variance Special Cases with No Risk Free Rate 117 6.6 Mean Variance Special Cases with No Risk Free Rate I collect in this section the special cases for the equivalence theorems of this chapter. The special cases all revolve around the problem that the expected discount factor, price of a unit payoff, or risk free rate must not be zero or infinity. This is typically an issue of theoretical rather than practical importance. In a complete, arbitrage free market, m 0 so we know E m 0. If a risk free rate is traded, you can observe E m 1 Rf 0. However, in an incomplete market in which no risk free rate is traded, there are many discount factors with the same asset pricing implica tions, and you might have happened to choose one with E m 0 in your manipulations. By and large, this is easy to avoid: choose another of the many discount factors with the same pricing implications that does not have E m 0. More generally, when you choose a particular discount factor you are choosing an extension of the current set of prices and payoffs; you are viewing the current prices and payoffs as a subset of a particular contingent claim economy. Make sure you pick a sensible one. Therefore, we could simply state the special cases as when a risk free rate is not traded, make sure you use discount factors with 0 E m . However, it is poten tially useful and it certainly is traditional to specify the special return on the mean variance frontier that leads to the infinite or zero implied risk free rate, and to rule it out directly. This section works out what those returns are and shows why they must be avoided. The Special Case for Mean Variance Frontier to Discount Factor In Section 6.2, we saw that we can form a discount factor a bRmv from any mean variance efficient return Rmv except one particular return, oftheformR E R 2 E R Re .Thisreturnledtoaninfinitem.Wenow We can find a discount factor from any mean variance efficient return except the constant mimicking return. We can find a beta representation from any mean variance efficient return except the minimum variance return. When there is no risk free rate, we can find a discount factor that is a linear function of any mean variance efficient return except the constant mimicking portfolio return.",
        "118 6. Discount Factors, Betas, and Mean Variance Frontiers recognize this return as the risk free rate, when there is one, or the constant mimicking portfolio return, if there is no risk free rate. Figure 6.4 shows the geometry of this case. To use no more than three dimensions I had to reduce the return and excess return sets to lines. The payoff space X is the plane joining the return and excess return sets as shown. The set of all discount factors is m x , E x 0, the Take any return on the mean variance frontier, Rmv. Since the return space only has two dimensions, all returns are on the frontier. For a given Rmv, the space a bRmv is the plane spanned by Rmv and the unit payoff. This plane lies sideways in the figure. As the figure shows, there is a vector a bRmv inthisplanethatliesonthelineofdiscountfactors. Next, the special case. This construction would go awry if the plane spanned by the unit payoff and the return Rmv were parallel to the line containing the discount factor. Thus, the construction would not work for the return marked R in the figure. This is a return corresponding to a payoff that is the projection of the unit payoff on to X , so that the residual will be orthogonal to X , as is the line of discount factors. With Figure 6.4 in front of us, we can also see why the constant mimicking portfolio return is not the same thing as the minimum variance return. Variance is the size or second moment of the residual in a projection regression on 1: var x E x E x 2 E x proj x 1 2 x proj x 1 2. Figure 6.4. One can construct a discount factor m a bRmv from any mean variance orthogonal to the payoff space X in the figure. I draw the unit payoff the dot marked 1 in Figure 6.4 closer to the viewer than the plane X , and I draw a vector through the unit payoff coming out of the page. line through x efficient return except the constant mimicking return R.",
        "6.6. Mean Variance Special Cases with No Risk Free Rate 119 Thus, the minimum variance return is the return closest to extensions of the unit vector. It is formed by projecting returns on the unit vector. The constant mimicking portfolio return is the return on the payoff closest to 1. It is formed by projecting the unit vector on the set of payoffs. The Special Case for Mean Variance Frontier to a Beta Model We already know mean variance frontiers discount factor and dis count factor single beta representation, so at a superficial level we can string the two theorems together to go from a mean variance efficient return to a beta representation. However, it is more elegant to go directly, and the special cases are also a bit simpler this way. Theorem: There is a single beta representation with a return Rmv as factor, E Ri Rmv i,Rmv E Rmv Rmv , if and only if Rmv is mean variance efficient and not the minimum variance return. This famous theorem is given by Roll 1977 and Hansen and Richard 1987 . We rule out minimum variance to rule out the special case E m 0. Graphically, the zero beta rate is formed from the tangency to the mean variance frontier as in Figure 6.3. I use the notation Rmv to emphasize that we use the zero beta rate corresponding to the particular mean variance return Rmv that we use as the reference return. If we used the minimum variance return, that would lead to an infinite zero beta rate. Proof: The mean variance frontier is R mv R wR e . Any return is Ri R wiRe ni.Thus, Now, We can use any return on the mean variance frontier as the reference return for a single beta representation, except the minimum variance return. E Ri E R wiE Re . 6.26 cov Ri,Rmv cov R wRe , R wiRe var R wwi var Re w wi E R E Re var R wE R E Re wi w var Re E R E Re .",
        "120 6. Discount Factors, Betas, and Mean Variance Frontiers Thus, cov Ri,Rmv and E Ri are both linear functions of wi. We can solve cov Ri,Rmv for wi, plug into the expression for E Ri and we are done. To do this, of course, we must be able to solve cov Ri,Rmv for wi. This requires E R E Re E R E Re E R w , 6.27 var Re E Re 2 E Re 2 1 E Re which is the condition for the minimum variance return. P Problems Chapter 6 1. In the argument that Rmv on the mean variance frontier, Rmv R wRe , implies a discount factor m a bRmv, do we have to rule out the case of risk neutrality? Hint: what is R e when the economy is risk neutral? 2. If you use factor mimicking portfolios as in 6.13 , you know that the predictions for expected returns are the same as they are if you use the factorsthemselves.Arethe , ,and forthefactor mimickingportfolio representation the same as the original  , , and  of the factor pricing model? 3. Suppose the CAPM is true, m a bR m prices a set of assets, and there is a risk free rate Rf . Find R in terms of the moments of Rm,Rf . 4. If you express the mean variance frontier as a linear combination of factor mimicking portfolios from a factor model, do the relative weights of the various factor portfolios in the mean variance efficient return change as you sweep out the frontier, or do they stay the same? Start with the risk free rate case. 5. For an arbitrary mean variance efficient return of the form R wRe , find its zero beta return and zero beta rate. Show that your rate reduces to the risk free rate when there is one. 6. When the economy is risk neutral, and if there is no risk free rate, show that the zero beta, minimum variance, and constant mimicking portfolio returns are again all equivalent, though not equal to the risk free rate. What is R in this economy? R 1 R f since there is no risk free rate.",
        "124 7. Implications of Existence and Equivalence Theorems For example, suppose the CAPM is true, the market portfolio is ex ante mean variance efficient, and sets pricing errors to zero if you use true or subjective probabilities. Nonetheless, the market portfolio is unlikely to be ex post mean variance efficient in any given sample. In any sample, there will be lucky winners and unlucky losers. An ex post mean variance efficient portfolio will be a Monday morning quarterback; it will tell you to put large weights on assets that happened to be lucky in a given sample, but are no more likely than indicated by their betas to generate high returns in the future. Oh, if I had only bought Microsoft in 1982. . . is not a useful guide to forming a mean variance efficient portfolio today. In fact, mean reversion and book market effects suggest that assets with unusually good returns in the past are likely to do poorly in the future! The only solution is to impose some kind of discipline in order to avoid dredging up spuriously good in sample pricing. The situation is the same as in traditional regression analysis. Regres sions are used to forecast or to explain a variable y by other variables x in a regression y x  . By blindly including right hand variables, one can produce models with arbitrarily good statistical measures of fit. But this kind of model is typically unstable out of sample or otherwise useless for explanation or forecasting. One has to carefully and thoughtfully limit the search for right hand variables x in order to produce good models. What makes for an interesting set of restrictions? Econometricians wrestling with y x   have been thinking about this question for about 50 years, and the best answers are 1 use economic theory to carefully specify the right hand side and 2 use a battery of cross sample and out of sample stability checks. Alas, this advice is hard to follow. Economic theory is usually either silent on what variables to put on the right hand side of a regression, or allows a huge range of variables. The same is true in finance. What are the fundamental risk factors? is still an unanswered question. At the same time one can appeal to the APT and ICAPM to justify the inclusion of just about any desirable factor. Fama 1991 calls these theories a fishing license. Thus, you will grow old waiting for theorists to provide sharp restrictions. Following the purely statistical advice, the battery of cross sample and out of sample tests often reveals the model is unstable, and needs to be changed. Once it is changed, there is no more out of sample left to check it. Furthermore, even if one researcher is pure enough to follow the method ology of classical statistics, and wait 50 years for another fresh sample to be available before contemplating another model, his competitors and journal editors are unlikely to be so patient. In practice, then, out of sample validation is not as strong a guard against fishing as one might hope. Nonetheless, these are the only tools we have to guard against fishing. In my opinion, the best hope for finding pricing factors that are robust",
        "128 7. Implications of Existence and Equivalence Theorems mean variance and beta language developed first, and to think about why the discount factor language seems to be taking over. Asset pricing started by putting mean and variance of returns on the axes, rather than payoff in state 1, payoff in state 2, etc. as we do now. The early asset pricing theorists, in particular Markowitz 1952 , posed the question just right: they wanted to treat assets in the apples and oranges, indifference curve and budget set framework of microeconomics. The prob lem was, what labels to put on the axes? Clearly, IBM stock and GM stock is not a good idea; investors do not value securities per se, but value some aspects of the stream of random cash flows that those securities give rise to. Their brilliant insight was to put the mean and variance of the portfolio return on the axes: to treat these as hedonics by which investors valued their portfolios. Investors plausibly want more mean and less variance. They gave investors utility functions defined over this mean and variance, just as standard utility functions are defined over apples and oranges. The mean variance frontier is the budget set. With this focus on portfolio mean and variance, the next step was to realize that each security s mean return measures its contribution to the portfolio mean, and that regression betas on the overall portfolio give each security s contribution to the portfolio variance. The mean return versus beta description for each security followed naturally Sharpe 1964 . In a deep sense, the transition from mean variance frontiers and beta models to discount factors represents the realization that putting con sumption in state 1 and consumption in state 2 on the axes specifying preferences and budget constraints over state contingent consumption is a much more natural mapping of standard microeconomics into finance than putting mean, variance, etc. on the axes. If for no other reason, the contingent claim budget constraints are linear, while the mean variance frontier is not. Thus, I think, the focus on means and variance, the mean variance frontier, and expected return beta models is all due to an accident of history, that the early asset pricing theorists happened to put mean and variance on the axes rather than state contingent consumption. Of course, contingent claims go back just as far, to Debreu 1959 . However, Debreu seemed to think of them as an unrealistic mathematical formalism. It has taken us a long time to realize that a contingent claim framework can be applied to real world phenomena. Well, here we are, why prefer one language over another? The discount factor language has an advantage for its simplicity, generality, mathematical convenience, and elegance. These virtues are to some extent in the eye of the beholder, but to this beholder, it is inspiring to be able to start every asset pricing calculation with one equation, p E mx . This equation covers all assets, including bonds, options, and real investment opportunities, while the expected return beta formulation is not useful or very cumbersome in",
        "Implications of Existence and Equivalence Theorems 129 the latter applications. Thus, it has seemed that there are several different asset pricing theories: expected return beta for stocks, yield curve models for bonds, arbitrage models for options. In fact all three are just cases of p E mx . As a particular example, arbitrage, in the precise sense of positive payoffs with negative prices, has not entered the equivalence discussion at all. I do not know of any way to cleanly graft absence of arbitrage on to expected return beta models. You have to tack it on after the fact by the way, make sure that every portfolio with positive payoffs has a positive price. It is trivially easy to graft it on to a discount factor model: just add m 0. The discount factor and state space language also makes it easier to think about different horizons and the present value statement of models. p E mx generalizes quickly to pt Et j mt,t jxt j, while returns have to be chained together to think about multiperiod models. The choice of language is not about normality or return distributions. There is a lot of confusion about where return distribution assumptions show up in finance. I have made no distributional assumptions in any of the discussion so far. Second moments as in betas and the variance of the mean variance frontier show up because p E mx involves a second moment. One does not need to assume normality to talk about the mean variance frontier. Returns on the mean variance frontier price other assets even when returns are not normally distributed.",
        "132 8. Conditioning Information by economic agents, and we cannot include even a fraction of observed conditioning information in our models. The basic feature and beauty of asset prices like all prices is that they summarize an enormous amount of information that only individuals see. The events that make the price of IBM stock change by a dollar, like the events that make the price of tomatoes change by 10 cents, are inherently unobservable to economists or would be social planners Hayek 1945 . Whenever possible, our treatment of conditioning information should allow agents to see more than we do. If we do not want to model conditional distributions explicitly, and if we want to avoid assuming that investors only see the variables that we include in an empirical investigation, we eventually have to think about unconditional moments, or at least moments conditioned on less information than agents see. Unconditional implications are also interesting in and of themselves. For example, we may be interested in finding out why the unconditional mean returns on some stock portfolios are higher than others, even if every agent fundamentally seeks high conditional mean returns. Most statistical estimation essentially amounts to characterizing unconditional means, as we will see in the chapter on GMM. Thus, rather than model conditional distri butions, this chapter focuses on what implications for unconditional moments we can derive from the conditional theory. 8.1 Scaled Payoffs Conditioning Down The unconditional implications of any pricing model are pretty easy to state. From pt Et mt 1xt 1 we can take unconditional expectations to obtain1 E pt E mt 1xt 1 . 8.1 1 We need a small technical assumption that the unconditional moment or moment con ditioned on a coarser information set exists. For example, if X and Y are normal 0, 1 , then E X Y 0butE X isinfinite. YY One can incorporate conditioning information by adding scaled payoffs and doing everything unconditionally. I interpret scaled returns as payoffs to managed portfolios. pt Et mt 1xt 1 E ptzt E mt 1xt 1zt .",
        "8.1. Scaled Payoffs 133 Thus, if we just interpret p to stand for E pt , everything we have done above applies to unconditional moments. In the same way, we can also condition down from agents fine information sets to coarser sets that we observe, pt E mt 1xt 1 E pt I E mt 1xt 1 I pt E mt 1xt 1 It t ifpt It. In making the above statements I used the law of iterated expectations, which is important enough to highlight it. This law states that your best forecast today of your best forecast tomorrow is the same as your best forecast today. In various useful guises, E Et x E x , Et 1 Et xt 1 Et 1 xt 1 , E E x I E x I . Instruments and Managed Portfolios We can do more than just condition down. Suppose we multiply the payoff andpricebyanyvariableorinstrumentzt observedattimet.Then, ztpt Et mt 1xt 1zt and, taking unconditional expectations, E pt zt E mt 1xt 1zt . 8.2 This is an additional implication of the conditional model, not captured by just conditioning down as in 8.1 . This trick originates from the GMM method of estimating asset pricing models, discussed below. The word instruments for the z variables comes from the instrumental variables estimation heritage of GMM. To think about equation 8.2 , group xt 1zt . Call this product a payoff x xt 1zt , with price p E pt zt . Then 8.2 reads p E mx once again. Rather than thinking about 8.2 as a instrumental variables estimate of a conditional model, we can think of it as a price and a payoff, and apply all the asset pricing theory directly. This interpretation is not as artificial as it sounds. zt xt 1 are the payoffs tomanagedportfolios.Aninvestorwhoobserveszt can,ratherthan buyand",
        "134 8. Conditioning Information hold, invest in an asset according to the value of zt . For example, if a high valueofzt forecaststhatassetreturnsarelikelytobehighthenextperiod, theinvestormightbuymoreoftheassetwhenzt ishighandviceversa.Ifthe investorfollowsalinearrule,heputsztpt dollarsintotheasseteachperiod and receives zt xt 1 dollars the next period. This all sounds new and different, but practically every test uses man aged portfolios. For example, the size, beta, industry, book market, and so forth portfolios of stocks are all managed portfolios, since their composition changes every year in response to conditioning information the size, beta, etc., of the individual stocks. This idea is also closely related to the idea of dynamic spanning. Markets that are apparently very incomplete can in reality provide many more state contingencies through dynamic conditioned on information trading strategies. Equation 8.2 offers a very simple view of how to incorporate the extra information in conditioning information: Add managed portfolio payoffs, and proceed with unconditional moments as if conditioning information did not exist! The linearity of xz is not an important restriction. If the investor wanted to place, say, 2 3z 2 dollars in the asset, we could capture this desire with an instrumentz2 2 3z2.Nonlinear measurable transformationsoftime t random variables are again random variables. We can thus incorporate conditioning information while still looking at unconditional moments instead of conditional moments, without any of the statistical machinery of explicit models with time varying moments. The only subtleties are: 1 The set of asset payoffs expands dramatically, since we can consider all managed portfolios as well as basic assets, potentially multiplying every asset return by every information variable. 2 Expected prices of managed portfolios show up for p instead of just p 0 and p 1 if we started with basic asset returns and excess returns. 8.2 Sufficiency of Adding Scaled Returns We have shown that we can derive some extra implications from the presence of conditioning information by adding scaled returns. But does this exhaust the implications of conditioning information? Are we missing something important by relying on this trick? The answer is, in principle, no. Checking the expected price of all managed portfolios is, in principle, sufficient to check all the implications of conditioning information. E ztpt E mt 1xt 1zt zt It pt E mt 1xt 1 It",
        "8.2. Sufficiency of Adding Scaled Returns 135 I rely on the following mathematical fact: The conditional expectation of a variable yt 1 given an information set It , E yt 1 It is equal to a regression forecast of yt 1 using every variable zt It. Now, every random variable means every variable and every nonlinear measurable transformation of every variable, so there are a lot of variables in this regression! The word projection and proj yt 1 zt are used to distinguish the best forecast of yt 1 using only linear combinations of zt . Applying this fact to our case, let yt 1 mt 1xt 1 pt . Then E mt 1xt 1 pt zt 0 for every zt It implies 0 E mt 1xt 1 pt It . Thus, no implications are lost in principle by looking at scaled returns. Wereallydon thavetowritethezt explicitly.xt 1zt isapayoffavailable at time t 1 and pt zt is its price. Thus, the space of all payoffs X t 1 already includes the time t 1 payoffs you can generate with a basis set of assets xt 1 and all dynamic strategies that use information in the set It . With that definition of the space X t 1 we can write the sufficiency of scaled returns, simply recognizing that zt pt is a price and zt xt 1 is a payoff, as E p E m x x X p E m x I t t 1t 1t 1t 1 t t 1t 1t All linear and nonlinear transformations of all variables observed at time t sounds like a lot of instruments, and it is. But there is a practical limit to the number of instruments zt one needs to scale by, since only variables that forecast returns or m or their higher moments and co moments add any information. Since adding instruments is the same thing as including potential man aged portfolios, the thoughtful choice of a few instruments is the same thing as the thoughtful choice of a few assets or portfolios that one makes in any test of an asset pricing model. Even when evaluating completely unconditional asset pricing models, one always forms portfolios and omits many possible assets from analysis. Few studies, in fact, go beyond check ing whether a model correctly prices 10 25 stock portfolios and a few bond portfolios. Implicitly, one feels that the chosen payoffs do a pretty good job of spanning the set of available risk loadings or mean returns, and hence that adding additional assets will not affect the results. Nonetheless, since data are easily available on thousands of NYSE, AMEX and NASDAQ stocks, to say nothing of government and corporate bonds, mutual funds, foreign exchange, foreign equities, real investment opportunities, etc., the use of a few portfolios means that a tremendous number of potential asset payoffs are left out in an ad hoc manner. Similarly, if one had a small set of instruments that capture all the pre dictability of discounted returns mt 1Rt 1, then there would be no need to add more instruments. Thus, we carefully but arbitrarily select a few instruments that we think do a good job of characterizing the conditional",
        "8.3. Conditional and Unconditional Models 137 and that s it. Examples include the consumption based model with power utility, m  c c  , and the log utility CAPM, m 1 RW . t 1 t 1 t t 1 t 1 However, linear factor models include parameters that may vary over time and as functions of conditioning information. In these cases the transi tion from conditional to unconditional moments is much more subtle. We cannot easily condition down the model at the same time as we condition down the prices and payoffs. Conditional vs. Unconditional Factor Models in Discount Factor Language As an example, consider the CAPM m a bRW , where RW is the return on the market or wealth portfolio. We can find a and b from the condition that this model correctly price any two returns, for example R W itself and a risk free rate: 1 EmRW t t 1 t 1 t R 1 E m R W f 8.3 EtR R f t t 1 t t 1 t b . f2W a bERW, 1 ftt 1 As you can see, a 0 and b 0. To make a payoff proportional to the minimum second moment return on the lower part of the mean variance frontier , we need a portfolio long the risk free rate and short the market RW . Equation 8.3 shows explicitly that a and b must vary over time, as E RW ,2 RW , and Rf vary over time. If it is to price assets condition t t 1 t t 1 t ally, the CAPM must be a linear factor model with time varying weights, of the form m a bRW. t 1 t tt 1 This fact means that we can no longer transparently condition down. The statement that does not imply that we can find constants a and b so that 1 E a bRW R . t 1 t 1 1 E a bRW R t t t t 1 t 1 Rt t Rt 1",
        "138 Just try it. Taking unconditional expectations, 1 Ea bRW R t tt 1t 1 EaR bRWR t t 1 t t 1 t 1 8. Conditioning Information t t 1 t t 1 t 1 E a E R E b ERW R cov a ,R cov b ,RW R . t t 1 t t 1 t 1 Thus, the unconditional model 1 E E a E b RW R t t t 1 t 1 Onlyholdsifthecovariancetermsabovehappentobezero.Sinceat andbt are formed from conditional moments of returns, the covariances will not, in general, be zero. On the other hand, suppose it is true that at and bt are constant over time. Even if R f and R W are not i.i.d., the combinations given by 8.3 may tt be constant over time. Then 1 E a bRW R t 1 t t 1 does imply Furthermore, the latter unconditional model implies the former condi 1 E a bRW R . t 1 t 1 tional model, if the latter holds for all managed portfolios. Conditional vs. Unconditional in an Expected Return Beta Model To put the same observation in beta pricing language, E Ri Rf i tt 1 t tt tt general lead to an unconditional model. Again, there are special cases in which the conditional model does condition down. If bt b con stant in the discount factor representation, we know the model conditions down. The risk premium in an expected return beta model is given by t vart f bt. Thus, if factor risk premia move in proportion to the conditional variance of the factors, this is equivalent to a constant b, so the model will condition down. There are additional special cases as does not imply that Obviously, conditioning down 8.4 to 8.5 leads to a covariance E Ri  i. t 1 8.5 term between i and  . Therefore, the conditional model does not in 8.4",
        "8.3. Conditional and Unconditional Models 139 well. If the covariance of returns with factors is constant over time, the model will condition down despite varying bt. You can see this simply by E Re   cov Re,f var f 1 , so with a constant conditional t tt t t t covariance,E Re cov Re,f E vart f 1t cov Re,f . Wedonot need  Et t . The model also conditions down if conditional betas are constant over time. A problem at the end of the chapter guides you through the algebra of these special cases, in both expected return beta and discount factor representations. A Precise Statement Let us formalize these observations somewhat. Let X denote the space of all portfolios of the primitive assets, including managed portfolios in which the weights may depend on conditioning information, i.e., scaled returns. A conditional factor pricing model is a model m t 1 a t bt ft 1 that satisfies pt Et mt 1xt 1 for all xt 1 X . Anunconditionalfactorpricingmodelismodelmt 1 a b ft 1 thatsatisfies E pt E mt 1xt 1 for all xt 1 X. It might be more appropriately called a fixed weight factor pricing model. Given these definitions it is almost trivial that the unconditional model is just a special case of the conditional model, one that happens to have fixed weights. Thus, a conditional factor model does not imply an unconditional factor model because the weights may vary but an unconditional factor model does imply a conditional factor model. There is one important subtlety. The payoff space X is common, and contains all managed portfolios in both cases. The payoff space for the unconditional factor pricing model is not just fixed combinations of a set of basis assets. For example, we might check that the static constant a,b CAPM captures the unconditional mean returns of a set of assets. If this model does not also price those assets scaled by instruments, then it is not a conditional model, or, as I argued above, really a valid factor pricing model at all. Of course, everything applies for the relation between a conditional fac tor pricing model using a fine information set like investors information sets and conditional factor pricing models using coarser information sets like ours . If a set of factors prices assets with respect to investors informa tion, that does not mean the same set of factors prices assets with respect to our, coarser, information sets. Mean Variance Frontiers Define the conditional mean variance frontier as the set of returns that minimize vart Rt 1 given Et Rt 1 . This definition includes the lower segment as",
        "140 8. Conditioning Information usual. Define the unconditional mean variance frontier as the set of returns including managed portfolio returns that minimize var Rt 1 given E Rt 1 . These two frontiers are related by: If a return is on the unconditional mean variance frontier, it is on the conditional mean variance frontier. However, If a return is on the conditional mean variance frontier, it need not be on the unconditional mean variance frontier. These statements are exactly the opposite of what you first expect from the language. The law of iterated expectations E Et x E x leads you to expect that conditional should imply unconditional. But we are studying the conditional versus unconditional mean variance frontier, not raw conditional and unconditional expectations, and it turns out that exactly the opposite words apply. Of course, unconditional can also mean conditional on a coarser information set. Again, keep in mind that the unconditional mean variance frontier includes returns on managed portfolios. This definition is eminently rea sonable. If you are trying to minimize variance for given mean, why tie your hands to fixed weight portfolios? Equivalently, why not allow yourself to include in your portfolio the returns of mutual funds whose advisers promise the ability to adjust portfolios based on conditioning information? You could form a mean variance frontier of fixed weight portfolios of a basis set of assets, and this is what many people often mean by uncon ditional mean variance frontier. The return on the true unconditional mean variance frontier will, in general, include some managed portfolio returns, and so will lie outside this mean variance frontier of fixed weight portfo lios. Conversely, a return on the fixed weight portfolio MVF is, in general, not on the unconditional or conditional mean variance frontier. All we know is that the fixed weight frontier lies inside the other two. It may touch, but it need not. This is not to say the fixed weight unconditional frontier is uninteresting. For example, returns on this frontier will price fixed weight portfolios of the basis assets. The point is that this frontier has no connec tion to the other two frontiers. In particular, a conditionally mean variance efficient return conditional CAPM need not unconditionally price the fixed weight portfolios. I offer several ways to see this relation between conditional and unconditional mean variance frontiers. Using the Connection to Factor Models We have seen that the conditional CAPM m a b RW does not imply an unconditional CAPM m a bRW . We have seen that the t 1 t 1 t 1 t tt 1",
        "8.3. Conditional and Unconditional Models 141 existence of such a conditional factor model is equivalent to the statement that the return R W lies on the conditional mean variance frontier, and the t 1 existence of an unconditional factor model m a bR W is equivalent t 1 t 1 to the statement that RW is on the unconditional mean variance frontier. Then, from the trivial fact that an unconditional factor model is a special case of a conditional one, we know that RW on the unconditional frontier implies R W on the conditional frontier but not vice versa. Using the Orthogonal Decomposition We can see the relation between conditional and unconditional mean variance frontiers using the orthogonal decomposition characterization of the mean variance frontier, R mv R wR e see chapter 5 . This beautiful argument is the main point of Hansen and Richard 1987 . E p Et x x E p E x x , E Et R 2 Et R R E R 2 E R R , E Et Re Re Et Re E Re Re E Re . This fact is subtle and important. For example, starting with x p E x x 1x , you might think we need a different x ,R ,Re to rep ttt 1t 1 t 1 resent expected prices and unconditional means, using unconditional probabilities to define inner products. The three lines above show that this is not the case. The same old x , R ,Re represent conditional as well as unconditional prices and means. Recall that a return is mean variance efficient if and only if it is of the form Rmv R wRe . Thus,Rmv isconditionallymean varianceefficientifwisanynumberinthe By the law of iterated expectations, x and R generate expected prices and Re generates unconditional means as well as conditional means: time t information set: conditional frontier: Rmv R w Re , t 1 t 1 t t 1 and R mv is unconditionally mean variance efficient if w is any constant: unconditional frontier: R mv R wR e . t 1 t 1 t 1 Constants are in the t information set; time t random variables are not necessarily constant. Thus unconditional efficiency including managed portfolios implies conditional efficiency but not vice versa. As with the fac tor models, once you see the decomposition, it is a trivial argument about whether a weight is constant or time varying.",
        "142 8. Conditioning Information Brute Force and Examples If you are still puzzled, an additional argument by brute force may be helpful. If a return is on the unconditional mean variance frontier it must be on the conditional mean variance frontier at each date. If not, you could improve the unconditional mean variance trade off by moving to the con ditional mean variance frontier at each date. The unconditional mean variance frontier solves minE R2 s.t. E R . Writing the unconditional moment in terms of conditional moments, the problem is minE Et R2 s.t. E Et R . Now, suppose you could lower Et R2 at one date t without affecting Et R at that date. This change would lower the objective, without changing the constraint. Thus, you should have done it: you should have picked returns on the conditional mean variance frontiers. It almost seems that, reversing the argument, we can show that conditional efficiency implies unconditional efficiency, but it does not. Just because you have minimized Et R 2 for given value of Et R at each date t does not imply that you have minimized E R2 for a given value of E R . In showing that unconditional efficiency implies conditional efficiency we held fixed Et R at each date at , and showed it is a good idea to minimize t R . In trying to go backwards, the problem is that a given value of E R does not specify what Et R should be at each date. We can increase Et R in one conditioning information set and decrease it in another, leaving the return on the conditional mean variance frontier. Figure 8.1 presents an example. Return B is conditionally mean variance efficient. It also has zero unconditional variance, so it is the unconditionally mean variance efficient return at the expected return shown. Return A is on the conditional mean variance frontiers, and has the same unconditional expected return as B. But return A has some unconditional variance, and so is inside the unconditional mean variance frontier. As a second example, the risk free rate is only on the unconditional mean variance frontier if it is a constant. Remember the expression 6.15 for the risk free rate, Rf R RfRe . The unconditional mean variance frontier is R wRe with w a constant. Thus, the risk free rate is only unconditionally mean variance efficient if",
        "8.3. Conditional and Unconditional Models 143 Figure 8.1. Return A is on the conditional mean variance frontiers but not the unconditional mean variance frontier. Return B is on the conditional and unconditional frontier. it is a constant. Of course, the risk free rate is always on the conditional frontier. Implications: Hansen Richard Critique Many models, such as the CAPM, imply a conditional linear factor model mt 1 at bt ft 1. These theorems show that such a model does not imply an unconditional model. Equivalently, if the model predicts that the market portfolio is conditionally mean variance efficient, this does not imply that the market portfolio is unconditionally mean variance efficient. We often test the CAPM by seeing if it explains the average returns of some portfolios or equivalently if the market is on the unconditional mean variance frontier. The CAPM may quite well be true conditionally and fail these tests; assets may do better in terms of unconditional mean versus unconditional variance, while obeying the CAPM conditionally. The situation is not repaired by simple inclusion of some conditioning information. Models such as the CAPM imply a conditional linear factor model with respect to investors information sets. However, the best we can hope to do is to test implications conditioned down on variables that we can observe and include in a test. Thus, a conditional linear factor model is not testable! I like to call this observation the Hansen Richard critique by analogy to the Roll Critique. Roll pointed out, among other things, that the wealth portfolio might not be observable, making tests of the CAPM impossible. Hansen and Richard point out that the conditioning information of agents might not be observable, and that one cannot omit it in testing a conditional model. Thus, even if the wealth portfolio were observable, the fact that we cannot observe agents information sets dooms tests of the CAPM.",
        "146 8. Conditioning Information Some factor models are conditional models, and have coefficients that are functions of investors information sets. In general, there is no way to test such models, but if you are willing to assume that the relevant conditioning information is well summarized by a few variables, then you can just add new factors, equal to the old factors scaled by the conditioning variables, and again forget that you ever heard about conditioning information. You may want to remember conditioning information as a diagnostic, or when giving an economic interpretation of the results. It may be interesting to take estimates of a many factor model, mt a0 a1zt b0 ft 1 b1zt ft 1, and see what they say about the implied conditional model, mt a0 a1zt b0 b1zt ft 1. You may want to make plots of conditional b s, betas, factor risk premia, expected returns, etc. But you do not have to worry a lot about conditioning information in estimation and testing. Problems Chapter 8 1. If there is a risk free asset, is it on the a conditional, or b unconditional mean variance frontier, or c on both? 2. If there is a conditionally risk free asset a claim to 1 is traded at each date, does this mean that there is an unconditionally risk free asset? Define the latter first! How about vice versa? 3. Suppose you took the unconditional population moments E R , E RR of assets returns and constructed the mean variance frontier. Does this frontier correspond to the conditional or the unconditional mean variance frontier, or neither? 4. a Showthat2 x E2 x 2 E x .Whendovari t 1 t t 1 t t 1 ances condition down when is the unconditional variance equal to the average conditional variance? Hint: Start with xt 1 Et xt 1 xt 1 Et xt 1 . b Find the analogous decomposition for covariances. When is the unconditional covariance equal to the average conditional covariance? A conditional model does not necessarily imply an unconditional model, but a conditional model might, with some other side conditions, condition down. Using the ER  representation and using the m a bf representa tion, show that the following three conditions are each sufficient for a model to condition down. To keep things simple, consider only the case of excess returns 0 E mRe ; E Re   , and without loss of generality normalize tttt 5.",
        "13.5. Mean Variance Frontier and Performance Evaluation 263 Figure 13.1. Mean variance frontiers might intersect rather than coincide. De Santis 1993 and Chen and Knez 1995, 1996 show how to test for spanning as opposed to intersection. For intersection, m a bd R d will price both Rd and Rf only for one value of a, or equivalently E m or choice of the intercept, as shown. If the frontiers coincide or span, then m a bd Rd pricesbothRd andRf foranyvalueofa.Thus,wecantestforcoincident frontiers by testing whether m a bd Rd prices both Rd and Rf for two prespecified values of a simultaneously. To see how this works, start by noting that there must be at least two assetsinRd.Ifnot,thereisnomean variancefrontierofRd assets;itissimply a point. If there are two assets in Rd, Rd1 and Rd2, then the mean variance frontier of domestic assets connects them; they are each on the frontier. If they are both on the frontier, then there must be discount factors and m 1 a 1 b 1 R d 1 m 2 a 2 b 2 R d 2 and, of course, any linear combination, m a1 1  a2 b 1Rd1 1  b 2Rd2 . Equivalently, for any value of a, there is a discount factor of the form m a b1Rd1 b2Rd2 ."
    ],
    "Topic 3": [
        "Contents ix 9.5 APTvs.ICAPM.................................... 182 Problems......................................... 183 Part II. Estimating and Evaluating Asset Pricing Models 185 10 GMM in Explicit Discount Factor Models 189 10.1 TheRecipe ....................................... 190 10.2 InterpretingtheGMMProcedure .................... 192 10.3 ApplyingGMM.................................... 196 11 GMM: General Formulas and Applications 201 11.1 GeneralGMMFormulas ............................ 202 11.2 TestingMoments .................................. 206 11.3 StandardErrorsofAnythingbyDeltaMethod .......... 207 11.4 UsingGMMforRegressions ......................... 207 11.5 Prespecified Weighting Matrices and Moment Conditions 210 11.6 Estimating on One Group of Moments, Testing onAnother....................................... 218 11.7 EstimatingtheSpectralDensityMatrix ................ 219 Problems......................................... 227 12 Regression Based Tests of Linear Factor Models 229 12.1 Time SeriesRegressions ............................ 230 12.2 Cross SectionalRegressions.......................... 235 12.3 Fama MacBethProcedure .......................... 245 Problems......................................... 251 13 GMM for Linear Factor Models in Discount Factor Form 253 13.1 GMM on the Pricing Errors Gives a Cross Sectional Regression........................................ 253 13.2 TheCaseofExcessReturns.......................... 256 13.3 HorseRaces ...................................... 259 13.4 TestingforPricedFactors:Lambdasorb s?............. 260 13.5 Mean Variance Frontier and Performance Evaluation . . . . 262 13.6 TestingforCharacteristics........................... 264 Problems......................................... 265 14 Maximum Likelihood 267 14.1 MaximumLikelihood .............................. 268 14.2 MLisGMMontheScores........................... 270 14.3 When Factors Are Returns, ML Prescribes a Time Series Regression........................................ 272",
        "x Contents 14.4 When Factors Are Not Excess Returns, ML Prescribes aCross SectionalRegression......................... 275 Problems......................................... 277 15 Time Series, Cross Section, and GMM DF Tests of Linear Factor Models 279 15.1 Three Approaches to the CAPM in Size Portfolios . . . . . . . 280 15.2 MonteCarloandBootstrap.......................... 286 16 Which Method? Part III. 17 Option Pricing 293 Bonds and Options 309 313 17.1 Background ...................................... 313 17.2 Black ScholesFormula ............................. 320 Problems......................................... 326 18 Option Pricing without Perfect Replication 327 18.1 OntheEdgesofArbitrage........................... 327 18.2 One PeriodGood DealBounds ...................... 329 18.3 MultiplePeriodsandContinuousTime................ 336 18.4 Extensions, Other Approaches, and Bibliography . . . . . . . 345 Problems......................................... 347 19 Term Structure of Interest Rates 349 19.1 19.2 19.3 19.4 19.5 19.6 Part IV. DefinitionsandNotation............................ 349 YieldCurveandExpectationsHypothesis .............. 355 Term Structure Models A Discrete Time Introduction . . 357 Continuous TimeTermStructureModels .............. 362 ThreeLinearTermStructureModels ................. 368 BibliographyandComments ........................ 379 Problems......................................... 382 Empirical Survey 385 20 Expected Returns in the Time Series and Cross Section 389 20.1 Time SeriesPredictability ........................... 391 20.2 The Cross Section: CAPM and Multifactor Models . . . . . . . 435 20.3 SummaryandInterpretation ........................ 449 Problems......................................... 453",
        "78 5. Mean Variance Frontier and Beta Representations 5.1 Expected Return Beta Representations The expected return beta expression of a factor pricing model is E Ri  i,aa i,bb . The model is equivalent to a restriction that the intercept is the same for all assets in time series regressions. When the factors are excess returns, then a E f a . If the test assets are also excess returns, then the intercept should be zero,  0. Much empirical work in finance is cast in terms of expected return beta representations of linear factor pricing models, of the form E Ri  i,aa i,bb , i 1,2,...,N. 5.1 The  terms are defined as the coefficients in a multiple regression of returns on factors, Ri a  fa  fb i, t 1,2,...,T. 5.2 tii,ati,bt t This is often called a time series regression, since one runs a regression over time for each security i. The factors f are proxies for marginal utility growth. I discuss the stories used to select factors at some length in Chapter 9. For the moment keep in mind the canonical examples, f consumption growth, or f the return on the market portfolio CAPM . Notice that we run returns R i on contemporaneous factors f j . This regression is not tt about predicting returns from variables seen ahead of time. Its objective is to measure contemporaneous relations or risk exposure: whether returns are typically high in good times or bad times as measured by the factors. The point of the beta model 5.1 is to explain the variation in aver age returns across assets. I write i 1, 2, . . . , N in 5.1 to emphasize this fact. The model says that assets with higher betas should get higher average returns. Thus the betas in 5.1 are the explanatory x variables, which vary asset by asset. The  and  common for all assets are the intercept and slope in this cross sectional relation. For example, equation 5.1 says that if we plot expected returns versus betas in a one factor model, we should expect all E Ri ,i pairs to line up on a straight line with slope  and intercept  . i,a isinterpretedastheamountofexposureofassetitofactorarisks, and a is interpreted as the price of such risk exposure. Read the beta pricing model to say: for each unit of exposure  to risk factor a, you must provide investors with an expected return premium a. Assets must give investors higher average returns low prices if they pay off well in times",
        "5.1. Expected Return Beta Representations 79 that are already good, and pay off poorly in times that are already bad, as measured by the factors. One way to estimate the free parameters , and to test the model 5.1 is to run a cross sectional regression of average returns on betas, E Ri  i,aa i,bb i, i 1,2,...,N. 5.3 Again, the i are the right hand variables, and the  and  are the intercept and slope coefficients that we estimate in this cross sectional regression. The errors i are pricing errors. The model predicts i 0, and they should be statistically insignificant and economically small in a test. In the chapters on empirical technique, we will see test statistics based on the sum of squared pricing errors. The fact that the betas are regression coefficients is crucially important. If the betas are also free parameters, then there is no content to the model. More importantly and this is an easier mistake to make , the betas can not be asset specific or firm specific characteristics, such as the size of the firm, book to market ratio, or to take an extreme example the first let ter of its ticker symbol. It is true that expected returns are associated with or correlated with many such characteristics. Stocks of small companies or of com panies with high book market ratios do have higher average returns. But this correlation must be explained by some beta regression coefficient. The proper betas should drive out any characteristics in cross sectional regres sions. If, for example, expected returns were truly related to size, one could buy many small companies to form a large holding company. It would be a large company, and hence pay low average returns to the shareholders, while earning a large average return on its holdings. The managers could enjoy the difference. What ruins this promising idea? The large holding company will still behave like a portfolio of small stocks it will have their high betas. Thus, only if asset returns depend on how you behave, not who you are on betas rather than characteristics can a market equilibrium survive such simple repackaging schemes. Some Common Special Cases If there is a risk free rate, its betas in 5.1 are all zero,1 so the intercept is equal to the risk free rate, Rf . 1 The betas are zero because the risk free rate is known ahead of time. When we consider the effects of conditioning information, i.e., that the interest rate could vary over time, we have to interpret the means and betas as conditional moments. Thus, if you are worried about time varying risk free rates, betas, and so forth, either assume all variables are i.i.d. and thus that the risk free rate is constant , or interpret all moments as conditional on time t information.",
        "80 5. Mean Variance Frontier and Beta Representations We can impose this condition rather than estimate  in the cross sectional regression 5.3 . If there is no risk free rate, then  must be estimated in the cross sectional regression. Since it is the expected return of a portfolio with zero betas on all factors,  is called the expected zero beta rate in this circumstance. We often examine factor pricing models using excess returns directly. There is an implicit, though not necessarily justified, division of labor between models of interest rates and models of equity risk premia. Differencing 5.1 between any two returns R ei R i R j R j does not have to be risk free , we obtain E Rei i,aa i,bb , i 1,2,...,N. 5.4 Here, ia represents the regression coefficient of the excess return Rei on the factors. This formulation removes the intercept  . It is often the case that the factors are also returns or excess returns. For example, the CAPM uses the return on the market portfolio as the single factor. In this case, the model should apply to the factors as well, and this fact allows us to measure the  coefficients directly rather than via a cross sectional regression. Each factor has beta of one on itself and zero on all the other factors, of course. Therefore, if the factors are excess returns, we have E f a a, and so forth. We can then write the factor model as E R ei i , a E f a i , b E f b , i 1, 2, . . . , N . 5.5 The cross sectional beta pricing model 5.1 5.5 and the time series regression definition of the betas in 5.2 look very similar. It seems that one can take expectations of the time series regression 5.2 and arrive at the beta model 5.1 , in which case the latter would be vacuous since one can always run a regression of anything on anything. The difference is subtle but crucial: the time series regressions 5.2 will in general have a different intercept ai for each return i, while the intercept  is the same for all assets in the beta pricing equation 5.1 . The beta pricing equation is a restriction on expected returns, and thus imposes a restriction on intercepts in the time series regression. In the special case that the factors are themselves excess returns, the restriction is particularly simple: the time series regression intercepts should all be zero. In this case, we can avoid the cross sectional regression entirely, since there are no free parameters left.",
        "7 Implications of Existence and Equivalence Theorems The existence of a discount factor means that p E mx is innocuous, and all content flows from the discount factor model. The theorems apply to sample moments too; the dangers of fishing up ex post or sample mean variance efficient portfolios. Sources of discipline in factor fishing expeditions. The joint hypothesis problem. How efficiency tests are the same as tests of economic discount factor models. Factors vs. their mimicking portfolios. Testing the number of factors. Plotting contingent claims on the axis vs. mean and variance. The theorems on the existence of a discount factor, and the equiva lence between the p E mx , expected return beta, and mean variance views of asset pricing have important implications for how we approach and evaluate empirical work. The equivalence theorems are obviously important to the theme of this book. They show that the choice of discount factor language versus expected return beta language or mean variance frontier is entirely one of convenience. Nothing in the more traditional statements is lost. p E mx is Innocuous Before Roll 1977 , expected return beta representations had been derived in the context of special and explicit economic models, especially the CAPM. In empirical work, the success of any expected return beta model seemed like a vindication of the whole structure. The fact that, for example, one might use the NYSE value weighted index portfolio in place of the return on total wealth predicted by the CAPM seemed like a minor issue of empirical implementation. 121",
        "122 7. Implications of Existence and Equivalence Theorems When Roll showed that mean variance efficiency implies a single beta representation, all that changed. Some single beta representation always exists, since there is some mean variance efficient return. The asset pric ing model only serves to predict that a particular return say, the market return will be mean variance efficient. Thus, if one wants to test the CAPM it becomes much more important to be choosy about the refer ence portfolio, to guard against stumbling on something that happens to be mean variance efficient and hence prices assets by construction. This insight led naturally to the use of broader wealth indices Stambaugh 1982 in the reference portfolio, to provide a more grounded test of the CAPM. However, this approach has not caught on. Stocks are priced with stock factors, bonds with bond factors, and so on. More recently, stocks sorted on size, book market, and past performance characteristics are priced by portfolios sorted on those characteristics Fama and French 1993 , 1996 . Part of the reason for this result is that the betas are small; asset classes are not highly correlated so risk premia from one source of betas have small impacts on another set of average returns. Also, more compre hensive wealth measures that include human capital and real estate do not come with high frequency price data, so adding them to a wealth portfolio has little effect on betas. However, one is left with the nagging and excit ing, to a researcher suspicion that markets may be somewhat segmented, especially at high frequency. The good news in Roll s existence theorem is that you can always start by writing an expected return beta model, knowing that you have imposed almost no structure in doing so. The bad news is that you have not gotten very far. All the economic, statistical, and predictive content comes in picking the factors. The theorem that, from the law of one price, there exists some discount factor m such that p E mx is just an updated restatement of Roll s theo rem. The content is all in m f data , not in p E mx . Again, an asset pricing framework that initially seemed to require a lot of completely unbe lievable structure the representative consumer consumption based model in complete frictionless markets turns out to require almost no structure at all. Again, the good news is that you can always start by writing p E mx , and need not suffer criticism about hidden contingent claim or represen tative consumer assumptions in so doing. The bad news is that you have not gotten very far by writing p E mx as all the economic, statistical, and predictive content comes in picking the discount factor model m f data . Ex Ante and Ex Post I have been deliberately vague about the probabilities underlying expecta tions and other moments in the theorems. The fact is, the theorems hold for",
        "Implications of Existence and Equivalence Theorems 125 out of sample and across different markets, is to try to understand the fundamental macroeconomic sources of risk. By this I mean, tying asset prices to macroeconomic events, in the way the ill fated consumption based model does via mt 1 u ct 1 u ct . The difficulties of the consumption based model have made this approach lose favor in recent years. However, the alternative approach is also running into trouble in that the number and identity of empirically determined risk factors do not seem stable. In the quarter century since Merton 1973a and Ross 1976a inaugurated multiple factor models, the standard set of risk factors has changed about every two years. Efforts such as Lettau and Ludvigson 2001a , to find macroeconomic explanations for empirically determined risk factors may prove a useful compromise. In any case, one should always ask of a factor model, what is the com pelling economic story that restricts the range of factors used? and or what statistical restraints are used to keep from discovering ex post mean variance efficient portfolios, or to ensure that the results will be robust across sam ples. The existence theorems tell us that the answers to these questions are the only content of the exercise. If the purpose of the model is not just to predict asset prices but also to explain them, this puts an additional burden on economic motivation of the risk factors. There is a natural resistance to such discipline built in to our current statistical methodology for evaluating models and papers. When the last author fished around and produced an ad hoc factor pricing model that generates 1 average pricing errors, it is awfully hard to persuade read ers, referees, journal editors, and clients that your economically motivated factor pricing model is interesting despite 2 average pricing errors. Your model may really be better and will therefore continue to do well out of sample when the fished model falls by the wayside of financial fashion, but it is hard to get past statistical measures of in sample fit. One hungers for a formal measurement of the number of hurdles imposed on a factor fishing expedition, like the degrees of freedom correction in R 2. Absent a numerical correction, we have to use judgment to scale back apparent statistical successes by the amount of economic and statistical fishing that produced them. Mimicking Portfolios Thetheoremx proj m X alsohasinterestingimplicationsforempirical work. The pricing implications of any model can be equivalently represented by its factor mimicking portfolio. If there is any measurement error in a set of economic variables driving m, the factor mimicking portfolios for the true m will price assets better than an estimate of m that uses the measured macroeconomic variables.",
        "126 7. Implications of Existence and Equivalence Theorems Thus, it is probably not a good idea to evaluate economically interesting models with statistical horse races against models that use portfolio returns as factors. Economically interesting models, even if true and perfectly measured, will just equal the performance of their own factor mimicking portfolios, even in large samples. Add any measurement error, and the eco nomic model will underperform its own factor mimicking portfolios. And both models will always lose in sample against ad hoc factor models that find nearly ex post efficient portfolios. This said, there is an important place for models that use returns as factors. After we have found the underlying macro factors, practitioners will be well advised to look at the factor mimicking portfolio on a day by day basis. Good data on the factor mimicking portfolios will be available on a minute by minute basis. For many purposes, one does not have to understand the economic content of a model. But this fact does not tell us to circumvent the process of understand ing the true macroeconomic factors by simply fishing for factor mimicking portfolios. The experience of practitioners who use factor models seems to bear out this advice. Large commercial factor models resulting from exten sive statistical analysis otherwise known as fishing perform poorly out of sample, as revealed by the fact that the factors and loadings  change all the time. Irrationality and Joint Hypothesis Finance contains a long history of fighting about rationality versus irrationality and efficiency versus inefficiency of asset markets. The results of many empirical asset pricing papers are sold as evidence that markets are inefficient or that investors are irrational. For example, the crash of October 1987, and various puzzles such as the small firm, book market, seasonal effects, or long term predictability have all been sold this way. However, none of these puzzles documents an exploitable arbitrage opportunity. Therefore, we know that there is a rational model a stochastic discount factor, an efficient portfolio to use in a single beta representation that rationalizes them all. And we can confidently pre dict this situation to continue; real arbitrage opportunities do not last long! Fama 1970 contains a famous statement of the same point. Fama emphasized that any test of efficiency is a joint test of efficiency and a model of market equilibrium. Translated, an asset pricing model, or a model of m. No test based only on asset market data can conclu sively show that markets are rational or not. Small wonder that 30 years and thousands of papers have not moved the debate an inch closer to resolution.",
        "136 8. Conditioning Information distribution of returns. Exclusion of potential instruments is exactly the same thing as exclusion of assets. It is no better founded, but the fact that it is a common sin may lead one to worry less about it. There is nothing really special about unscaled returns, and no funda mental economic reason to place them above scaled returns. A mutual fund might come into being that follows the managed portfolio strategy and then its unscaled returns would be the same as an original scaled return. Models that cannot price scaled returns are no more interesting than models that can only price say stocks with first letter A through L. There are some econometric reasons to trust results for nonscaled returns a bit more, since the correlation of a slow moving instrument with a discounted payoff may be poorly measured in short samples. Transactions costs raise doubts about instruments that move quickly, implying highly dynamic trading strategies. But transactions costs also raise doubts about the unconditional pricing errors of small illiquid and thinly traded stocks. Of course, the other way to incorporate conditioning information is by constructing explicit parametric models of conditional distributions. With this procedure one can in practice test all of a model s implica tions about conditional moments. However, the parametric model may be incorrect, or may not reflect some variable used by investors. Includ ing instruments may not be as efficient if the statistical model is literally and completely true, but it is still consistent if the parametric model is incorrect. 8.3 Conditional and Unconditional Models A conditional factor model does not imply a fixed weight or uncondi tional factor model: 1 m b f ,p E m x doesnotimplythat bs.t.m t 1 t t 1 t t t 1 t 1 b ft 1,E pt E mt 1xt 1 . t 1 2 E R   doesnotimplyE R tt 1 tt  . 3 Conditional mean variance efficiency does not imply unconditional t 1 mean variance efficiency. The converse statements are true, if managed portfolios are included. For explicit discount factor models models whose parameters are con stant over time the fact that one looks at conditional versus unconditional implications makes no difference to the statement of the model: pt Et mt 1xt 1 E pt E mt 1xt 1",
        "144 8. Conditioning Information 8.4 Scaled Factors: A Partial Solution Theproblemisthattheparametersofthefactorpricingmodelmt 1 a t b t f t 1 may vary over time. A partial solution is to model the dependence of parametersat andbt onvariablesinthetime tinformationset;letat a zt , bt b zt , where zt is a vector of variables observed at time t including a constant . In particular, why not try linear models at a zt, bt b zt. Linearity is not restrictive: zt2 is just another instrument. The only crit icism one can make is that some instrument zjt might be important for capturing the variation in at and bt, and was omitted. For instruments on which we have data, we can meet this objection by trying zjt and seeing whetheritdoes,infact,entersignificantly.However,forinstrumentszt that are observed by agents but not by us, this criticism remains valid. Linear discount factor models lead to a nice interpretation as scaled factors, in the same way that linearly managed portfolios are scaled returns. With a single factor and instrument, write mt 1 a zt b zt ft 1 8.6 a0 a1zt b0 b1zt ft 1 a0 a1zt b0ft 1 b1 zt ft 1 . 8.7 Thus, in place of the one factor model with time varying coefficients 8.6 , wehaveathree factormodel zt,ft 1,zt ft 1 withfixedcoefficients, 8.7 . Since the coefficients are now fixed, we can use the scaled factor model with unconditional moments: pt Et a0 a1zt b0ft 1 b1ztft 1 xt 1 E pt E a0 a1zt b0 ft 1 b1 zt ft 1 xt 1 . For example, in standard derivations of CAPM, the market wealth port folio return is conditionally mean variance efficient; investors want to hold portfolios on the conditional mean variance frontier; conditionally expected returns follow a conditional single beta representation, or the discount factor You can expand the set of factors to test conditional factor pricing models factors ft 1 zt .",
        "8.5. Summary 145 m follows a conditional linear factor model m a bRW t 1 t tt 1 as we saw above. But none of these statements mean that we can use the CAPM uncondi tionally. Rather than throw up our hands, we can add some scaled factors. Thus, if, say, the dividend price ratio and term premium do a pretty good job of summarizing variation in conditional moments, the conditional CAPM implies an unconditional, five factor plus constant model. The factors are a con stant, the market return, the dividend price ratio, the term premium, and the market return times the dividend price ratio and the term premium. Jagannathan and Wang 1996 test whether similar factors can explain CAPM anomalies. The unconditional pricing implications of such a five factor model could, of course, be summarized by a single beta representation. The refer ence portfolio would not be the market portfolio, of course, but a mimicking portfolio of the five factors. However, the single mimicking portfolio would not be easily interpretable in terms of a single factor conditional model and two instruments. In this case, it might be more interesting to look at a multiple beta or multiple factor representation. Ifwehavemanyfactorsf andmanyinstrumentsz,weshouldinprinciple multiply every factor by every instrument, m b1f1 b2f1z1 b3f1z2 bN 1f2 bN 2f2z1 bN 3f2z2 . This operation can be compactly summarized with the Kronecker product nota tion, a b, which means multiply every element in vector a by every element in vector b, or mt 1 b ft 1 zt . 8.5 Summary When you first think about it, conditioning information sounds scary how do we account for time varying expected returns, betas, fac tor risk premia, variances, covariances, etc. However, the methods outlined in this chapter allow a very simple and beautiful solution to the problems raised by conditioning information. To express the conditional implica tions of a given model, all you have to do is include some well chosen scaled or managed portfolio returns, and then pretend you never heard about conditioning information.",
        "Problems 147 your factors to have conditional mean zero, m 1 b ft 1; Et ft 1 0 with excess returns, the mean Et m is unidentified . a bt b constant t vart ff b;marketpricesofriskmove one for one with conditional variance, no restriction on conditional betas. b cov R e , f constant, even though b may vary over time. tt 1t 1 t c Constant conditional betas, var ff 1cov Re ,f  even t t t 1 t 1 though the individual covariance and variance may vary arbitrarily over time. No limit on .",
        "190 10. GMM in Explicit Discount Factor Models 10.1 The Recipe Definitions: GMM estimate: Standard errors: ut 1 b mt 1 b xt 1 pt , g b ET ut b , T S  b2 argmin g b S 1g b .  1 g b var b2 d S 1d 1; d T . j E ut b ut j b . bTT T b Test of the model overidentifying restrictions : TJT T min g b S 1g b 2 moments parameters . TT It is easiest to start our discussion of GMM in the context of an explicit discount factor model, such as the consumption based model. I treat the special structure of linear factor models later. I start with the basic classic recipe as given by Hansen and Singleton 1982 . Discount factor models involve some unknown parameters as well as data, so I write mt 1 b when it is important to remind ourselves of this dependence. For example, if mt 1  ct 1 ct  , then b   . I write b to denote an estimate when it is important to distinguish estimated from other values. Any asset pricing model implies E pt E mt 1 b xt 1 . It is easiest to write this equation in the form E 0, E mt 1 b xt 1 pt 0. 10.3 10.4 x and p are typically vectors; we typically check whether a model for m can price a number of assets simultaneously. Equations 10.4 are often called the moment conditions. It is convenient to define the errors ut b as the object whose mean should be zero, ut 1 b mt 1 b xt 1 pt .",
        "192 where or, more explicitly, d ET More precisely, d should be written as the object to which g b con T verges, and g b is an estimate of that object used to form a consistent T estimate of the asymptotic variance covariance matrix. This variance covariance matrix can be used to test whether a parameter or group of parameters is equal to zero, via 10. GMM in Explicit Discount Factor Models g b T d b mt 1 b xt 1 pt . b b b and b i N 0,1 var b ii b var b 1b 2 included b j jj j wherebj subvector,var b jj submatrix. Finally, the test of overidentifying restrictions is a test of the overall fit of the model. It states that T times the minimized value of the second stage objective is distributed 2 with degrees of freedom equal to the number of moments less the number of estimated parameters: TJT T min g b S 1g b 2 moments parameters . b T T 10.2 Interpreting the GMM Procedure g b isapricingerror.Itisproportionalto. T GMM picks parameters to minimize a weighted sum of squared pricing errors. The second stage picks the linear combination of pricing errors that are best measured, by having smallest sampling variation. First and second stage are like OLS and GLS regressions. The standard error formula can be understood as an application of the delta method. TheJT testevaluatesthemodelbylookingatthesumofsquaredpricing errors.",
        "196 10. GMM in Explicit Discount Factor Models parameter and one moment. S T is the variance matrix of the moment g . d . Then the delta method formula gives 1 1 TT is g b b g var b2 var g . T  b b g T g The actual formula 10.7 just generalizes this idea to vectors. JT Test TT Once you have estimated the parameters that make a model fit best, the natural question is, how well does it fit? It is natural to look at the pricing errors and see if they are big. The JT test asks whether they are big by statistical standards if the model is true, how often should we see a weighted sum of squared pricing errors this big? If not often, the model is rejected. The test is TJT T g b S 1g b 2 moments parameters . TT Since S is the variance covariance matrix of gT , this statistic is the minimized pricing errors divided by their variance covariance matrix. Sample means converge to a normal distribution, so sample means squared divided by variance converges to the square of a standard normal, or 2. The reduction in degrees of freedom corrects for the fact that S is really the covariance matrix of gT for fixed b. We set a linear combination of the gT to zero in each sample, so the actual covariance matrix of gT is singular, with rank moments parameters. 10.3 Applying GMM Notation; Instruments and Returns Most of the effort involved with GMM is simply mapping a given problem into the very general notation. The equation Emt 1 b xt 1 pt 0 Notation. Forecast errors and instruments. Stationarity and choice of units.",
        "11 GMM: General Formulas and Applications Lots of calculations beyond parameter estimation and model testing are useful in the process of evaluating a model and comparing it to other models. You still want to understand sampling variation in such calculations, and mapping the questions into the GMM framework allows you to do this easily. In addition, alternative estimation and evaluation procedures may be more intuitive or robust to model misspecification than the two or multi stage procedure described in the last chapter. In this chapter I lay out the general GMM framework, and I discuss five applications and variations on the basic GMM method. 1 I show how to derive standard errors of nonlinear functions of sample moments, such as correlation coefficients. 2 I apply GMM to OLS regressions, eas ily deriving standard error formulas that correct for autocorrelation and conditional heteroskedasticity. 3 I show how to use prespecified weight ing matrices W in asset pricing tests in order to overcome the tendency of efficient GMM to focus on spuriously low variance portfolios. 4 As a good parable for prespecified linear combination of moments a, I show how to mimic calibration and evaluation phases of real business cycle models. 5 I show how to use the distribution theory for the gT beyond just forming the JT test in order to evaluate the importance of individual pricing errors. The next chapter continues, and collects GMM variations useful for evaluating linear factor models and related mean variance frontier questions. ManyofthesecalculationsamounttocreativechoicesoftheaT matrix that selects which linear combination of moments are set to zero, and reading off the resulting formulas for variance covariance matrix of the esti mated coefficients, equation 11.4 and variance covariance matrix of the moments gT , equation 11.5 . 201",
        "12 Regression Based Tests of Linear Factor Models The next four chapters study the question, how should we estimate and evaluate linear factor models: models of the form p E mx , m b f or equivalently E Re  ? These models are by far the most common in empirical asset pricing, and there is a large literature on econometric tech niques to estimate and evaluate them. Each technique focuses on the same questions: how to estimate parameters, how to calculate standard errors of the estimated parameters, how to calculate standard errors of the pricing errors, and how to test the model, usually with a test statistic of the form  V 1  . I start in this chapter with simple and long standing time series and cross sectional regression tests. In Chapter 13, I pursue the GMM approach to the model expressed in p E mx , m b f form. Chapter 14 summa rizes the principle of maximum likelihood estimation and derives maximum likelihood estimates and tests. Finally, Chapter 15 compares the different approaches. As always, the theme is the underlying unity. All of the techniques come down to one of two basic ideas: time series regression or cross sectional regression. Time series regression turns out to be a limiting case of cross sectional regression. The GMM, p E mx approach turns out to be almost identical to cross sectional regressions. Maximum likelihood with appro priate statistical assumptions justifies the time series and cross sectional regression approaches. The formulas for parameter estimates, standard errors, and test statistics are all strikingly similar. 229",
        "230 12. Regression Based Tests of Linear Factor Models 12.1 Time Series Regressions When the factor is also a return, we can evaluate the model E Rei iE f by running OLS time series regressions Rei   f i, t 1,2,...,T, tiitt for each asset. The OLS distribution formulas with corrected standard errors provide standard errors of  and . With errors that are i.i.d. over time, homoskedastic, and independent of the factors, the asymptotic joint distribution of the intercepts gives the model test statistic, E f 2 1 T 1 T  1   2 . N to this statistic, when the errors are also normally distributed, 1 ET f 1ET f  f The Gibbons Ross Shanken test is a multivariate, finite sample counterpart T N K N 1  1 FN,T N K. I show how to construct the same test statistics with heteroskedastic and autocorrelated errors via GMM. I start with the simplest case. We have a factor pricing model with a single factor. The factor is an excess return for example, the CAPM, with R em R m R f , and the test assets are all excess returns. We express the model in expected return beta form. The betas are defined by regression coefficients Rei  f i 12.1 tiitt and the model states that expected returns are linear in the betas: E Rei iE f . 12.2 Since the factor is also an excess return, the model applies to the factor as well, so E f 1 . Comparing the model 12.2 and the expectation of the time series regression 12.1 , we see that the model has one and only one implication for the data: all the regression intercepts i should be zero. The regression intercepts are equal to the pricing errors. Given this fact, Black, Jensen, and Scholes 1972 suggested a natural strategy for estimation and evaluation: Run time series regressions 12.1 for",
        "12.1. Time Series Regressions 231 each test asset. The estimate of the factor risk premium is just the sample mean of the factor,  E T f . Then, use standard OLS formulas for a distribution theory of the para meters. In particular, you can use t tests to check whether the pricing errors  are in fact zero. These distributions are usually presented for the case that the regression errors in 12.1 are uncorrelated and homoskedastic, but the formulas in Section 11.4 show easily how to calculate standard errors for arbitrary error covariance structures. We also want to know whether all the pricing errors are jointly equal to zero. This requires us to go beyond standard formulas for the regres sion 12.1 taken alone, as we want to know the joint distribution of  estimates from separate regressions running side by side but with errors correlated across assets E i  j 0 . We can think of 12.1 as a panel tt regression, and then it is a test whether the firm dummies are jointly zero. The classic form of these tests assume no autocorrelation or heteroskedas ticity. Dividing the  regression coefficients by their variance covariance matrix leads to a 2 test, 1 E f 2 T1 T  f  1 2, N 12.3 where ET f denotes sample mean,  2 f denotes sample variance,  is a vector of the estimated intercepts,   1  2  N . is the residual covariance matrix, i.e., the sample estimate of E   , tt where As usual when testing hypotheses about regression coefficients, this test is valid asymptotically. The asymptotic distribution theory assumes that  2 f i.e., X X and have converged to their probability limits; there fore, it is asymptotically valid even though the factor is stochastic and is estimated, but it ignores those sources of variation in a finite sample. It does not require that the errors are normal, relying on the central limit theorem so that  is normal. I derive 12.3 below. Also as usual in a regression context, we can derive a finite sample F distribution for the hypothesis that a set of parameters are jointly zero, T N 1 N 1 E f 2 1 T  1 FN,T N 1. 12.4  f  12 N . tttt",
        "232 12. Regression Based Tests of Linear Factor Models This is the Gibbons, Ross, and Shanken 1989 or GRS test statistic. The F distribution recognizes sampling variation in , which is not included in 12.3 . This distribution requires that the errors  are normal as well as uncorrelated and homoskedastic. With normal errors, the  are normal and is an independent Wishart the multivariate version of a 2 , so the ratio is F . This distribution is exact in a finite sample. Tests 12.3 and 12.4 have a very intuitive form. The basic part of the test is a quadratic form in the pricing errors,  1. If there were no f in the model, then the  would simply be the sample mean of the regression errors t . Assuming i.i.d. t , the variance of their sample mean is just 1 T . Thus, if we knew , then T  1 would be a sum of squared sample means divided by their variance covariance matrix, which would have an asymptotic 2 distribution, or a finite sample 2 distribution if the  are normal. But NNt we have to estimate , which is why the finite sample distribution is F rather than 2. We also estimate the , and the second term in 12.3 and 12.4 accounts for that fact. Recall that a single beta representation exists if and only if the reference return is on the mean variance frontier. Thus, the test can also be inter preted as a test whether f is ex ante mean variance efficient whether it is on the mean variance frontier using population moments after accounting for sampling error. Even if f is on the true or ex ante mean variance fron tier, other returns will outperform it in sample due to luck, so the return f will usually be inside the ex post mean variance frontier i.e., the frontier drawn using sample moments. Still, it should not be too far inside the sam ple frontier. Gibbons, Ross, and Shanken show that the test statistic can be expressed in terms of how far inside the ex post frontier the return f is, T N 1 q q ET f  f 22 . 12.5   2 is the Sharpe ratio of the ex post tangency portfolio maximum The asset pricing model Rei   f i. iitt E Rei  E f i N 1 ET f  f 2 qq ex post Sharpe ratio formed from the test assets plus the factor f . The last term in the numerator is the Sharpe ratio of the factor, so the numerator expresses how for the factor is inside the ex post frontier. If there are many factors that are excess returns, the same ideas work, with some cost of algebraic complexity. The regression equation is again predicts that the intercepts should be zero. We can estimate  and  with OLS time series regressions. Assuming normal i.i.d. errors,",
        "12.1. Time Series Regressions 233 the quadratic form  1 has the distribution 1 ET f 1ET f N number of assets, K number of factors, T N K N 1  1 FN,T N K, 12.6 where t 1 The main difference is that the Sharpe ratio of the single factor is replaced by the natural generalization ET f 1 ET f . Derivation of The 2 Statistic, and Distributions with General Errors I derive 12.3 as an instance of GMM. This approach allows us to gen erate straightforwardly the required corrections for autocorrelated and heteroskedastic disturbances. MacKinlay and Richardson 1991 advocate GMM approaches to regression tests in this way. It also serves to remind us that GMM and p E mx are not necessarily paired; one can do a GMM estimate of an expected return beta model, too. The mechanics are only slightly different than what we did to generate distributions for OLS regres sion coefficients in Section 11.4, since we keep track of N OLS regressions simultaneously. Write the equations for all N assets together in vector form, Re  f . 1 T T t 1 1 T ft ET f ft ET f ,   . Ttt We use the usual OLS moments to estimate the coefficients, ttt E Re  f g b T t t E t 0.  T E Re  f f T f Tttttt These moments exactly identify the parameters ,, so the a matrix in ag b 0istheidentitymatrix.Solving,theGMMestimatesareofcourse T the OLS estimates,  E R e  E f , TtTt E Re E Re f cov Re,f  T t T t t T t t . ET ft ET ft ft varT ft",
        "234 12. Regression Based Tests of Linear Factor Models The d matrix in the general GMM formula is g b IN INE ft 1 E ft d T IN, b I E f I E f2 E f E f2 NtNttt whereIN isanN Nidentitymatrix.TheSmatrixis E  E  f t t j t t j t j . S Using the GMM variance formula 11.4 with a I , we have j E f E f f t t t j t t t j t j  1 1 1 var d Sd . 12.7  T At this point, we are done. The upper left hand corner of var   gives us var  and the test we are looking for is  var  1 2 . N The standard formulas make this expression prettier by assuming that the errors are uncorrelated over time and not heteroskedastic. These assumptions simplify the S matrix, as for the standard OLS formulas in Section 11.4. If we assume that f and  are independent as well as orthog onal, E f  E f E  and E f 2 E f 2 E  . If we assume that the errors are independent over time as well, we lose all the lead and lag terms. Then, the S matrix simplifies to E   E   E f 1 E f S tt ttt t . 12.8 E f E   E   E f 2 E f E f 2 tttttttt Now we can plug into 12.7 . Using A B 1 A 1 B 1 and A B C D AC BD,weobtain  1 1 E ft 1 var .  TE ft E f2 t Evaluating the inverse,  1 1 E f2 E ft var t .  Tvar f E ft 1 Weareinterestedinthetopleftcorner.UsingE f2 E f 2 var f , var  T 1 . 1 E f 2 var f",
        "12.2. Cross Sectional Regressions 235 This is the traditional formula 12.3 . Though this formula is pretty, there is now no real reason to assume that the errors are i.i.d. or independent of the factors. By simply calculating 12.7 , we can easily construct standard errors and test statistics that do not require these assumptions. 12.2 Cross Sectional Regressions We can fit E R ei    ii by running a cross sectional regression of average returns on the betas. This technique can be used whether the factor is a return or not. I discuss OLS and GLS cross sectional regressions, I find formulas for the standard errors of , and a 2 test whether the  are jointly zero. I derive the distributions as an instance of GMM, and I show how to implement the same approach for autocorrelated and heteroskedastic errors. I show that the GLS cross sectional regression is the same as the time series regression when the factor is also an excess return, and is included in the set of test assets. Start again with the K factor model, written as E Rei  , i 1,2,...,N. i The central economic question is why average returns vary across assets; expected returns of an asset should be high if that asset has high betas or a large risk exposure to factors that carry high risk premia. Figure 12.1 graphs the case of a single factor such as the CAPM. Each dot represents one asset i. The model says that average returns should be proportional to betas, so plot the sample average returns against the betas. Even if the model is true, this plot will not work out perfectly in each sample, so there will be some spread as shown. Given these facts, a natural idea is to run a cross sectional regression to fit a line through the scatterplot of Figure 12.1. First find estimates of the betas from time series regressions, Rei a  f i, t 1,2,...,T foreachi. 12.9 tiitt Then estimate the factor risk premia  from a regression across assets of average returns on the betas, E Rei   , i 1,2,...,N. 12.10 Tii As in the figure,  are the right hand variables,  are the regression coeffi cients,andthecross sectionalregressionresidualsi arethepricingerrors.",
        "236 12. Regression Based Tests of Linear Factor Models Figure 12.1. Cross sectional regression. This is also known as a two pass regression estimate, because one estimates first time series and then cross sectional regressions. You can run the cross sectional regression with or without a constant. The theory says that the constant or zero beta excess return should be zero. You can impose this restriction or estimate a constant and see if it turns out to be small. The usual trade off between efficiency impose the null as much as possible to get efficient estimates and robustness applies. OLS Cross Sectional Regression It will simplify notation to consider a single factor; the case of multiple factors looks the same with vectors in place of scalars. I denote vectors from 1 to N with missing sub or superscripts, i.e.,  1 2 N , tttt , and similarly for Rte and . For simplicity take the case of no intercept in the cross sectional regression. With this notation OLS  1 2 N cross sectional estimates are    1  E T R e ,  ET Re . 12.11 Next, we need a distribution theory for the estimated parameters. The most natural place to start is with the standard OLS distribution formulas. I start with the traditional assumption that the true errors are i.i.d. over time, and independent of the factors. This will give us some easily inter pretable formulas, and we will see most of these terms remain when we do the distribution theory right later on.",
        "12.2. Cross Sectional Regressions 237 In an OLS regression Y X u and E uu , the variance of the  estimate is X X 1X X X X 1. The residual covariance matrix is I X X X 1X I X X X 1X . To apply these formulas we need cov ,  , the error covariance in the cross sectional regression. With the traditional assumption that the factors and errors are i.i.d. over time, the answer is cov ,  1  f  , T where cov f,f and cov  . To see this, start with  ftttt E Re .WithRe a f ,wehaveE Re a E f E  . TtttTtTtTt Under the null that the model is correct, so E Re a E f , then,wehavecov , cov ET Re ,ET Re 1  f . Don t T confuse this covariance with the covariance of the estimated  in the cross sectional regression. Like a residual covariance vs. an error covariance, there are additional terms in the covariance of the estimated , which I develop below. Yes, we want the covariance of ET Re , not of E Re , which is a number and has no covariance, or of Rte . ET R e is the y variable in the cross sectional regression. Then, the conventional OLS formulas for the covariance matrices of OLS estimates and residuals, accounting for correlated errors, give T 2  1   1    1 f T cov  12.12 12.13 1 1 1 I    I    The correct formulas, 12.19 and 12.20 , which account for the fact that  are estimated, are straightforward generalizations. The f term cancels in 12.13 . We could test whether all pricing errors are zero with the statistic  cov  1 2 . 12.14 N 1 The distribution is 2 , not 2 , because the covariance matrix is sin N 1 N gular. The singularity and the extra terms in 12.13 result from the fact that the  coefficient was estimated along the way, and means that we have to use a generalized inverse. If there are K factors, we obviously end up with 2 . N K A test of the residuals is unusual in OLS regressions. We do not usually test whether the residuals are too large, since we have no information other than the residuals themselves about how large they should be. In this case, however, the first stage time series regression gives us some indepen dent information about the size of cov  , information that we could not get from looking at the cross sectional residual  itself.",
        "238 12. Regression Based Tests of Linear Factor Models GLS Cross Sectional Regression Since the residuals in the cross sectional regression 12.10 are correlated with each other, standard textbook advice is to run a GLS cross sectional regression rather than OLS, using E  1  f  as the error covariance matrix: T   1 1 1ET Re ,  ET Re . 12.15 The standard regression formulas give the variance of these estimates as The GLS formula is 1 1    1    1 1E Re . ffT However, it turns out that we can drop the  1 term.1 f 1 Here s the algebra. Let Then, Now, Thus, T 2 1  1 1 f , 12.16 12.17 A I  1 1. f cov  1   1 1 . T 1 1 1    1  A 1A  1 ET Re ff 1 1 1 A  1  A  1 ET Re . ff A I  1 1  f  I 1 1 f  1  1 . f   1 1 1ET Re .",
        "12.2. Cross Sectional Regressions 239 The comments of Section 11.5 warning that OLS is sometimes much more robust than GLS apply in this case. The GLS regression should improve efficiency, i.e., give more precise estimates. However, may be hard to estimate and to invert, especially if the cross section N is large. One may well choose the robustness of OLS over the asymptotic statistical advantages of GLS. A GLS regression can be understood as a transformation of the space of returns, to focus attention on the statistically most informative portfolios. Finding say, by Choleski decomposition a matrix C such that CC 1, the GLS regression is the same as an OLS regression of CET R e on C  , i.e., of testing the model on the portfolios CR e . The statistically most informative portfolios are those with the lowest residual variance . But this asymptotic statistical theory assumes that the covariance matrix has converged to its true value. In most samples, the ex post or sample mean variance frontier still seems to indicate lots of luck, and this is especially true if the cross section islarge,anythingmorethan1 10ofthetimeseries.TheportfoliosCRe are likely to contain many extreme long short positions. Again, we could test the hypothesis that all the  are equal to zero with 12.14 . Though the appearance of the statistic is the same, the covari ance matrix is smaller, reflecting the greater power of the GLS test. As with theJT test, 11.10 ,wecandevelopanequivalenttestthatdoesnotrequire a generalized inverse: T 1 2 . 12.18 N 1 Toderive 12.18 ,IproceedexactlyasinthederivationoftheJT test 11.10 . Define, say by Choleski decomposition, a matrix C such that CC 1. Now, find the covariance matrix of TC : cov where TC  C CC 1   CC  1 C I    1 ,  C . cov TC  isanidempotentmatrixwithrankN 1;thereforeT CC  T 1 is2 . N 1 Correction for the Fact that  Are Estimated, and GMM Formulas that Do Not Need i.i.d. Errors In applying standard OLS formulas to a cross sectional regression, we assume that the right hand variables  are fixed. The  in the cross sectional regression are not fixed, of course, but are estimated in the time series regression. This turns out to matter, even asymptotically. In sum,  is asymptotically normal so TC  is asymptotically normal,",
        "240 12. Regression Based Tests of Linear Factor Models In this section, I derive the correct asymptotic standard errors. With the simplifying assumption that the errors  are i.i.d. over time and independent of the factors, the result is 2  1   1    1 1  1 , OLST ff 12.19  2  1  1  1 1  1  , GLST ff where f is the variance covariance matrix of the factors. This correction is due to Shanken 1992b . Comparing these standard errors to 12.12 and 12.16 ,weseethatthereisamultiplicativecorrection 1  1. f The asymptotic variance covariance matrix of the pricing errors is cov OLS 1 IN    1 IN    1 T 1  1 f cov  1   1 1 1  1 . GLST f 12.20 12.21 Comparing these results to 12.13 and 12.17 , we see the same multiplica tive correction. We can form the asymptotic 2 test of the pricing errors by dividing pricing errors by their variance covariance matrix,  cov  1 . Following 12.18 , we can simplify this result for the GLS pricing errors resulting in T 1  1  1 2 . 12.22 f GLS GLS N K Is the correction important relative to the simple to derive regression formulas 12.12 , 12.13 , 12.16 , 12.17 ? In the CAPM,  E Rem so 2  2 R em 0.08 0.16 2 0.25 in annual data. In annual data, then, the multiplicative term is too large to ignore. However, the mean and vari ance both scale with horizon, so the Sharpe ratio scales with the square root of horizon. Therefore, for a monthly interval 2 2 Rem 0.25 12 0.02, which is quite small, so ignoring the multiplicative term makes little difference. Suppose the factor is in fact a return. Then the factor risk premium is  E f , and we use f T, the standard error of the factor mean, as the standard error of . The terms in  correct for the small differences between cross sectional and time series estimates. They are therefore likely to be small, and the f T term is likely to be the most important term. Comparing 12.22 to the GRS tests for a time series regression, 12.3 , 12.4 , 12.6 , we see the same statistic. The only difference is that",
        "12.2. Cross Sectional Regressions 241 by estimating  from the cross section rather than imposing  E f , the cross sectional regression loses degrees of freedom equal to the number of factors. Though these formulas are standard classics, I emphasize that we do not have to make the severe assumptions on the error terms that are used to derive them. As with the time series case, I derive a general formula for the distribution of  and  , and only at the last moment make classic error term assumptions to make the spectral density matrix pretty. Derivation and Formulas that Do Not Require i.i.d. Errors The easy and elegant way to account for the effects of generated regressors such as the  in the cross sectional regression is to map the whole thing into GMM. Then, we treat the moments that generate the regressors  at the same time as the moments that generate the cross sectional regression coefficient , and the covariance matrix S between the two sets of moments captures the effects of generating the regressors on the standard error of the cross sectional regression coefficients. Comparing this straightforward derivation with the difficulty of Shanken s 1992b paper that originally derived the corrections for , and noting that Shanken did not go on to find the formulas 12.20 that allow a test of the pricing errors is a nice argument for the simplicity and power of the GMM framework. To keep the algebra manageable, I treat the case of a single factor. The moments are E Re a f 0 tt g b E Re a f f 0 . 12.23 Tttt E Re  0 The top two moment conditions exactly identify a and  as the time series OLS estimates. Note a not . The time series intercept is not necessar ily equal to the pricing error in a cross sectional regression. The bottom moment condition is the asset pricing model. It is in general overiden tified in a sample, since there is only one extra parameter  and N extra moment conditions. If we use a weighting vector  on this condi tion, we obtain the OLS cross sectional estimate of . If we use a weighting vector  1, we obtain the GLS cross sectional estimate of . To accommo date both cases, use a weighting vector  , and then substitute   or   1 at the end. However, once we abandon i.i.d. errors, the GLS cross sectional regression weighted by 1 is no longer the optimal esti mate. Once we recognize that the errors do not obey classical assumptions, and if we want efficient estimates, we might as well calculate the correct and",
        "242 12. Regression Based Tests of Linear Factor Models fully efficient estimates. Having decided on a cross sectional regression, the efficient estimates of the moments 12.23 are d S 1g The standard errors for  come straight from the general GMM standard error formula 11.4 . The  are not parameters, but are the last N moments. Their covariance matrix is thus given by the GMM formula 11.5 for the sample variation of the gT . All we have to do is map the problem into the GMM notation. The parameter vector is b a   . The a matrix chooses which moment conditions are set to zero in estimation, a I2N 0. 0 The d matrix is the sensitivity of the moment conditions to the parameters, S E R e a  f f R e t tt t j Re  t a  f t j f t j Re  t j j Re a f Re a ft j t t t j E t t j IN IN E f 0 gT 2 INE f INE f 0 . 0 IN  d The S matrix is the long run covariance matrix of the moments, b  tft t jft j .  ft Ef t  ft j Ef t j j In the second expression, I have used the regression model and the restric tion under the null that E Re . In calculations, of course, you could t simply estimate the first expression. We are done. We have the ingredients to calculate the GMM standard error formula 11.4 and formula for the covariance of moments 11.5 . With a vector f , the moments are ETRe a f IN IK 1 E Re a f f 0,  T ET Re  wherei N 1,and  forOLSand  1 forGLS. T a,, 0.",
        "12.2. Cross Sectional Regressions 243 Note that the GLS estimate is not the efficient GMM estimate when returns are not i.i.d. The efficient GMM estimate is d S 1g d T E f E ff I 0 . N 0. The d matrix is We can recover the classic formulas 12.19 , 12.20 , 12.21 by adding the assumption that the errors are i.i.d. and independent of the factors, and that the factors are uncorrelated over time as well. The assumption that the errors and factors are uncorrelated over time means we can ignore the lead and lag terms. Thus, the top left corner of S is E   . The assumption tt that the errors are independent from the factors ft simplifies the terms in which andf aremultiplied:E   f E f forexample.Theresultis g 1E f 0 T 0  12 ttttt E f Multiplying a,d,S together as specified by the GMM formula for the covariance matrix of parameters 11.4 , we obtain the covariance matrix of all the parameters, and its 3, 3 element gives the variance of  . Multiplying the terms together as specified by 11.5 , we obtain the sampling distribution of the  , 12.20 . The formulas 12.19 reported above are derived the same waywithavectoroffactorsft ratherthanascalar;thesecond momentcondi tion in 12.23 then reads E R e a  f f . The matrix multiplication ttt is not particularly enlightening. Once again, there is really no need to make the assumption that the errors are i.i.d. and especially that they are conditionally homoskedastic that the factor f and errors  are independent. It is quite easy to estimate an S matrix that does not impose these conditions and calculate standard errors. They will not have the pretty analytic form given above, but they will more closely report the true sampling uncertainty of the estimate. Furthermore, if one is really interested in efficiency, the GLS cross sectional estimate should use the spectral density matrix as weight ing matrix applied to all the moments rather than 1 applied only to the pricing errors. Time Series vs. Cross Section How are the time series and cross sectional approaches different? Most importantly, you can run the cross sectional regression when the factor is not a return. The time series test requires factors that are also S E f E f2 E f . E f   2 f",
        "244 12. Regression Based Tests of Linear Factor Models returns, so that you can estimate factor risk premia by  ET f . The asset pricing model does predict a restriction on the intercepts in the time series regression. Why not just test these? If you impose the restriction E Rei  , you can write the time series regression 12.9 as i Rei    f E f i, t 1,2,...,T foreachi. tiitt Thus, the intercept restriction is a   E f . 12.24 This restriction makes sense. The model says that mean returns should be proportional to betas, and the intercept in the time series regression controls the mean return. You can also see how  E f results in a zero intercept. Finally, however, you see that without an estimate of , you cannot check this intercept restriction. If the factor is not a return, you will be forced to do something like a cross sectional regression. When the factor is a return, so that we can compare the two methods, time series and cross sectional regressions are not necessarily the same. The time series regression estimates the factor risk premium as the sample mean of the factor. Hence, the factor receives a zero pricing error in each sample. Also, the predicted zero beta excess return is also zero. Thus, the time series regression describes the cross section of expected returns by drawing a line as in Figure 12.1 that runs through the origin and through the factor, ignoring all of the other points. The OLS cross sectional regression picks the slope and intercept, if you include one, to best fit all the points: to minimize the sum of squares of all the pricing errors. If the factor is a return, the GLS cross sectional regression, including the factor as a test asset, is identical to the time series regression. The time series regression for the factor is, of course, ft 0 1ft 0, so it has a zero intercept, beta equal to one, and zero residual in every sample. The residual variance covariance matrix of the returns, including the factor, is Re a f 0 E . f 0 1f 00 ii Since the factor has zero residual variance, a GLS regression puts all its weight on that asset. Therefore,  ET f just as for the time series regression. The pricing errors are the same, as is their distribution and the 2 test. You gain a degree of freedom by adding the factor to the cross sectional regression, so the test is a 2 . N",
        "12.3. Fama MacBeth Procedure 245 Why does the efficient technique ignore the pricing errors of all of the other assets in estimating the factor risk premium, and focus only on the mean return? The answer is simple, though subtle. In the regression model Re a f , ttt the average return of each asset in a sample is equal to beta times the average return of the factor in the sample, plus the average residual in the sample. An average return carries no information about the mean of the factor that is not already observed in the sample mean of the factor. A signal plus noise carries no additional information beyond that in the same signal. Thus, an efficient cross sectional regression wisely ignores all the information in the other asset returns and uses only the information in the factor return to estimate the factor risk premium. 12.3 Fama MacBeth Procedure I introduce the Fama MacBeth procedure for running cross sectional regression and calculating standard errors that correct for cross sectional cor relation in a panel. I show that, when the right hand variables do not vary over time, Fama MacBeth is numerically equivalent to pooled time series, cross section OLS with standard errors corrected for cross sectional correla tion, and also to a single cross sectional regression on time series averages with standard errors corrected for cross sectional correlation. Fama MacBeth standard errors do not include corrections for the fact that the betas are also estimated. Fama and MacBeth 1973 suggest an alternative procedure for running cross sectional regressions, and for producing standard errors and test statis tics. This is a historically important procedure, it is computationally simple to implement, and is still widely used, so it is important to understand it and relate it to other procedures. First, you find beta estimates with a time series regression. Fama and MacBeth use rolling 5 year regressions, but one can also use the technique with full sample betas, and I will consider that simpler case. Second, instead of estimating a single cross sectional regression with the sample averages, we now run a cross sectional regression at each time period, i.e., Rei    , i 1,2,...,N foreacht. t it it I write the case of a single factor for simplicity, but it is easy to extend the model to multiple factors. Then, Fama and MacBeth suggest that",
        "246 12. Regression Based Tests of Linear Factor Models we estimate  and i as the average of the cross sectional regression estimates, 1 T 1 T   ,   . T t i T it t 1 t 1 Most importantly, they suggest that we use the standard deviations of the cross sectional regression estimates to generate the sampling errors for these estimates, 2  1   2, 2  1   2. T T2 t T i T2 it i t 1 t 1 It is 1 T 2 because we are finding standard errors of sample means,  2 T . This is an intuitively appealing procedure once you stop to think about it. Sampling error is, after all, about how a statistic would vary from one sample to the next if we repeated the observations. We cannot do that with only one sample, but why not cut the sample in half, and deduce how a statistic would vary from one full sample to the next from how it varies from the first half of the sample to the next half? Proceeding, why not cut the sample in fourths, eighths, and so on? The Fama MacBeth procedure carries this idea to its logical conclusion, using the variation in the statistic  overtimetodeduceitsvariationacrosssamples. t We are used to deducing the sampling variance of the sample mean of a series x by looking at the variation of x through time in the sample, t t using 2 x 2 x T 1 x x 2. The Fama MacBeth technique T2tt just applies this idea to the slope and pricing error estimates. The formula assumes that the time series is not autocorrelated, but one could easily extend the idea to estimates  that are correlated over time by using a long run variance matrix, i.e., estimate 1 t 2  cov , . T Ttt j j One should of course use some sort of weighting matrix or a parametric description of the autocorrelations of  , as explained in Section 11.7. Asset return data are usually not highly correlated, but accounting for such cor relation could have a big effect on the application of the Fama MacBeth technique to corporate finance data or other regressions in which the cross sectional estimates are highly correlated over time. It is natural to use this sampling theory to test whether all the pric ing errors are jointly zero as we have before. Denote by  the vector of",
        "12.3. Fama MacBeth Procedure 247 pricing errors across assets. We could estimate the covariance matrix of the sample pricing errors by 1 T the test   t , 1 T T t 1 cov  or a general version that accounts for correlation over time and then use T2 t 1 t  t  ,  cov  1 2 . N 1 Fama MacBeth in Depth The GRS procedure and the analysis of a single cross sectional regression are familiar from any course in regression. We will see them justified by maximum likelihood below. The Fama MacBeth procedure seems unlike anything you have seen in any econometrics course, and it is obviously a use ful and simple technique that can be widely used in panel data in economics and corporate finance as well as asset pricing. Is it truly different? Is there something different about asset pricing data that requires a fundamentally new technique not taught in standard regression courses? Or is it similar to standard techniques? To answer these questions it is worth looking in a little more detail at what it accomplishes and why. It is easier to do this in a more standard setup, with left hand variable y and right hand variable x. Consider a regression yit  xit it, i 1,2,...,N, t 1,2,...,T. The data in this regression have a cross sectional element as well as a time series element. In corporate finance, for example, you might be interested in the relationship between investment and financial variables, and the data set has many firms N as well as time series observations for each firm T . Inanexpectedreturn betaassetpricingmodel,thexit standsforthei and  stands for . An obvious thing to do in this context is simply to stack the i and t observations together and estimate  by OLS. I will call this the pooled time series cross section estimate. However, the error terms are not likely to be uncorrelated with each other. In particular, the error terms are likely to be cross sectionally correlated at a given time. If one stock s return is unusually high this month, another stock s return is also likely to be high;",
        "248 12. Regression Based Tests of Linear Factor Models if one firm invests an unusually great amount this year, another firm is also likely to do so. When errors are correlated, OLS is still consistent, but the OLS distribution theory is wrong, and typically suggests standard errors that are much too small. In the extreme case that the N errors are perfectly cor related at each time period, there is really only one observation for each time period, so one really has T rather than NT observations. Therefore, a pooled time series cross section estimate must include corrected standard errors. People often ignore this fact and report OLS standard errors. Another thing we could do is first take time series averages and then run a pure cross sectional regression of ET yit  ET xit ui, i 1,2,...,N. Thisprocedurewouldloseanyinformationduetovariationofthexit over time, but at least it might be easier to figure out a variance covariance matrix for ui and correct the standard errors for residual correlation. You could also average cross sectionally and then run a single time series regression. We will get to that option later. In either case, the standard error corrections are just applications of the standard formula for OLS regressions with correlated error terms. Finally, we could run the Fama MacBeth procedure: run a cross sectional regression at each point in time, average the cross sectional  estimates to get an estimate , and use the time series standard deviation of  to estimate the standard error of . It turns out that the Fama MacBeth procedure is another way of calculating the standard errors, corrected for cross sectional correlation. Proposition: If the xit variables do not vary over time, and if the errors are cross sectionally correlated but not correlated over time, then the Fama MacBeth estimate, the pure cross sectional OLS estimate, and the pooled time series cross sectional OLS estimates are identical. Also, the Fama MacBeth standard errors are identical to the cross sectional regression or stacked OLS standard errors, corrected for residual correlation.Noneoftheserelationsholdifthexit varythroughtime. Since they are identical procedures, whether one calculates estimates and standard errors in one way or the other is a matter of taste. I emphasize one procedure that is incorrect: pooled time series and cross section OLS with no correction of the standard errors. The errors are so highly cross sectionally correlated in most finance applications that the standard errors so computed are often off by a factor of 10. The assumption that the errors are not correlated over time is probably not so bad for asset pricing applications, since returns are close to indepen dent. However, when pooled time series cross section regressions are used in corporate finance applications, errors are likely to be as severely corre lated over time as across firms, if not more so. The other factors  that t t",
        "12.3. Fama MacBeth Procedure 249 cause, say, company i to invest more at time t than predicted by a set of right hand variables is surely correlated with the other factors that cause company j to invest more. But such factors are especially likely to cause company i to invest more at time t 1 as well. In this case, any standard errors must also correct for serial correlation in the errors; the GMM based formulas in Section 11.4 can do this easily. The Fama MacBeth standard errors also do not correct for the fact that  are generated regressors. If one is going to use them, it is a good idea to at least calculate the Shanken correction factors outlined above, and check that the corrections are not large. Proof: Wejusthavetowriteoutthethreeapproachesandcomparethem. Having assumed that the x variables do not vary over time, the regression is y x  . it i it We can stack up the cross sections i 1, . . . , N and write the regression as yt x t. x is now a matrix with the xi as rows. The error assumptions mean E   . tt Pooled OLS: To run pooled OLS, we stack the time series and cross sections by writing and then with The estimate and its standard error are then  X X 1X Y, OLS cov  X X 1X X X X 1. OLS y1 x 1 y x  2 2 Y . , X . ,  . . . . yT x T Y X , E  .. . .",
        "250 12. Regression Based Tests of Linear Factor Models Writing this out from the definitions of the stacked matrices, with X X Tx x,  x x 1x E y , OLS Tt cov  1 x x 1 x x x x 1. OLS T We can estimate this sampling variance with E     ,   y x   . Ttt t t OLS Pure cross section: The pure cross sectional estimator runs one cross sectional regression of the time series averages. So, take those averages, ET yt x ET t , where x ET x since x is constant. Having assumed i.i.d. errors over time, the error covariance matrix is EE  E 1 . T The cross sectional estimate and corrected standard errors are then  x x 1x E y , TtTt XS Tt 2  1 x x 1x x x x 1. XS T Thus, the cross sectional and pooled OLS estimates and standard errors are exactly the same, in each sample. Fama MacBeth: The Fama MacBeth estimator is formed by first run ning the cross sectional regression at each moment in time,  1 t xx xyt. Then the estimate is the average of the cross sectional regression estimates,   1 FM ETt xx xETyt. Thus, the Fama MacBeth estimator is also the same as the OLS estimator, in each sample. The Fama MacBeth standard error is based on the time series standard deviation of the  . Using cov to denote sample covariance, tT cov  1 cov  1 x x 1x cov y x x x 1, FMTTtT Tt",
        "Problems with we have and finally 251 y x , t FM t cov y E  , TtTtt T  1 1 1 cov FM x x x x x x . Thus, the FM estimator of the standard error is also numerically equivalent to the OLS corrected standard error. Varying x: If the xit vary through time, none of the three procedures are equal anymore, since the cross sectional regressions ignore time series variationinthexit.Asanextremeexample,supposeascalarxit variesover time but not cross sectionally, yit  xt it, i 1,2,...,N, t 1,2,...,T. The grand OLS regression is x t yit x t 1 N yit it t i  OLS , it x 2t t x 2t where x x ET x denotes the de meaned variables. The estimate is driven by the covariance over time of xt with the cross sectional average of the yit , which is sensible because all of the information in the sample lies in time variation. It is identical to a regression over time of cross sectional averages. However, you cannot even run a cross sectional estimate, since the right hand variable is constant across i. As a practical example, you might be interested in a CAPM specification in which the betas vary over time t but not across test assets. This sam ple still contains information about the CAPM: the time variation in betas should be matched by time variation in expected returns. But any method based on cross sectional regressions will completely miss it. In historical context, the Fama MacBeth procedure was also impor tant because it allowed changing betas, which a single unconditional cross sectional regression or a time series regression test cannot easily handle. Problems Chapter 12 1. When we express the CAPM in excess return form, can the test assets be differences between risky assets, R i R j ? Can the market excess return on",
        "252 12. Regression Based Tests of Linear Factor Models the right hand side also use a risky asset, or must it be relative to a risk free rate? Hint:startwithE Ri Rf i,m E Rm Rf andseeifyoucan get to the other forms. Betas must be regression coefficients. 2. Can you run the GRS test on a model that uses industrial production growth as a factor, E Rei i, ipip? 3. Fama and French 1997 report that pricing errors are correlated with betas in a test of a factor pricing model on industry portfolios. How is this possible? 4. We saw that a GLS cross sectional regression of the CAPM passes through the market and risk free rate by construction, though the OLS regression does not do so. Show that if the market return is an equally weighted portfolio of the test assets, then an OLS cross sectional regression with an estimated intercept passes through the market return by construction. What if you force the intercept to zero? Does either regression also pass through the risk free rate or origin? 5. Factor models with factors that are not returns are usually estimated and tested by cross sectional regressions. There is a way to use a time series regression to estimate and test the model, however. The time series regression is Rei a f i; t 1,2...T foreachi. tiitt Recall from 12.24 that the asset pricing model does not leave ai free; insteadtheymustsatisfya   E f .Writedownasetofmomentcon ditions that you can use to estimate this model and test the restriction on the ai . Describe how you would estimate the parameters, and what formula you wouldlookuptocomputeatest howyouwoulduse b ,cov g ,JT,etc. T Hint: start with the moments for unrestricted OLS time series regressions, then impose the constraint between a and . You will estimate the restricted model.",
        "13 GMM for Linear Factor Models in Discount Factor Form In this chapter, I study estimation and testing of linear discount factor models expressed as p E mx , m b f . This form naturally suggests a GMM approach using the pricing errors as moments. The resulting estimates look a lot like the regression estimates of Chapter 12. 13.1 GMM on the Pricing Errors Gives a Cross Sectional Regression The first stage GMM estimate is an OLS cross sectional regression, and the second stage is a GLS regression, First stage : b d d 1d E p , 1T Second stage : b Standard errors are the corresponding regression formulas, and the variance of the pricing errors are the standard regression formula for variance of a residual. 2 d S 1d 1d S 1E p . Treating the constant a 1 as a constant factor, the model is m b f , offs respectively; f is a K 1 vector of factors, and b is a K 1 vector of 253 E p E mx , E p E xf b. or simply Keep in mind that p and x are N 1 vectors of asset prices and pay 13.1",
        "254 13. GMM for Linear Factor Models in Discount Factor Form parameters. I suppress the time indices mt 1, ft 1, xt 1, pt . The payoffs are typically returns or excess returns, including returns scaled by instruments. The prices are typically one returns , zero excess returns , or instruments. To implement GMM, we need to choose a set of moments. The obvious set of moments to use are the pricing errors, g b ET xf b p . T This choice is natural but not necessary. You do not have to use p E mx with GMM, and you do not have to use GMM with p E mx . You can use GMM on expected return beta models, and you can use maximum likeli hood on p E mx . It is a choice, and the results will depend on this choice of moments as well as the specification of the model. The GMM estimate is formed from ming b Wg b b with first order condition d Wg b d W ET xf b p 0, T where This is the second moment matrix of payoffs and factors. The first stage has W I, the second stage has W S 1. Since this is a linear model, we can solve analytically for the GMM estimate, and it is First stage : b d d 1d E p , 1T Second stage : b d S 1d 1d S 1E p . 2T The first stage estimate is an OLS cross sectional regression of average prices on the second moment of payoff with factors, and the second stage estimate is a GLS cross sectional regression. What could be more sensible? The model 13.1 says that average prices should be a linear function of the second moment of payoff with factors, so the estimate runs a linear regression. These are cross sectional regressions since they operate across assets on sample averages. The data points intheregressionaresampleaverageprices y andsecondmoments of payoffs with factors x across test assets. We are picking the parameter b to make the model fit explain the cross section of asset prices as well as possible. TT g b d T ET fx . b",
        "13.1. GMM on the Pricing Errors Gives a Cross Sectional Regression 255 We find the distribution theory from the usual GMM standard error 13.2 Unsurprisingly, these are exactly the formulas for OLS and GLS regres sion errors with error covariance S. The pricing errors are correlated across assets, since the payoffs are correlated. Therefore the OLS cross sectional regression standard errors need to be corrected for correlation, as they are in 13.2 , and one can pursue an efficient estimate as in GLS. The analogy in GLS is close, since S is the covariance matrix of E p E xf b ; S is the covariance matrix of the errors in the cross sectional regression. The covariance matrix of the pricing errors is, from 11.5 , 11.9 , and 11.10 , First stage : T cov g b I d d d 1d S I d d d 1d , T 13.3 formulas 11.2 and 11.8 . In the first stage, a d : First stage : cov b 1 d d 1d Sd d d 1, 1T Second stage : cov b 1 d S 1d 1.  1 1 Secondstage:Tcovg b S ddS d d. T 2T These are obvious analogues to the standard regression formulas for the covariance matrix of regression residuals. The model test is g b cov g 1g b 2 moments parameters , which specializes for the second stage estimate as Tg b S 1g b 2 moments parameters . TT There is not much point in writing these out, other than to point out that the test is a quadratic form in the vector of pricing errors. TTT",
        "256 13. GMM for Linear Factor Models in Discount Factor Form 13.2 The Case of Excess Returns When mt 1 a b ft 1 and the test assets are excess returns, the GMM estimate is a GLS cross sectional regression of average returns on the second moments of returns with factors, First stage : b d d 1d E Re , 1T Second stage : b d S 1d 1d S 1E Re , 2T where d is the covariance matrix between returns and factors. The other formulas are the same. The analysis of the last section requires that at least one asset has a nonzeroprice.Ifallassetsareexcessreturns,thenb d d 1d E p 0. 1T Linear factor models are most often applied to excess returns, so this case is important. The trouble is that in this case the mean discount factor is not identified. If E mRe 0, then E 2 m Re 0. Analogously in expected return beta models, if all test assets are excess returns, then we have no information on the level of the zero beta rate. Writing out the model as m a b f , we cannot separately identify a and b so we have to choose some normalization. The choice is entirely one of convenience; lack of identification means precisely that the pricing errors do not depend on the choice of normalization. The easiest choice is a 1. Then g b ET mRe ET Re E Ref b. T The sign makes the resulting formulas prettier. We have b the second moment matrix of returns and factors. The first order condition g b d T E Ref , to min gT WgT is Then, the GMM estimates of b are First stage : b d d 1d E Re , 1T Second stage : b d S 1d 1d S 1E Re . 2T d W ET R e db 0.",
        "258 13. GMM for Linear Factor Models in Discount Factor Form where Ef is the mean of the factors, a parameter to be estimated just like b. We can capture the first and second stage regressions above with the weighting matrix aT ET f Re W 0 0 IK with W I or W S 1. I use the notation S to denote the first block of 11 11 thespectraldensitymatrix,correspondingtotheET Re Ref b moments only. The first block of estimates delivers the OLS and GLS cross sectional regression estimates of b, while the identity matrix in the second block delivers the sample mean estimate Ef ET f . Now the GMM standard error and cov g formulas will correct for the fact that Ef is estimated. A T problem at the end of the chapter leads you through the algebra to verify that the resulting standard errors resemble those of the Shanken correction in Chapter 12. This correction only affects the standard errors of the b estimates. The distribution of the pricing errors and the 2 statistics are not affected. In my experience so far with this method, the correction for the fact that Ef is estimated is very small in practice, so that little damage is done in ignoring it as is the case with the Shanken correction . On the other hand, once the issue is understood it is easy to do it right. As was the case with the Shanken corrections, the second stage regres sion here is not in fact the efficient GMM estimate. The efficient estimate does not use this a with W S 1. Instead it uses a d S 1, where T 11 T g E Ref E Re b d T , b Ef and where S, the spectral density matrix of both sets of moments, is S E utut j utft j , f u f f j t t j t t j u Re 1 f b . ttt Therefore, the optimal weighting matrix aT d S 1 does not have the block diagonalformofaT givenabove.EfficientGMMletssomemoments deviate from their sample values if by doing so it can make other moments closer to zero, trading off these errors by the S 1 matrix. If an estimate E f ET f will make the pricing errors smaller, then efficient GMM will choose such an estimate. Thus, if one really wants efficiency, this is the 0 IK",
        "13.3. Horse Races 259 way to do it, rather than the second stage cross sectional regression given above. 13.3 Horse Races It is often interesting to test whether one set of factors drives out another. For example, Chen, Roll, and Ross 1986 test whether their five macro economic factors price assets so well that one can ignore even the market return. Given the large number of factors that have been proposed, a sta tistical procedure for testing which factors survive in the presence of the others is desirable. In this framework, such a test is very easy. Start by estimating a general model m b1 f1 b2 f2. 13.4 We want to know, given factors f1, do we need the f2 to price assets i.e., is b2 0? There are two ways to do this. First and most obviously, we have an asymptotic covariance matrix for b1b2 , so we can form a t test if b2 is scalar or 2 test for b2 0 by forming the statistic b var b 1b 2 , 2 2 2 b2 where b2 is the number of elements in the b2 vector. This is a Wald test. Second, we can estimate a restricted system m b 1f1. Since there are fewer free parameters than in 13.4 , and the same number of moments, we expectthecriterionJT torise.Ifweusethesameweightingmatrix usually theoneestimatedfromtheunrestrictedmodel 13.4 ,thentheJT cannot in fact decline. But if b 2 really is zero, it should not rise much. How much? The 2 difference test answers that question; TJT restricted TJT unrestricted 2 of restrictions . This is very much like a likelihood ratio test. How to test whether one set of factors drives out another. Test b2 0 in m b f b f using the standard error of b , or the 2 difference test. 1122 2",
        "260 13. GMM for Linear Factor Models in Discount Factor Form 13.4 Testing for Priced Factors: Lambdas or b s? In the context of expected return beta models, it has been more tradi tional to evaluate the relative strengths of models by testing the factor risk premia  of additional factors, rather than test whether their b is zero. The b s are not the same as the  s. b are the regression coefficients of m on f , aretheregressioncoefficientsofRi onf. To keep the equations simple, I will use mean zero factors, excess returns, and normalize to E m 1, since the mean of m is not identified with excess returns. The parameters b and  are related by  E ff b. See Section 6.3. Briefly, 0 E mR e E R e 1 f b , E Re cov Re,f b cov Re,f E ff 1E ff b  . bj asks whether factor j helps to price assets given the other factors. bj gives themultipleregressioncoefficientofmonfj giventheotherfactors. j asks whether factor j is priced, or whether its factor mimicking portfolio carries a positive risk premium. j gives the single regression coefficient of m on fj . Therefore, when factors are correlated, one should test bj 0 to see whethertoincludefactorjgiventheotherfactorsratherthantestj 0. When the factors are orthogonal, E ff is diagonal, and each j and only if the corresponding bj 0. The distinction between b and  only matters when the factors are correlated. Factors are often correlated, however. j captures whether factor fj is priced. We can write  E f f b E mf to see that  is the negative of the price that the discount factor m assigns to f . b captures whether factor fj is marginally useful in pricing assets,giventhepresenceofotherfactors.Ifbj 0,wecanpriceassetsjust aswellwithoutfactorfj aswithit. j is proportional to the single regression coefficient of m on f . j cov m,fj . j 0 asks the corresponding single regression coefficient question is factor j correlated with the true discount factor? bj is the multiple regression coefficient of m on fj given all the other factors. This just follows from m b f . Regressions do not have to have error terms! A multiple regression coefficient j in y x  is the way 0 if",
        "13.4. Testing for Priced Factors: Lambdas or b s? 261 to answer does xj help to explain variation in y given the presence of the other x s? When you want to ask the question, should I include factor j given the other factors? you want to ask the multiple regression question. For example, suppose the CAPM is true, which is the single factor model m a bR em , whereRem isthemarketexcessreturn.ConsideranyotherexcessreturnRex x for extra , positively correlated with R em . If we try a factor model with the spurious factor R ex , the answer is m a bRem 0 Rex. bx isobviouslyzero,indicatingthataddingthisfactordoesnothelptoprice assets. However, since the correlation of Rex with Rem is positive, the CAPM beta of Rex on Rem is positive, Rex earns a positive expected excess return, andx E Rex 0.Intheexpectedreturn betamodel E Rei imm ixx m E Rem isunchangedbytheadditionofthespuriousfactor.However, sincethefactorsRem,Rex arecorrelated,themultipleregressionbetasofRei onthefactorschangewhenweaddtheextrafactorRex.Ifix ispositive,im will decline from its single regression value, so the new model explains the same expected return E R ei . The expected return beta model will indicate a riskpremiumforx exposure,andmanyassetswillhavex exposure Rx for example! eventhoughfactorRx isspurious.Inparticular,Rex willofcourse havemultipleregressioncoefficientsx,m 0andx,x 1,anditsexpected return will be entirely explained by the new factor x. So, as usual, the answer depends on the question. If you want to know whether factor i is priced, look at  or E mf i . If you want to know whether factor i helps to price other assets, look at bi . This is not an issue about sampling error or testing. All moments above are population values. Of course, testing b 0 is particularly easy in the GMM, p E mx setup. But you can always test the same ideas in any expression of the model. In an expected return beta model, estimate b by E ff 1 and test the elements of that vector rather than  itself. YoucanwriteanassetpricingmodelasERe  andusethetotest whether each factor can be dropped in the presence of the others, if you use single regression betas rather than multiple regression betas. In this case each  is proportional to the corresponding b. Problem 2 at the end of this chapter helps you to work out this case. You can also make sure that your factors are orthogonal, in which case testing  is the same thing as testing b.",
        "262 13. GMM for Linear Factor Models in Discount Factor Form 13.5 Mean Variance Frontier and Performance Evaluation We often summarize asset return data by mean variance frontiers. For example, a large literature has examined the desirability of interna tional diversification in a mean variance context. Stock returns from many countries are not perfectly correlated, so it looks like one can reduce portfolio variance a great deal for the same mean return by holding an internationally diversified portfolio. But is this phenomenon real or just sampling error? Even if the value weighted portfolio were ex ante mean variance efficient, an ex post mean variance frontier constructed from historical returns on the roughly 6000 NYSE stocks would leave the value weighted portfolio well inside the ex post frontier. So is I should have bought Japanese stocks in 1960 and sold them in 1990! a signal that broad based international diversification is a good idea now, or is it sim ply 20 20 hindsight regret like I should have bought Microsoft in 1982? Similarly, when evaluating fund managers, we want to know whether the manager is truly able to form a portfolio that beats mean variance effi cient passive portfolios, or whether better performance in sample is just due to luck. Since a factor model is true if and only if a linear combination of the factors or factor mimicking portfolios if the factors are not returns is mean variance efficient, one can interpret a test of any factor pricing model as a test whether a given return is on the mean variance frontier. Section 12.1 showed how the Gibbons, Ross, and Shanken pricing error statistic can be interpreted as a test whether a given portfolio is on the mean variance frontier, when returns and factors are i.i.d., and the GMM distribution the ory of that test statistic allows us to extend the test to non i.i.d. errors. A GMM, p E mx , m a bRp test analogously tests whether Rp is on the mean variance frontier of the test assets. We may want to go one step further, and not just test whether a com bination of a set of assets Rd say, domestic assets is on the mean variance frontier, but whether the Rd assets span the mean variance frontier of Rd and Ri say, foreign or international assets. The trouble is that if there is no risk free rate, the frontier generated by Rd might just intersect the fron tiergeneratedbyRd andRi together,ratherthanspanorcoincidewiththe latter frontier, as shown in Figure 13.1. Testing that m a b R d prices both Rd and Ri only checks for intersection. A GMM, p E mx approach to testing whether a return expands the mean variance frontier. Just test whether m a bR prices all returns. If there is no risk free rate, use two values of a.",
        "264 13. GMM for Linear Factor Models in Discount Factor Form Thus,youcantestforspanningwithaJT testonthemoments E a1 b1 Rd Rd 0, E a1 b1 Rd Ri 0, E a2 b2 Rd Rd 0, E a2 b2 Rd Ri 0, for any two fixed values of a1, a2. 13.6 Testing for Characteristics It is often interesting to characterize a model by checking whether the model drives out a characteristic. For example, portfolios organized by size or market capitalization show a wide dispersion in average returns at least up to 1979 . Small stocks gave higher average returns than large stocks. A good asset pricing model should account for average returns by betas. It is ok if a characteristic is associated with average returns, but in the end betas should drive out the characteristic; the alphas or pricing errors should not be associated with the characteristic. The original tests of the CAPM similarly checked whether the variance of the individual portfolio had anything to do with average returns once betas were included. Denote the characteristic of portfolio i by yi. An obvious idea is to include both betas and the characteristic in a multiple, cross sectional regres sion. In addition, the characteristic is sometimes estimated rather than being a fixed number such as the size rank of a size portfolio, and you would like to include the sampling uncertainty of its estimation in the standard errors ofthecharacteristic seffect.Letyi denotethetimeserieswhosemeanE yi tt determines the characteristic. Now, write the moment condition for the ith asset as gi E m b xi pi yi , T T t 1 t 1 t t and let y denote the vector of yi across assets. The estimate of  tells you how the characteristic E yi is associated with model pricing errors E m b x p . The standard GMM formulas for the standard deviation t 1 t 1 t How to check whether an asset pricing model drives out a characteristic such as size, book market, or volatility. Run cross sectional regressions of pricing errors on characteristics; use the formulas for covariance matrix of the pricing errors to create standard errors.",
        "Problems 265 of  or the 2 difference test for  0 tell you whether the  estimate is statistically significant, including the fact that E y must be estimated. Problems Chapter 13 1. Work out the GMM distribution theory for the first stage estimate of the model m 1 b f E f using excess returns. The distribution should recognize the fact that Ef is estimated in the sample. To do this, set up ET Re Re f Ef b g , T ET f Ef aT ET f Re 0 . 0 IK The estimated parameters are b , Ef . You should end up with a formula for the standard error of b that resembles the Shanken correction 12.19 , and anunchangedJT test. YoucanusethenullE Re cov Re,f btosimplify the formulas. 2. Show that if you use single regression betas, then the corresponding  can be used to test for the marginal importance of factors. However, the  are no longer the expected return of factor mimicking portfolios. 3. Obtain the Fama French factors and 25 portfolios formed on size and book to market from Ken French s website, http: mba.tuck.dartmouth .edu pages faculty ken.french data library.html. Fama and French advo cate a factor pricing model using the three factors RMRF, HML, and SMB to price the 25 portfolios. Evaluate the Fama French three factor model using the following techniques. In each case, present the coefficient esti mates ,  or b , standard errors, and the  V 1 test. Also compute the root mean square pricing errors, plot actual vs. predicted mean returns, and compute the R2 of actual vs. predicted mean returns. Compare the results the point of this problem is both to make you really understand how to do each procedure, but also to see if there are important differences between the procedures. a Time series regression i OLS standard errors. Calculate both the asymptotic  2 test and the GRS F test. ii GMM standard errors using 0 lags.",
        "266 b 13. GMM for Linear Factor Models in Discount Factor Form Cross sectional OLS and GLS regression. i Standard errors and cov  with no Shanken correction. ii Shanken standard errors. iii GMM standard errors with 0 lags. Fama MacBeth. Plain vanilla: OLS cross sectional regressions, no Shanken corrections to standard errors. Do compute the  cov  1 test however. d Express the model in discount factor form m 1 b f and use i First stage GMM 0 lags in standard errors . Don t forget that the first stage JT test is g cov g g 2 not Tg S 1g 2. TTTTT ii Second stage GMM 0 lags again . e Express the model in discount factor form m 1 f Ef b so your cross sectional regressions are returns on covariances. Do first and second stage GMM. f Can we drop the market factor in favor of a Fama French two factor model? Can we drop the SMB factor? g Optional: Use 1 year of Newey West lags in the GMM estimates as well. Hints and tips: avoid the missing observations 99 by starting later than the first data point in the sample. It s a good idea to start with simple characterizations make tables of mean returns and betas to make sure things are lining up well. It is also important in any test to check that there is a spread of average returns to explain and that the betas do vary across stocks. The portfolio returns are not yet excess use the risk free rate in the factor data set. Or, estimate the model in levels with no risk free assumption. As you saw in a previous problem, it is not really right to subtract a risky return from the market for the CAPM. c",
        "272 14. Maximum Likelihood The lead and lag terms in S are all zero since we showed above that scores should be unforecastable. This is the outer product definition of the infor mation matrix. There is no a matrix, since the moments themselves are set to zero. The GMM asymptotic distribution of  is therefore T   N 0, d 1Sd 1 N 0, I 1 . We recover the inverse information matrix, as specified by the ML asymptotic distribution theory. 14.3 When Factors Are Returns, ML Prescribes a Time Series Regression Given a linear factor model whose factors are also returns, as with the CAPM, ML prescribes a time series regression test. To keep notation simple, I again treat a single factor f . The economic model is E Re E f . 14.9 Re isanN 1vectoroftestassets,andisanN 1vectorofregression coefficientsoftheseassetsonthefactor themarketreturnRem inthecase of the CAPM . To apply maximum likelihood, we need to add an explicit statistical model that fully describes the joint distribution of the data. I assume that the market return and regression errors are i.i.d. normal, i.e., I add to the economic model E Re E f a statistical assumption that the regression errors are independent over time and independent of the factors. ML then prescribes a time series regression with no constant. To pre scribe a time series regression with a constant, we drop the model prediction  0. I show how the information matrix gives the same result as the OLS standard errors. Re  f , ttt ft E f ut, 14.10 We can get by with nonnormal factors, but the notation will be messier. Equation 14.10 has no content other than normality. The zero correlation t 0 0 N,2. u00 tu",
        "14.3. When Factors Are Returns 273 between ut and t identifies  as a regression coefficient. You can just write Re,Rem asageneralbivariatenormal,andyouwillgetthesameresults. The economic model 14.9 implies restrictions on this statistical model. Taking expectations of 14.10 , the CAPM implies that the intercepts  should all be zero. Again, this is also the only restriction that the CAPM places on the statistical model 14.10 . The most principled way to apply maximum likelihood is to impose the null hypothesis throughout. Thus, we write the likelihood function imposing  0. To construct the likelihood function, we reduce the statis tical model to independent error terms, and then add their log probability densities to get the likelihood function: . 1 T 1 T ft E f 2 L const. Re f 1 Re f L  1 TTT 1 Re f f 0  f2 Ref, tttttt t 1 t 1 t 1 2 t t t t 2 2 t 1 t 1 u The estimates follow from the first order conditions, L1 T 1 T f t E f 0 E f  f t . Thus, cov  1 1 1 T E f2 T E f 2 2 f E f 2 u t 1 T t 1 L and L 2 also produce ML estimates of the covariance matrices, which turn out to be the standard averages of squared residuals. The ML estimate of  is the OLS regression without a constant. The null hypothesis says the constant is zero, and the ML estimator uses that fact to avoid estimating a constant. Since the factor risk premium is equal to the expected value of the factor, it is not too surprising that the  estimate is equal to the sample average of the factor. We know that the ML distribution theory must give the same result as the GMM distribution theory which we already derived in Section 12.1, but it is worth seeing it explicitly. The asymptotic standard errors follow from either estimate of the information matrix, for example, 2 L   1 T f 2. t t 1 1 . 14.11 This is the standard OLS formula.",
        "274 14. Maximum Likelihood We also want pricing error measurements, standard errors, and tests. We can apply maximum likelihood to estimate an unconstrained model, containing intercepts, and then use Wald tests estimate standard error to test the restriction that the intercepts are zero. We can also use the unconstrained model to run the likelihood ratio test. The unconstrained likelihood function is 1 T 2 t 1 I ignore the term in the factor, since it will again just tell us to use the sample mean to estimate the factor risk premium. The estimates are now L T L const. Re  f 1 Re  f . tttt  1 R e   f f 0  t . Tt Re  f 0  E Re E f , ttTtTt 1 L  t tt 2f t 1 T covT Re,ft t 1 Unsurprisingly, the unconstrained maximum likelihood estimates of  and  are the OLS estimates, with a constant. The inverse of the information matrix gives the asymptotic distribution of these estimates. Since they are just OLS estimates, we are going to get the OLS standard errors, but it is worth seeing it come out of ML: 2L 1    1 1E f cov  T 1 E f2 E f . 1 E f 2  f , 1 cov  1 1 . 14.12 1 1E f 1E f2 2 f E f 1 The covariance matrices of  and  are thus T 2 f These are just the usual OLS standard errors, which we derived in Section 12.1 as a special case of GMM standard errors for the OLS time series",
        "14.4. When Factors Are Not Excess Returns 275 regressions when errors are uncorrelated over time and independent of the factors, or by specializing 2 X X 1. You cannot just invert 2L   to find the covariance of . That attempt would give just as the covariance matrix of , which would be wrong. You have to invert the entire information matrix to get the standard error of any parameter. Otherwise, you are ignoring the effect that estimat ing  has on the distribution of  . In fact, what I presented is really wrong, since we also must estimate . However, it turns out that is indepen dent of  and  the information matrix is block diagonal so the top left two elements of the true inverse information matrix are the same as I have written here. The variance of  in 14.12 is larger than it was in 14.11 when we imposed the null of no constant. True, constrained ML uses all the infor mation it can to produce efficient estimates estimates with the smallest possible covariance matrix. The ratio of the two formulas is equal to the familiar term 1 E f 2  2 f . In annual data for the CAPM,  R em 16 , E R em 8 , means that unrestricted estimate 14.12 has a variance 25 larger than the restricted estimate 14.11 , so the gain in efficiency can be important. In monthly data, however, the gain is smaller since variance and mean both scale with the horizon. We can also view this fact as a warning: ML will ruthlessly exploit the null hypothesis and do things like running regressions without a constant in order to get any small improvement in efficiency. We can use these covariance matrices to construct a Wald estimate standard error test of the restriction of the model that the alphas are all zero, T 1 2 1 E f  f  1 2. N 14.13 Again, we already derived this 2 test in 12.3 , and its finite sample F counterpart, the GRS F test 12.4 . The other test of the restrictions is the likelihood ratio test 14.5 . Quite generally, likelihood ratio tests are asymptotically equivalent to Wald tests, and so give the same result. 14.4 When Factors Are Not Excess Returns, ML Prescribes a Cross Sectional Regression If the factors are not returns, we do not have a choice between time series and cross sectional regression, since the intercepts are not zero. As you might suspect, ML prescribes a cross sectional regression in this case.",
        "276 14. Maximum Likelihood The factor model, expressed in expected return beta form, is E Rei   , i 1,2,...,N. ii The betas are defined from time series regressions Rei a  f i. The intercepts ai in the time series regressions need not be zero, since the model does not apply to the factors. They are not unrestricted, however. Taking expectations of the time series regression 14.15 and comparing it to 14.14 as we did to derive the restriction  0 for the time series regression , the restriction  0 implies a   E f . 14.16 iit Plugging into 14.15 , the time series regressions must be of the restricted form Rei    f E f i. 14.17 tiittt In this form, you can see that   determines the mean return. Since there i are fewer factors than returns, this is a restriction on the regression 14.17 . Stack assets i 1, 2, . . . , N to a vector, and introduce the auxiliary sta tistical model that the errors and factors are i.i.d. normal and uncorrelated with each other. Then, the restricted model is Re B B f E f , tiitt 14.14 14.15 tttt ft E f ut, t 0 N 0, , ut 0V where B denotes a N K matrix of regression coefficients of the N assets on the K factors. The likelihood function is 1 T 1 T u V 1u, 2tt2tt L const.  1 t 1 t 1  Re B  f E f , u f E f . ttttt Maximizing the likelihood function, L T T B 1 Re B  f E f V 1 f E f , ttt :0 L T 1 Re B  f E f . tt E f  t 1 :0 B t 1 t 1",
        "277 14.18 14.19 The maximum likelihood estimate of the factor risk premium is a GLS cross sectional regression of average returns on betas. The maximum likelihood estimates of the regression coefficients B are again not the same as the standard OLS formulas. Again, ML imposes the null to improve efficiency: Problems The solution to this pair of equations is E f ET ft ,  1 1 1 e  B B B ETRt. L T 1 Re B  f E f  f E f 0, ttt t 1 t 1 : 14.20 This is true, even though the B are defined in the theory as population regression coefficients. The matrix notation hides a lot here! If you want to rederive these formulas, it is helpful to start with scalar parameters, e.g., Bij , and to think of it as L  T L t t . Therefore, to really t 1 B e t 1 T T 1 B Rt ft  E f ft  E f ft  E f . implement ML, you have to solve 14.19 and 14.20 simultaneously for  , B, along with whose ML estimate is the usual second moment matrix of the residuals. This can usually be done iteratively: Start with OLS B, run an OLS cross sectional regression for , form , and iterate. Problems Chapter 14 1. Why do we use restricted ML when the factor is a return, but unrestricted ML when the factor is not a return? To see why, try to formulate a ML estimator based on an unrestricted regression when factors are not returns, equation 12.1 . Add pricing errors i to the regression as we did for the unrestricted regression in the case that factors are returns, and then find ML estimators for B, , ,E f . Treat V and as known to make the problem easier. 2. Instead of writing a regression, build up the ML for the CAPM a little more formally. Write the statistical model as just the assumption that",
        "278 14. Maximum Likelihood individual returns and the market return are jointly normal, The model s restriction is E Re  cov Rem,Re . Estimate  and show that this is the same time series estimator as we derived by presupposing a regression. Re E Re cov Rem,Re N,. Rem E Rem cov Rem,Re 2 m",
        "15 Time Series, Cross Section, and GMM DF Tests of Linear Factor Models The GMM discount factor, time series, and cross sectional regression pro cedures and distribution theory are similar, but not identical. Cross sectional regressions on betas are not the same thing as cross sectional regressions on second moments. Cross sectional regressions weighted by the residual covariance matrix are not the same thing as cross sectional regressions weighted by the spectral density matrix. GLS cross sectional regressions and second stage GMM have a theoreti cal efficiency advantage over OLS cross sectional regressions and first stage GMM, but how important is this advantage, and is it outweighed by worse finite sample performance? Finally, and perhaps most importantly, the GMM discount factor approach is still a new procedure. Many authors still do not trust it. It is important to verify that it produces similar results and well behaved test statistics in the setups of the classic regression tests. To address these questions, I first apply the various methods to a classic empirical question. How do time series regression, cross sectional regres sion, and GMM stochastic discount factor compare when applied to a test of the CAPM on CRSP size portfolios? I find that three methods produce almost exactly the same results for this classic exercise. They produce almost exactly the same estimates, standard errors, t statistics, and 2 statistics that the pricing errors are jointly zero. Then I conduct a Monte Carlo and bootstrap evaluation. Again, I find little difference between the methods. The estimates, standard errors, and size and power of tests are almost identical across methods. The bootstrap does reveal that the traditional i.i.d. assumption gener ates 2 statistics with about 1 2 the correct size they reject half as often as they should under the null. Simple GMM corrections to the distribution theory repair this size defect. Also, you can ruin any estimate and test with a bad spectral density matrix estimate. I try an estimate with 24 lags and 279",
        "15.1. Three Approaches to the CAPM in Size Portfolios 281 Figure 15.1. Average excess returns vs. betas on CRSP size portfolios, 1926 1998. The line gives the predicted average return from the time series regression, E R e  E R em . Time Series and Cross Section Figures 15.1 and 15.2 illustrate the difference between time series and cross sectional regressions, in an evaluation of the CAPM on monthly size portfolios. Figure 15.1 presents the time series regression. The time series regres sion estimates the factor risk premium from the average of the factor, ignoring any information in the other assets,  ET R em . Thus, a Figure 15.2. Average excess returns vs. betas of CRSP size portfolios, 1926 1998, and the fit of cross sectional regressions.",
        "282 15. Time Series, Cross Section, GMM DF Tests time series regression draws the expected return beta line across assets by making it fit precisely on two points, the market return and the risk free rate the market and risk free rate have zero estimated pricing error in every sample. The far right portfolios are the smallest firm portfolios, and their positive pricing errors are the small firm anomaly this data set is the first serious failure of the CAPM. I come back to the substantive issue in Chapter 20. The time series regression is the ML estimator in this case, since the factor is a return. As we saw in Section 12.2, when we write Re a ft t and  independent of f , we tell ML that a sample of returns already includes the same sample of the factor, plus extra noise. Thus, the sample of test asset returns cannot possibly tell ML anything more than the sample of the factor alone about the mean of the factor. Second, we tell ML that the factor risk premium equals the mean of the factor, so it may not consider the possibility that the two quantities are different in trying to match the data. The OLS cross sectional quantities regression in Figure 15.2 draws the expected return beta line by minimizing the squared pricing error across all assets. Therefore, it allows some pricing error for the market return, if by doing so the pricing errors on other assets can be reduced. Thus, the OLS cross sectional regression gives some pricing error to the market return in order to lower the pricing errors of the other portfolios. When the factor is not also a return, ML prescribes a cross sectional regression. ML still ignores anything but the factor data in estimating The GLS cross sectional regression weights the various portfolios by the inverse of the residual covariance matrix . As we saw in Section 12.2, if we include the market return as atestasset,itobviouslyhasnoresidualvariance Rem 0 1 Rem 0 so tt the GLS estimate pays exclusive attention to it in fitting the market line. The same thing happens if the test assets span the factors if a linear com bination of the test assets is equal to the factor and hence has no residual variance. The size portfolios nearly span the market return, so the GLS cross sectional regression is visually indistinguishable from the time series regression in this case. the mean of the factor E f ET ft . However, ML is now allowed to use a different parameter for the factor risk premium that fits average returns to betas, which it does by cross sectional regression. However, ML is a GLS cross sectional regression, not an OLS cross sectional regression. The GLS cross sectional regression in Figure 15.2 is almost exactly identical to the time series regression result it passes right through the origin and the market return, ignoring all the other pricing errors.   1 1 1ET Re",
        "15.1. Three Approaches to the CAPM in Size Portfolios 283 If we allow a free constant in the OLS cross sectional regression, thus allowing a pricing error for the risk free rate, you can see from Figure 15.2 that the OLS cross sectional regression line will fit the size portfolios even better, though allowing a pricing error in the risk free rate as well as the market return. However, a free intercept in an OLS regression on excess returns puts no weight at all on the intercept pricing error. It is a better idea to include the risk free rate as a test asset, either directly by doing the whole thing in levels of returns rather than excess returns or by adding E R e 0,  0 to the cross sectional regression. The GLS cross sectional regression will notice that the T bill rate has no residual variance and so will send the line right through the origin, as it does for the market return. GMM Discount Factor First and Second Stage Figure 15.3 illustrates the GMM discount factor estimate with the same data. The horizontal axis is the second moment of returns and factors rather than beta, but you would not know it from the placement of the dots. The esti mates are calculated using the formulas from Section 13.2. The first stage estimate is an OLS cross sectional regression of average returns on second moments. It minimizes the sum of squared pricing errors, and so produces pricing errors almost exactly equal to those of the OLS cross sectional regres sion of returns on betas. The second stage estimate minimizes pricing errors weighted by the spectral density matrix. The spectral density matrix is not the same as the residual covariance matrix, so the second stage GMM does not Figure 15.3. Average excess return vs. predicted value of 10 CRSP size portfolios, 1926 1998, based on GMM SDF estimate. The model predicts E Re bE ReRem . The second stage estimate of b uses a spectral density estimate with zero lags.",
        "284 15. Time Series, Cross Section, GMM DF Tests go through the market portfolio as does the GLS cross sectional regression. In fact, the slope of the line is slightly higher for the second stage estimate. The spectral density matrix of the discount factor formulation does not reduce to the residual covariance matrix even if we assume the regression model, the asset pricing model, is true, and factors and residuals are i.i.d. normal. In particular, when the market is a test asset, the GLS cross sectional regression focuses all attention on the market portfolio but the second stage GMM DF does not do so. The parameter b is related to  by b  E Rem2 . The other assets still are useful in determining the parameter b , even though, given the market return and the regression model R ei  R em i , seeing titt the other assets does not help to determine the mean of the market return. Overall, the figures do not suggest any strong reason to prefer first and second stage GMM discount factor, time series, OLS, or GLS cross sectional regression in this standard model and data set. The results are affected by the choice of method. In particular, the size of the small firm anomaly is substantially affected by how one draws the market line. But the graphs and analysis do not strongly suggest that any method is better than any other for purposes other than fishing for the answer one wants. Parameter Estimates, Standard Errors, and Tests Table 15.1 presents the parameter estimates and standard errors from time series, cross section, and GMM discount factor approach in the CAPM size portfolio test illustrated by Figures 15.1 and 15.2. The main parameter to be estimated is the slope of the lines in the above figures, the market price of risk  in the expected return beta model and the relation between mean returns and second moments b in the stochastic discount factor model. The big point Table 15.1. Parameter estimates and standard errors Time Series 0.66 0.18 3.67 0.18 3.67 0.20 3.30 0.16 4.13 Beta model  Cross section 1st stage 2.35 0.63 3.73 0.69 3.41 1.00 2.35 GMM DF b 2nd stage Estimate StandardError 2.46 0.61 4.03 2.39 0.64 3.73 2.15 0.69 3.12 Estimate i.i.d. 0 lags 3 lags, NW 24 lags OLS 0.71 0.20 3.55 0.19 3.74 0.21 3.38 0.16 4.44 GLS 0.66 0.18 3.67 0.18 3.67 0.20 3.30 0.16 4.13 Estimates are shown in italic, standard errors in roman type, and t statistics in parentheses. The time series estimate is the mean market return in percent per month. The cross sectional estimate is the slope coefficient  in E Re . The GMM estimate is the parameter b in E Re E Re f b. CRSP monthly data 1926 1998. Lags gives the number of lags in the spectral density matrix. NW uses Newey West weighting in the spectral density matrix.",
        "15.1. Three Approaches to the CAPM in Size Portfolios 285 of Table 15.1 is that the GMM discount factor estimate and standard errors behave very similarly to the traditional estimates and standard errors. The rows compare results with various methods of calculating the spectral density matrix. The row marked Independent and identical dis tribution imposes no serial correlation and regression errors independent of right hand variables, and is identical to the maximum likelihood based formulas. The 0 lag estimate allows conditional heteroskedasticity, but no times series correlation of residuals. The 3 lag, Newey West estimate is a sensible correction for short order autocorrelation. I include the 24 lag spec tral density matrix to show how things can go wrong if you use a ridiculous spectral density matrix. The OLS cross sectional estimate of the beta model 0.71 is a little higher than the mean market return 0.66, in order to better fit all of the assets, as seen in Figure 15.2. The GLS cross sectional estimate is almost exactly the same as the mean market return, and the GLS standard errors are almost exactly the same as the time series standard errors. The b estimates are not directly comparable to the risk premium esti mates, but it is easy to translate their units. Applying the discount factor model with normalization a 1 to the market return itself, E Rem b . E Rem2 With E Rem 0.66 and  Rem 5.47 , we have 100 b 100 0.66 0.662 5.472 2.17. The entries in Table 15.1 are close to this magnitude. Most are slightly larger, as is the OLS cross sectional regres sion, in order to better fit the size portfolios. The t statistics are quite close across methods. The second stage GMM DF estimates as well as standard errors depend on which spectral density weighting matrix is used as a weighting matrix. The results are quite similar for all the sensible spectral density esti mates. The 24 lag spectral density matrix starts to produce unusual estimates. This spectral density estimate will cause lots of problems below. Table 15.2 presents the 2 and F statistics that test whether the pricing errors are jointly significant. The OLS and GLS cross sectional regression, and the first and second stage GMM discount factor tests give exactly the same  2 statistic, though the individual pricing errors and covariance matrix are not the same, so I do not present them separately. The big point of Table 15.2 is that the GMM discount factor method gives almost exactly the same result as the cross sectional regression. For the time series regression, the GRS F test gives almost exactly the same rejection probability as does the asymptotic 2 test. Apparently, the advantages of a statistic that is valid in finite samples is not that important in",
        "286 i.i.d. GRS F 0 lags 3 lags NW 24 lags 15. Time Series, Cross Section, GMM DF Tests Table 15.2. 2 tests that all pricing errors are jointly equal to zero Time series 2 p value 10 8.5 58 0.8 59 10.5 40 11.0 36 432 100 Cross section GMM DF 2 p value this data set. The 2 tests for the time series case without the i.i.d. assump tion are a bit more conservative, with 30 40 p value rather than almost 60 . However, this difference is not large. The one exception is the 2 test using 24 lags and no weights in the spectral density matrix. That matrix turns out not to be positive definite in this sample, with disastrous results for the 2 statistic. Somewhat surprisingly, the CAPM is not rejected. This is because the small firm effect vanishes in the latter part of the sample. I discuss this fact further in Chapter 20. See in particular Figure 20.14. Looking across the rows, the 2 statistic is almost exactly the same for each method. The cross sectional regression and GMM DF estimate have one lower degree of freedom the market premium is estimated from the cross section rather than from the market return , and so show slightly greater rejection probabilities. For a given spectral density estimation technique, the cross sectional regression and the GMM DF approach give almost exactly the same 2 values and rejection probabil ities. The 24 lag spectral density matrix is a disaster as usual. In this case, it is a greater disaster for the time series test than for the cross section or GMM discount factor test. It turns out not to be positive defi nite, so the sample pricing errors produce a nonsensical negative value of  cov  1 . 15.2 Monte Carlo and Bootstrap The parameter distribution for the time series regression estimate is quite similar to that from the GMM discount factor estimate. The size and power of 2 test statistics are nearly identical for the time series regression test and the GMM discount factor test. 2 p value 9 9 8.5 49 10.6 31 11.1 27 7.6 57 10.5 31 11.1 27 7.7 57",
        "15.2. Monte Carlo and Bootstrap 287 A bad spectral density matrix can ruin either time series or GMM discount factor estimates and tests. There is enough serial correlation and heteroskedasticity in the data that conventional i.i.d. formulas produce test statistics with about 1 2 the cor rect size. If you want to do classic regression tests, you should correct the distribution theory rather than use the ML i.i.d. distributions. Econometrics is not just about sensible point estimates, it is about sam pling variability of those estimates, and whether standard error formulas correctly capture that sampling variability. How well do the various stan dard error and test statistic formulas capture the true sampling distribution of the estimates? To answer this question I conduct two Monte Carlos and two bootstraps. I conduct one each under the null that the CAPM is correct, to study size, and one each under the alternative that the CAPM is false, to study power. The Monte Carlo experiments follow the standard ML assumption that returns and factors are i.i.d. normally distributed, and the factors and resid uals are independent as well as uncorrelated. I generate artificial samples of the market return from an i.i.d. normal, using the sample mean and variance of the value weighted return. I then generate artificial size decile returns under the null by Rei 0  Rem  , using the sample resid t itit ual covariance matrix to draw i.i.d. normal residuals it and the sample regression coefficients i . To generate data under the alternative, I add the sample i . I draw 5000 artificial samples. I try a long sample of 876 months, matching the CRSP sample analyzed above. I also draw a short sample of 240 months or 20 years, which is about as short as one should dare try to test a factor model. The bootstraps check whether nonnormalities, autocorrelation, het eroskedasticity, and nonindependence of factors and residuals matter to the sampling distribution in this data set. I do a block bootstrap, resam pling the data in groups of three months with replacement, to preserve the short order autocorrelation and persistent heteroskedasticity in the data. To impose the CAPM, I draw the market return and residuals in the time series regression, and then compute artificial data on decile portfolio returns by Rei 0  Rem  . To study the alternative, I simply redraw all the data in t itit groups of three. Of course, the actual data may display conditioning infor mation not displayed by this bootstrap, such as predictability and conditional heteroskedasticity based on additional variables such as the dividend price ratio, lagged squared returns, or implied volatilities. The first stage GMM discount factor and OLS cross sectional regres sion are nearly identical in every artificial sample, as the GLS cross sectional regression is nearly identical to the time series regression in every",
        "288 15. Time Series, Cross Section, GMM DF Tests sample. Therefore, the important question is to compare the time series regression which is ML with i.i.d. normal returns and factors to the first and second stage GMM DF procedure. For this reason and to save space, I do not include the cross sectional regressions in the Monte Carlo and bootstrap. 2 Tests Table 15.3 presents the 2 tests of the hypothesis that all pricing errors are zero under the null that the CAPM is true, and Table 15.4 presents the 2 tests under the null that the CAPM is false. Each table presents the percent age of the 5000 artificial data sets in which the  2 tests rejected the null at the indicated level. The central point of these tables is that the GMM discount factor test performs almost exactly the same way as the time series test. Com pare the GMM DF entry to its corresponding time series entry; they are all nearly identical. Neither the small efficiency advantage of time series versus cross section, nor the difference between betas and second moments seems to make any difference to the sampling distribution. Table 15.3. Size. Probability of rejection for 2 statistics under the null that all pricing errors are zero Monte Carlo Time series GMM DF Sample size: 240 876 240 876 level : 5 5 1 5 5 1 5 5 1 5 5 1 i.i.d. 7.5 6.0 1.1 6.0 2.8 0.6 0 lags 7.7 6.1 1.1 7.5 6.3 1.0 7.7 4.3 1.0 6.6 3.7 0.9 3 lags, NW 10.7 6.5 1.4 9.7 6.6 1.3 10.5 5.4 1.3 9.5 5.3 1.3 24lags25 39 32 25 41 31 23 38 31 24 41 32 Table 15.4. Power. Probability of rejection for 2 statistics under the null that the CAPM is false, and the true means of the decile portfolio returns are equal to their sample means Block Bootstrap Time series GMM DF 240 876 240 876 Monte Carlo Time series GMM DF Sample size: 240 876 240 876 level : 5 5 1 5 5 1 5 5 1 5 5 1 Block Bootstrap Time series GMM DF i.i.d. 17 48 26 11 40 0lags 17 48 26 17 50 27 15 54 3lags,NW 22 49 27 21 51 29 18 57 24lags 29 60 53 29 66 57 27 63 18 28 14 55 29 31 17 59 33 56 29 68 60 240 876 240 876",
        "294 16. Which Method? quantitative modeling if you knew how to model them, they would not be there. We can only think about the lessons of past experiences. The rest of this chapter collects some thoughts on the choice between ML and a less formal GMM approach, that focuses on economically inter esting rather than statistically informative moments, in the context of empirically evaluating asset pricing models. ML vs. GMM The debate is often stated as a choice between maximum likelihood and GMM. This is a bad way to put the issue. ML is a special case of GMM: it suggests a particular choice of moments that are statistically optimal in a well defined sense. It is all GMM; the issue is the choice of moments. The choice is between moments selected by an auxiliary statistical model, even if completely economically uninterpretable, and moments selected for their economic or data summary interpretation, even if not statistically efficient. Also, there is no such thing as the GMM estimate. GMM is a flexible tool; you can use any aT matrix and gT moments that you want to use. Both ML and GMM are tools that a thoughtful researcher can use in learning what the data says about a given asset pricing model, rather than as stone tablets giving precise directions that lead to truth if followed literally. If fol lowed literally and thoughtlessly, both ML and GMM can lead to horrendous results. Of course, we do not have to pair GMM with the discount factor expres sion of a model, and ML with the expected return beta formulation. Many studies pair discount factor expressions of the model with ML, and many others evaluate expected return beta model by GMM, as I did in Chapter 12 to adjust regression standard errors for non i.i.d. residuals. ML Is Often Ignored As we have seen, ML plus the assumption of normal i.i.d. disturbances leads to easily interpretable time series or cross sectional regressions, empirical procedures that are close to the economic content of the model. However, asset returns are not normally distributed or i.i.d. They have fatter tails than a normal, they are heteroskedastic times of high and times of low volatil ity , they are autocorrelated, and predictable from a variety of variables. If one were to take seriously the ML philosophy and its quest for efficiency, one should model these features of returns. The result would be a differ ent likelihood function, and its scores would prescribe different moment conditions than the familiar and intuitive time series or cross sectional regressions. Interestingly, few empirical workers do this. The exceptions tend to be papers whose primary point is illustration of econometric technique rather than empirical findings. ML seems to be fine when it suggests easily",
        "Which Method? 295 interpretable regressions; when it suggests something else, people use the regressions anyway. For example, ML prescribes that one estimate  s without a constant.  s are almost universally estimated with a constant. Researchers often run cross sectional regressions rather than time series regressions, even when the factors are returns. ML specifies a GLS cross sectional regression, but many empirical workers use OLS cross sectional regressions instead, dis trusting the GLS weighting matrix. The true ML formulas require one to iterate between betas, covariance matrix, and the cross sectional regression. Empirical applications usually use the unconstrained estimates of all these quantities. And of course, any of the regression tests continue to be run at all, with ML justifications, despite the fact that returns are not i.i.d. nor mal. The regressions came first, and the maximum likelihood formalization came later. If we had to assume that returns had a gamma distribution to justify the regressions, it is a sure bet that we would make that assumption behind ML instead of the normal i.i.d. assumption! Researchers must not really believe that their null hypotheses, statisti cal and economic, are exactly correct. They want estimates and tests that are robust to reasonable model misspecifications. They also want estimates and tests that are easily interpretable, that capture intuitively clear stylized facts in the data, and that relate directly to the economic concepts of the model. Such estimates are persuasive because the reader can see that they are robust.1 In pursuit of these goals, researchers seem willing to sacrifice some of the efficiency that would obtain if the null economic and statistical models were exactly correct. ML does not necessarily produce robust or easily interpretable esti mates. It was not designed to do so. The point and advertisement of ML is that it provides efficient estimates; it uses every scrap of information in the statistical and economic model in the quest for efficiency. It does the right efficient thing if the model is true. It does not necessarily do the reasonable thing for approximate models. OLS vs. GLS Cross Sectional Regressions One place in which this argument crystallizes is in the choice between OLS and GLS cross sectional regressions, or equivalently between first and second stage GMM. Chapter 15 can lead to a mistaken impression that the choice does not matter that much. This is true to some extent in that simple environ ment, but not in more complex environments. For example, Fama and 1 Following this train of thought, one might want to pursue estimation strategies that are even more robust than OLS, since OLS places a lot of weight on outliers. For example, Knez and Ready 1997 claim that size and value effects depend crucially on a few outliers.",
        "296 16. Which Method? French 1997 report important correlations between betas and pricing errors in a time series test of a three factor model on industry portfo lios. This correlation cannot happen with an OLS cross sectional estimate, as the cross sectional estimate sets the cross sectional correlation between right hand variables betas and error terms pricing errors to zero by con struction. As another example, first stage estimates seem to work better in factor pricing models based on macroeconomic data. Figure 2.4 presents the first stage estimate of the consumption based model. The second stage esti mate produced much larger individual pricing errors, because by so doing it could lower pricing errors of portfolios with strong long short positions required by the spectral density matrix. The same thing happened in the investment based factor pricing model of Cochrane 1996 , and the scaled consumption based model of Lettau and Ludvigson 2001a . Authors as far back as Fama and MacBeth 1973 have preferred OLS cross sectional regressions, distrusting the GLS weights. GLS and second stage GMM gain their asymptotic efficiency when the covariance and spectral density matrices have converged to their population values. GLS and second stage GMM use these matrices to find well measured portfolios: portfolios with small residual variance for GLS, and small vari ance of discounted return for GMM. The danger is that these quantities are poorly estimated in a finite sample, that sample minimum variance portfo lios bear little relation to population minimum variance portfolios. This by itself should not create too much of a problem for a perfect model, one that prices all portfolios. But an imperfect model that does a very good job of pricing a basic set of portfolios may do a poor job of pricing strange lin ear combinations of those portfolios, especially combinations that involve strong long and short positions, positions that really are outside the pay off space given transactions, margin, and short sales constraints. Thus, the danger is the interaction between spurious sample minimum variance portfolios and the specification errors of the model. Interestingly, Kandel and Stambaugh 1995 and Roll and Ross 1995 argue for GLS cross sectional regressions also as a result of model misspeci fication. They start by observing that so long as there is any misspecification at all so long as the pricing errors are not exactly zero; so long as the market proxy is not exactly on the mean variance frontier then there are portfolios that produce arbitrarily good and arbitrarily bad fits in plots of expected returns versus betas. Since even a perfect model leaves pricing errors in sample, this is always true in samples. It is easy to see the basic argument. Take a portfolio long the positive alpha securities and short the negative alpha securities; it will have a really big alpha! More precisely, if the original securities follow E R e   ,",
        "304 16. Which Method? would require at least some transformation of units so that OLS is not wildly inefficient. Statistical Philosophy The history of empirical work that has been persuasive that has changed people s understanding of the facts in the data and which economic mod els understand those facts looks a lot different than the statistical theory preached in econometrics textbooks. The CAPM was taught, believed in, and used for years despite formal statistical rejections. It only fell by the wayside when other, coherent views of the world were offered in the multifactor models. And the multifactor models are also rejected! It seems that it takes a model to beat a model, not a rejection. Even when evaluating a specific model, most of the interesting cal culations come from examining specific alternatives rather than overall pricing error tests. The original CAPM tests focused on whether the inter cept in a cross sectional regression was higher or lower than the risk free rate, and whether individual variance entered into cross sectional regres sions. The CAPM fell when it was found that characteristics such as size and book market do enter cross sectional regressions, not when generic pricing error tests rejected. Influential empirical work tells a story. The most efficient procedure does not seem to convince people if they cannot transparently see what styl ized facts in the data drive the result. A test of a model that focuses on its ability to account for the cross section of average returns of interesting port folios will in the end be much more persuasive than one that say focuses on the model s ability to explain the fifth moment of the second portfolio, even if ML finds the latter moment much more statistically informative. Most recently, Fama and French 1988b and 1993 are good examples of empirical work that changed many people s minds, in this case that long horizon returns really are predictable, and that we need a multifactor model rather than the CAPM to understand the cross section of average returns. These papers are not stunning statistically: long horizon predictability is on the edge of statistical significance, and the multifactor model is rejected by the GRS test. But these papers made clear what stylized and robust facts in the data drive the results, and why those facts are economically sensi ble. For example, the 1993 paper focused on tables of average returns and betas. Those tables showed strong variation in average returns that was not matched by variation in market betas, yet was matched by variation in betas on new factors. There is no place in statistical theory for such a table, but it is much more persuasive than a table of 2 values for pricing error tests. On the other hand, I can think of no case in which the application of a clever statistical model to wring the last ounce of efficiency out of a data set,",
        "436 20. Expected Returns in the Time Series and Cross Section The CAPM The first tests of the CAPM such as Lintner 1965b were not a great success. If you plot or regress the average returns versus betas of individual stocks, you find a lot of dispersion, and the slope of the line is much too flat it does not go through any plausible risk free rate. Miller and Scholes 1972 diagnosed the problem. Betas are mea sured with error, and measurement error in right hand variables biases down regression coefficients. Fama and MacBeth 1973 and Black, Jensen, and Scholes 1972 addressed the problem by grouping stocks into port folios. Portfolio betas are better measured because the portfolio has lower residual variance. Also, individual stock betas vary over time as the size, lever age, and risks of the business change. Portfolio betas may be more stable over time, and hence easier to measure accurately. There is a second reason for portfolios. Individual stock returns are so volatile that you cannot reject the hypothesis that all average returns are the same.  T is big when  40 80 . By grouping stocks into portfolios based on some characteristic other than firm name related to average returns, you reduce the portfolio variance and thus make it possible to see average return deferences. Finally, I think much of the attachment to portfolios comes from a desire to more closely mimic what actual investors would do rather than simply form a statistical test. Fama and MacBeth and Black, Jensen, and Scholes formed their port folios on betas. They found individual stock betas, formed stocks into portfolios based on their betas, and then estimated the portfolio s beta in the following period. More recently, size, book market, industry, and many other characteristics have been used to form portfolios. Ever since, the business of testing asset pricing models has been conducted in a simple loop: 1. Find a characteristic that you think is associated with average returns. Sort stocks into portfolios based on the characteristic, and check that there is a difference in average returns between portfolios. Worry here about measurement, survival bias, fishing bias, and all the other things that can ruin a pretty picture out of sample. 2. Compute betas for the portfolios, and check whether the average return spread is accounted for by the spread in betas. 3. If not, you have an anomaly. Consider multiple betas. This is the traditional procedure, but econometrics textbooks urge you not to group data in this way. They urge you to use the characteristic as an For a generation, portfolios with high average returns also had high betas. I illustrate with the size based portfolios.",
        "20.2. The Cross Section: CAPM and Multifactor Models 437 instrument for the poorly measured right hand variable instead. It is an inter esting and unexplored idea whether this instrumental variables approach could fruitfully bring us back to the examination of individual securities rather than portfolios. The CAPM proved stunningly successful in empirical work. Time after time, every strategy or characteristic that seemed to give high average returns turned out to also have high betas. Strategies that one might have thought gave high average returns such as holding very volatile stocks turned out not to have high average returns when they did not have high betas. To give some sense of that empirical work, Figure 20.8 presents a typ ical evaluation of the Capital Asset Pricing Model. Chapter 15 presented some of the methodological issues surrounding this evaluation; here I focus on the facts. I examine 10 portfolios of NYSE stocks sorted by size total market capitalization , along with a portfolio of corporate bonds and long term government bonds. As the spread along the vertical axis shows, there is a sizeable spread in average returns between large stocks lower average return and small stocks higher average return , and also a large spread between stocks and bonds. The figure plots these average returns against market betas. You can see how the CAPM prediction fits: portfolios with higher average returns have higher betas. In particular, notice that the Figure 20.8. The CAPM. Average returns vs. betas on the NYSE value weighted portfolio for 10 size sorted stock portfolios, government bonds, and corporate bonds, 1947 1996. The solid line draws the CAPM prediction by fitting the market proxy and treasury bill rates exactly a time series test . The dashed line draws the CAPM prediction by fitting an OLS cross sectional regression to the displayed data points. The small firm portfolios are at the top right. The points far down and to the left are the government bond and treasury bill returns.",
        "438 20. Expected Returns in the Time Series and Cross Section long term and corporate bonds have mean returns in line with their low betas, despite their standard deviations nearly as high as those of stocks. Comparing this graph with the similar Figure 2.4 of the consumption based model, the CAPM fits very well. In fact, Figure 20.8 captures one of the first significant failures of the CAPM. The smallest firms the far right portfolio seem to earn an average return a few percent too high given their betas. This is the celebrated small firm effect Banz 1981 . Would that all failed economic theories worked so well! It is also atypical in that the estimated market line through the stock portfolios is steeper than predicted, while measurement error in betas usually means that the estimated market line is too flat. Fama French 3 Factors In retrospect, it is surprising that the CAPM worked so well for so long. The assumptions on which it is built are very stylized and simplified. Asset pricing theory recognized at least since Merton 1971, 1973a the theoretical possibility, indeed probability, that we should need factors, state variables, or sources of priced risk beyond movements in the market portfolio in order to explain why some average returns are higher than others. The Fama French model is one of the most popular multifactor models that now dominate empirical research. Fama and French 1993 presents the model; Fama and French 1996 gives an excellent summary, and also shows how the three factor model performs in evaluating expected return puzzles beyond the size and value effects that motivated it. Value stocks have market values that are small relative to the accoun tant s book value. Book values essentially track past investment expen ditures. Book value is a better divisor for individual firm price than are dividends or earnings, which can be negative. This category of stocks has given large average returns. Growth stocks are the opposite of value and have had low average returns. Since low prices relative to dividends, earnings, or book value forecast times when the market return will be high, it is natural to suppose that these same signals forecast categories of stocks that will do well; the value effect is the cross sectional analogy to price ratio predictability in the time series. Book market sorted portfolios show a large variation in average returns that is unrelated to market betas. The Fama and French three factor model successfully explains the average returns of the 25 size and book market sorted portfolios with a three factor model, consisting of the market, a small minus big SMB portfolio, and a high minus low HML portfolio.",
        "20.2. The Cross Section: CAPM and Multifactor Models 439 Figure 20.9. Average returns vs. market beta for 25 stock portfolios sorted on the basis of size and book market ratio. High average returns are consistent with the CAPM, if these categories of stocks have high sensitivities to the market, high betas. However, small and especially value stocks seem to have abnormally high returns even after accounting for market beta. Conversely, growth stocks seem to do sys tematically worse than their CAPM betas suggest. Figure 20.9 shows this value size puzzle. It is just like Figure 20.8, except that the stocks are sorted into portfolios based on size and book market ratio1 rather than size alone. As you can see, the highest portfolios have three times the average excess return of the lowest portfolios, and this variation has nothing at all to do with market betas. Figures 20.10 and 20.11 dig a little deeper to diagnose the problem, by connecting portfolios that have different size within the same book market category, and different book market within size category. As you can see, variation in size produces a variation in average returns that is positively related to variation in market betas, as we had in Figure 20.9. Variation in book market ratio produces a variation in average return that is negatively related to market beta. Because of this value effect, the CAPM is a disaster when confronted with these portfolios. Since the size effect disappeared in 1980, it is likely that almost the whole story can be told with book market effects alone. To explain these patterns in average returns, Fama and French advocate a multifactor model with the market return, the return of small less big stocks 1 I thank Gene Fama for providing me with these data.",
        "440 20. Expected Returns in the Time Series and Cross Section Figure 20.10. Average excess returns vs. market beta. Lines connect portfolios with different size category within book market categories. Figure 20.11. Average excess returns vs. market beta. Lines connect portfolios with different book market categories within size categories. SMB and the return of high book market minus low book market stocks HML as three factors. They show that variation in average returns of the 25 size and book market portfolios can be explained by varying loadings betas on the latter two factors. All their portfolios have betas close to one on the market portfolio. Thus, market beta explains the average return difference between stocks and bonds, but not across categories of stocks.",
        "20.2. The Cross Section: CAPM and Multifactor Models 441 Figure 20.12. Average excess return vs. prediction of the Fama French three factor model. Lines connect portfolios of different size categories within book market category. Figure 20.13. Average excess return vs. prediction of the Fama French three factor model. Lines connect portfolios of different book market category within the same size category. Figures 20.12 and 20.13 illustrate Fama and French s results. The vertical axis is still the average return of the 25 size and book market portfolios. Now, the horizontal axis is the predicted values from the Fama French three factormodel.Thepointsshouldalllieona45 lineifthemodeliscorrect. The points lie much closer to this prediction than they do in Figures 20.10",
        "442 20. Expected Returns in the Time Series and Cross Section and 20.11. The worst fit is for the growth stocks lowest line, Figure 20.12 , for which there is little variation in average return despite large variation in size beta as one moves from small to large firms. What Are the Size and Value Factors? What are the macroeconomic risks for which the Fama French factors are proxies or mimicking portfolios? There are hints of some sort of distress or recession factor at work. A central part of the Fama French model is the fact that these three pricing factors also explain a large part of the ex post variation in the 25 portfolios the R2 in time series regressions are very high. In this sense, one can regard it as an APT rather than a macroeconomic factor model. The Fama French model is not a tautology, despite the fact that factors and test portfolios are based on the same set of characteristics. We would like to understand the real, macroeconomic, aggregate, nondiversifiable risk that is proxied by the returns of the HML and SMB portfolios. Why are investors so concerned about holding stocks that do badly at the times that the HML value less growth and SMB small cap less large cap portfolios do badly, even though the market does not fall? Fama and French 1996 note that the typical value firm has a price that has been driven down from a long string of bad news, and is now in or near financial distress. Stocks bought on the verge of bankruptcy have come back more often than not, which generates the high average returns of this strategy. This observation suggests a natural interpretation of the value premium: If a credit crunch, liquidity crunch, flight to quality, or similar financial event comes along, stocks in financial distress will do very badly, and this is just the sort of time at which one particularly does not want to hear that one s stocks have become worthless! One cannot count the distress of the individual firm as a risk factor. Such distress is idiosyncratic and can be diversified away. Only aggregate events that average investors care about can result in a risk premium. Unfortunately, empirical support for this theory is weak, since the HML portfolio does not covary strongly with other measures of aggregate financial distress. Still, it is a possible and not totally tested interpretation, since we have so few events of actual systematic financial stress in recent history. Heaton and Lucas 1997b results add to this story for the value effect. They note that the typical stockholder is the proprietor of a small, privately held business. Such an investor s income is of course particularly sensitive to the kinds of financial events that cause distress among small firms and dis tressed value firms. Such an investor would therefore demand a substantial",
        "20.2. The Cross Section: CAPM and Multifactor Models 443 premium to hold value stocks, and might hold growth stocks despite a low premium. Lettau and Ludvigson 2001a also discussed in the next section doc ument that HML has a time varying beta on both the market return and on consumption. Thus, though there is very little unconditional correlation between HML and recession measures, Lettau and Ludvigson document that HML is sensitive to bad news in bad times. Liew and Vassalou 1999 is an example of current attempts to link value and small firm returns to macroeconomic events. They find that in many countries counterparts to HML and SMB contain information above and beyond that in the market return for forecasting GDP growth. For example, they report a regression GDPt t 1 a 0.065 MKTt 1 t 0.058 HMLt 1 t t 1. GDPt t 1 denotes the next year s GDP growth and MKT,HML denote the previous year s return on the market index and HML portfolio. Thus, a 10 HML return reflects a 1 2 percentage point rise in the GDP forecast. On the other hand, one can ignore Fama and French s motivation and regard the model as an arbitrage pricing theory. If the returns of the 25 size and book market portfolios could be perfectly replicated by the returns of the three factor portfolios if the R2 in the time series regressions were 100 then the multifactor model would have to hold exactly, in order to preclude arbitrage opportunities. In fact the R 2 of Fama and French s time series regressions are all in the 90 95 range, so extremely high Sharpe ratios for the residuals would have to be invoked for the model not to fit well. Equivalently, given the average returns and the failure of the CAPM to explain those returns, there would be near arbitrage opportunities if value and small stocks did not move together in the way described by the Fama French model. One way to assess whether the three factors proxy for real macroeco nomic risks is by checking whether the multifactor model prices additional portfolios, and especially portfolios that do not have high R2 values. Fama and French 1996 extend their analysis in this direction: They find that the SMB and HML portfolios comfortably explain strategies based on alterna tive price multiples P E, B M , strategies based on five year sales growth this is especially interesting since it is the only strategy that does not form portfolios based on price variables , and the tendency of five year returns to reverse. All of these strategies are not explained by CAPM betas. How ever, they all also produce portfolios with high R2 values in a time series regression on the HML and SMB portfolios! This is good and bad news. It might mean that the model is a good APT: that the size and book market characteristics describe the major sources of priced variation in all stocks. On the other hand, it might mean that these extra sorts just have not iden tified other sources of priced variation in stock returns. Fama and French",
        "444 20. Expected Returns in the Time Series and Cross Section also find that HML and SMB do not explain momentum, despite large R2 values. More on momentum later. One s first reaction may be that explaining portfolios sorted on the basis of size and book market by factors sorted on the same basis is a tautology. This is not the case. For example, suppose that average returns were higher for stocks whose ticker symbols start later in the alphabet. Maybe investors search for stocks alphabetically, so the later stocks are overlooked. This need not trouble us if Z stocks happened to have higher betas. If not if letter of the alphabet were a CAPM anomaly like book market however, it would not necessarily follow that letter based stock portfolios move together. Adding A L and M Z portfolios to the right hand side of a regression of the 26 A,B,C, etc. portfolios on the market portfolio need not and probably does not increase the R 2 at all. The size and book market premia are hard to measure, and seem to have declined substantially in recent years. But even if they decline back to CAPM values, Fama and French will still have found a surprisingly large source of common movement in stock returns. More to the point, in testing a model, it is exactly the right thing to do to sort stocks into portfolios based on characteristics related to expected returns. When Black, Jensen, and Scholes and Fama and MacBeth first tested the CAPM, they sorted stocks into portfolios based on betas, because betas are a good characteristic for sorting stocks into portfolios that have a spread in average returns. If your portfolios have no spread in average returns if you just choose 25 random portfolios, then there will be nothing for the asset pricing model to test. In fact, despite the popularity of the Fama French 25, there is really no fundamental reason to sort portfolios based on two way or larger sorts of individual characteristics. You should use all the characteristics at hand that believably! indicate high or low average returns and simply sort stocks according to a one dimensional measure of expected returns. The argument over the status of size and book market factors contin ues, but the important point is that it does so. Faced with the spectacular failure of the CAPM documented in Figures 20.9 and 20.11 one might have thought that any hope for a rational asset pricing theory was over. Now we are back where we were, examining small anomalies and argu ing over refinements and interpretations of the theory. That is quite an accomplishment! Macroeconomic Factors Labor income, industrial production, news variables, and conditional asset pricing models have also all had some successes as multifactor models.",
        "20.2. The Cross Section: CAPM and Multifactor Models 445 I have focused on the size and value factors since they provide the most empirically successful multifactor model to date, and have therefore attracted much attention. Several authors have used macroeconomic variables as factors in order to examine directly the story that stock performance during bad macroeco nomic times determines average returns. Jagannathan and Wang 1996 and Reyfman 1997 use labor income; Chen, Roll, and Ross 1986 use indus trial production and inflation among other variables. Cochrane 1996 uses investment growth. All these authors find that average returns line up against betas calculated using these macroeconomic indicators. The factors are the oretically easier to motivate, but none explains the value and size portfolios as well as the theoretically less solid, so far size and value factors. Lettau and Ludvigson 2001a specify a macroeconomic model that does just as well as the Fama French factors in explaining the 25 Fama French portfolios. Their plots of actual average returns versus model pre dictions show a relation as strong as those of Figures 20.12 and 20.13. Their model is mt 1 a b cay ct 1, t where cay is a measure of the consumption wealth ratio. This is a scaled factor model of the sort advocated in Chapter 8. You can think of it as capturing a time varying risk aversion. Though Merton s 1971, 1973a theory says that variables which predict market returns should show up as factors which explain cross sectional vari ation in average returns, surprisingly few papers have actually tried to see whether this is true, now that we do have variables that we think forecast the market return. Campbell 1996 and Ferson and Harvey 1999 are among the few exceptions. Momentum and Reversal Since a string of good returns gives a high price, it is not surprising that stocks that do well for a long time and hence build up a high price subsequently do poorly, and stocks that do poorly for a long time and Sorting stocks based on past performance, you find that a portfolio that buys long term losers and sells long term winners does better than the opposite individual stock long term returns mean revert. This reversal effect makes sense given return predictability and mean reversion, and is explained by the Fama French three factor model. However, a port folio that buys short term winners and sells short term losers also does well momentum. This effect is a puzzle.",
        "Problems 453 Figure 20.14. Average returns vs. market betas. CRSP size portfolios less treasury bill rate, monthly data 1979 1998. stocks in the same industry move together; the fact that value or small stocks also move together need not cause a ripple. The surprise is that investors seem to earn an average return premium for holding these addi tional sources of common movement, whereas the CAPM predicts that given beta such common movements should have no effect on a portfolio s average returns. Problems Chapter 20 1. Does equation 20.11 condition down to information sets coarser than those observed by agents? Or must we assume that whatever VAR is used by the econometrician contains all information seen by agents? 2. Show that the two regressions in Table 20.9 are complementary that the coefficients add up to one, mechanically, in sample. 3. Derive the return innovation decomposition 20.11 , directly. Write the return rt dt  pt dt pt 1 dt 1 ."
    ],
    "Topic 2": [
        "Preface xv varying risk premium. Similarly, we have learned that some measure of risk aversion must be quite high, or people would all borrow like crazy to buy stocks. Most macroeconomics pursues small deviations about perfect foresight equilibria, but the large equity premium means that volatility is a first order effect, not a second order effect. Standard macroeconomic models predict that people really do not care much about business cycles Lucas 1987 . Asset prices reveal that they do that they forego substantial return premia to avoid assets that fall in recessions. This fact ought to tell us something about recessions! This book advocates a discount factor generalized method of moments view of asset pricing theory and associated empirical procedures. I summa rize asset pricing by two equations: pt E mt 1xt 1 , mt 1 f data, parameters , wherept assetprice,xt 1 assetpayoff,mt 1 stochasticdiscountfactor. The major advantages of the discount factor moment condition approach are its simplicity and universality. Where once there were three apparently different theories for stocks, bonds, and options, now we see each as special cases of the same theory. The common language also allows us to use insights from each field of application in other fields. This approach allows us to conveniently separate the step of specify ing economic assumptions of the model second equation from the step of deciding which kind of empirical representation to pursue or under stand. For a given model choice of f we will see how the first equation can lead to predictions stated in terms of returns, price dividend ratios, expected return beta representations, moment conditions, continuous ver sus discrete time implications, and so forth. The ability to translate between such representations is also very helpful in digesting the results of empir ical work, which uses a number of apparently distinct but fundamentally connected representations. Thinking in terms of discount factors often turns out to be much sim pler than thinking in terms of portfolios. For example, it is easier to insist that there is a positive discount factor than to check that every possible port folio that dominates every other portfolio has a larger price, and the long arguments over the APT stated in terms of portfolios are easy to digest when stated in terms of discount factors. The discount factor approach is also associated with a state space geom etry in place of the usual mean variance geometry, and this book emphasizes the state space intuition behind many classic results. For these reasons, the discount factor language and the associated state space geometry are common in academic research and high tech practice.",
        "1 Consumption Based Model and Overview An investor must decide how much to save and how much to consume, and what portfolio of assets to hold. The most basic pricing equation comes from the first order condition for that decision. The marginal utility loss of consuming a little less today and buying a little more of the asset should equal the marginal utility gain of consuming a little more of the asset s payoff in the future. If the price and payoff do not satisfy this relation, the investor should buy more or less of the asset. It follows that the asset s price should equal the expected discounted value of the asset s payoff, using the investor s marginal utility to discount the payoff. With this simple idea, I present many classic issues in finance. Interest rates are related to expected marginal utility growth, and hence to the expected path of consumption. In a time of high real interest rates, it makes sense to save, buy bonds, and then consume more tomorrow. There fore, high real interest rates should be associated with an expectation of growing consumption. Most importantly, risk corrections to asset prices should be driven by the covariance of asset payoffs with marginal utility and hence by the covariance of asset payoffs with consumption. Other things equal, an asset that does badly in states of nature like a recession, in which the investor feels poor and is consuming little, is less desirable than an asset that does badly in states of nature like a boom in which the investor feels wealthy and is consuming a great deal. The former asset will sell for a lower price; its price will reflect a discount for its riskiness, and this riskiness depends on a co variance, not a variance. Marginal utility, not consumption, is the fundamental measure of how you feel. Most of the theory of asset pricing is about how to go from marginal utility to observable indicators. Consumption is low when marginal utility is high, of course, so consumption may be a useful indicator. Consumption is also low and marginal utility is high when the investor s other assets have done poorly; thus we may expect that prices are low for assets that covary 3",
        "1.4. Classic Issues in Finance 11 Real interest rates are high when people are impatient  , when expected consumption growth is high intertemporal substitution , or when risk is low precautionary saving . A more curved utility function  or a lower elas ticity of intertemporal substitution 1  means that interest rates are more sensitive to changes in expected consumption growth. The risk free rate is given by Rf 1 E m . 1.6 The risk free rate is known ahead of time, so p E mx becomes 1 E mRf E m Rf . If a risk free security is not traded, we can define Rf 1 E m as the shadow risk free rate. In some models it is called the zero beta rate. If one introduced a risk free security with return Rf 1 E m , investors wouldbejustindifferenttobuyingorsellingit.IuseRf tosimplifyformulas below with this understanding. To think about the economics behind real interest rates in a simple setup, use power utility u c c  . Start by turning off uncertainty, in which case Rf 1ct 1 .  ct We can see three effects right away: 1. Real interest rates are high when people are impatient, i.e. when  is low. If everyone wants to consume now, it takes a high interest rate to convince them to save. 2. Real interest rates are high when consumption growth is high. In times of high interest rates, it pays investors to consume less now, invest more, and consume more in the future. Thus, high interest rates lower the level of consumption today, while raising its growth rate from today to tomorrow. 3. Realinterestratesaremoresensitivetoconsumptiongrowthifthepower parameter  is large. If utility is highly curved, the investor cares more about maintaining a consumption profile that is smooth over time, and is less willing to rearrange consumption over time in response to interest rate incentives. Thus it takes a larger interest rate change to induce him to a given consumption growth. To understand how interest rates behave when there is some uncertainty, I specify that consumption growth is lognormally distributed. In this case, the real risk free rate equation becomes 2 rf  E lnc 2 lnc , 1.7 t tt 12tt 1",
        "12 1. Consumption Based Model and Overview whereIhavedefinedthelogrisk freeraterf andsubjectivediscountrateby rf lnRf;  e , tt and denotes the first difference operator, lnct 1 lnct 1 lnct. To derive expression 1.7 for the risk free rate, start with  Rf 1 E  ct 1 . t t ct Using the fact that normal z means we have Then take logarithms. The combination of lognormal distributions and power utility is one of the basic tricks to getting analytical solutions in this kind of model. Section 1.5 shows how to get the same result in continuous time. Looking at 1.7 , we see the same results as we had with the deterministic case. Real interest rates are high when impatience  is high and when con sumption growth is high; higher  makes interest rates more sensitive to consumption growth. The new  2 term captures precautionary savings. When consumption is more volatile, people with this utility function are more wor ried about the low consumption states than they are pleased by the high consumption states. Therefore, people want to save more, driving down interest rates. We can also read the same terms backwards: consumption growth is high when real interest rates are high, since people save more now and spend it in the future, and consumption is less sensitive to interest rates as the desire for a smooth consumption stream, captured by  , rises. Section 2.2 takes up the question of which way we should read this equation as consumption determining interest rates, or as interest rates determining consumption. For the power utility function, the curvature parameter  simul taneously controls intertemporal substitution aversion to a consump tion stream that varies over time, risk aversion aversion to a consumption stream that varies across states of nature, and precautionary savings, which turns out to depend on the third derivative of the utility function. This link is particular to the power utility function. More general utility functions loosen the links between these three quantities. E ez eE z 1 2 2 z you can check this by writing out the integral that defines the expectation , Rf e e Et lnct 1 2 2 t2 lnct 1 1. t t",
        "1.4. Classic Issues in Finance 13 Risk Corrections Payoffs that are positively correlated with consumption growth have lower prices, to compensate investors for risk. E x p cov m,x , Rf E Ri Rf Rfcovm,Ri . Expected returns are proportional to the covariance of returns with discount factors. Using the definition of covariance cov m,x E mx E m E x , we can write p E mx as p E m E x cov m,x . Substituting the risk free rate equation 1.6 , we obtain E x p cov m,x . Rf 1.8 1.9 The first term in 1.9 is the standard discounted present value formula. This is the asset s price in a risk neutral world if consumption is constant or if utility is linear. The second term is a risk adjustment. An asset whose payoff covaries positively with the discount factor has its price raised and vice versa. To understand the risk adjustment, substitute back for m in terms of consumption, to obtain E x cov u ct 1 ,xt 1 p . 1.10 Marginal utility u c declines as c rises. Thus, an asset s price is lowered if its payoff covaries positively with consumption. Conversely, an asset s price is raised if it covaries negatively with consumption. Why? Investors do not like uncertainty about consumption. If you buy an asset whose payoff covaries positively with consumption, one that pays off well when you are already feeling wealthy, and pays off badly when you are already feeling poor, that asset will make your consumption stream more volatile. You will require a low price to induce you to buy such an asset. If you buy an asset whose payoff covaries negatively with consumption, it helps to smooth consumption and so is more valuable than its expected payoff might indicate. Insurance is an extreme example. Insurance pays off exactly when Rf u ct",
        "20 1. Consumption Based Model and Overview Slope of the Mean Standard Deviation Frontier and Equity Premium Puzzle The Sharpe ratio is limited by the volatility of the discount factor. The maximal risk return trade off is steeper if there is more risk or more risk aversion, E R Rf  m  R  lnc . E m This formula captures the equity premium puzzle, which suggests that either people are very risk averse, or the stock returns of the last 50 years were good luck which will not continue. The ratio of mean excess return to standard deviation E Ri Rf  Ri is known as the Sharpe ratio. It is a more interesting characterization of a security than the mean return alone. If you borrow and put more money into a security, you can increase the mean return of your position, but you do not increase the Sharpe ratio, since the standard deviation increases at the same rate as the mean. The slope of the mean standard deviation frontier is the largest available Sharpe ratio, and thus is naturally interesting. It answers how much more mean return can I get by shouldering a bit more volatility in my portfolio? Let Rmv denote the return of a portfolio on the frontier. From equation 1.17 , the slope of the frontier is E Rmv Rf  Rmv E m  m  m Rf . Thus, the slope of the frontier is governed by the volatility of the discount factor. For an economic interpretation, again consider the power utility function, u c c  , E Rmv Rf  Rmv  ct 1 ct   . 1.19 E ct 1 ct The standard deviation on the right hand side is large if consumption is volatile or if  is large. We can state this approximation precisely using the",
        "1.4. Classic Issues in Finance 21 lognormal assumption. If consumption growth is lognormal, E Rmv Rf 2 2  Rmv e  lnct 1 1  lnc . 1.20 A problem at the end of the chapter guides you through the algebra of the first equality. The relation is exact in continuous time, and thus the approximation is easiest to derive by reference to the continuous time result; see Section 1.5. Reading the equation, the slope of the mean standard deviation frontier is higher if the economy is riskier if consumption is more volatile or if investors are more risk averse. Both situations naturally make investors more reluc tant to take on the extra risk of holding risky assets. Both situations also raise the slope of the expected return beta line of the consumption beta model, 1.16 . Or, conversely, in an economy with a high Sharpe ratio, low risk aversion investors should take on so much risk that their consumption becomes volatile. In postwar U.S. data, the slope of the historical mean standard deviation frontier, or of average return beta lines, is much higher than reasonable risk aversion and consumption volatility estimates suggest. This is the equity premium puzzle. Over the last 50 years in the United States, real stock returns have averaged 9 with a standard deviation of about 16 , while the real return on treasury bills has been about 1 . Thus, the historical annual market Sharpe ratio has been about 0.5. Aggregate nondurable and services consumption growth had a mean and standard deviation of about 1 . We can only reconcile these facts with 1.20 if investors have a risk aversion coefficient of 50! Obvious ways of generalizing the calculation just make matters worse. Equation 1.20 relates consumption growth to the mean variance frontier of all contingent claims. Market indices with 0.5 Sharpe ratios are if anything inside that frontier, so recognizing market incompleteness makes matters worse. Aggregate consumption has about 0.2 correlation with the market return, while the equality 1.20 takes the worst possible case that consump tion growth and asset returns are perfectly correlated. If you add this fact, you need risk aversion of 250 to explain the market Sharpe ratio! Individu als have riskier consumption streams than aggregate, but as their risk goes up their correlation with any aggregate must decrease proportionally, so to first order recognizing individual risk will not help either. Clearly, either 1 people are a lot more risk averse than we might have thought, 2 the stock returns of the last 50 years were largely good luck rather than an equilibrium compensation for risk, or 3 something is deeply wrong with the model, including the utility function and use of aggregate consumption data. This equity premium puzzle has attracted the attention",
        "22 1. Consumption Based Model and Overview of a lot of research in finance, especially on the last item. I return to the equity premium in more detail in Chapter 21. Random Walks and Time Varying Expected Returns So far, we have concentrated on the behavior of prices or expected returns across assets. We should also consider the behavior of the price or return of a given asset over time. Going back to the basic first order condition, pt u ct Et u ct 1 pt 1 dt 1 . 1.21 If investors are risk neutral, i.e., if u c is linear or there is no variation in consumption, if the security pays no dividends between t and t 1, and for short time horizons where  is close to 1, this equation reduces to pt Et pt 1 . Equivalently, prices follow a time series process of the form If investors are risk neutral, returns are unpredictable, and prices follow martingales. In general, prices scaled by marginal utility are martingales, and returns can be predictable if investors are risk averse and if the conditional second moments of returns and discount factors vary over time. This is more plausible at long horizons. pt 1 pt t 1. If the variance 2  is constant, prices follow a random walk. More gen t t 1 erally, prices follow a martingale. Intuitively, if the price today is a lot lower than investors expectations of the price tomorrow, then investors will try to buy the security. But this action will drive up the price of the security until the price today does equal the expected price tomorrow. Another way of saying the same thing is that returns should not be predictable; dividing by pt , expected returns Et pt 1 pt 1 should be constant; returns should be like coin flips. The more general equation 1.21 says that prices should follow a mar tingale after adjusting for dividends and scaling by marginal utility. Since martingales have useful mathematical properties, and since risk neutral ity is such a simple economic environment, many asset pricing results are easily derived by scaling prices and dividends by discounted marginal util ity first, and then using risk neutral formulas and risk neutral economic arguments.",
        "1.4. Classic Issues in Finance 23 Since consumption and risk aversion do not change much day to day, we might expect the random walk view to hold pretty well on a day to day basis. This idea contradicts the still popular notion that there are systems or technical analysis by which one can predict where stock prices are going on any given day. The random walk view has been remarkably successful. Despite decades of dredging the data, and the popularity of media reports that purport to explain where markets are going, trading rules that reliably survive transactions costs and do not implicitly expose the investor to risk have not yet been reliably demonstrated. However, more recently, evidence has accumulated that long horizon excess returns are quite predictable, and to some this evidence indicates that the whole enterprise of economic explanation of asset returns is flawed. To think about this issue, write our basic equation for expected returns as Et Rt 1 Rtf covt mt 1,Rt 1 Et mt 1 t Rt 1 t mt 1, Rt 1 t t ct 1 t Rt 1 t mt 1, Rt 1 , t mt 1 1.22 Et mt 1 where ct 1 denotes consumption growth. I include the t subscripts to emphasize that the relation applies to con ditional moments. Sometimes, the conditional mean or other moment of a random variable is different from its unconditional moment. Conditional on tonight s weather forecast, you can better predict rain tomorrow than just knowing the average rain for that date. In the special case that random vari ables are i.i.d. independent and identically distributed , like coin flips, the conditional and unconditional moments are the same, but that is a special case and not likely to be true of asset prices, returns, and macroeconomic variables. In the theory so far, we have thought of an investor, today, forming expectations of payoffs, consumption, and other variables tomorrow. Thus, the moments are really all conditional, and if we want to be precise we should include some notation to express this fact. I use subscripts Et xt 1 to denote conditionalexpectation;thenotationE xt 1 It whereIt istheinformation set at time t is more precise but a little more cumbersome. Examining equation 1.22 , we see that returns can be somewhat predictable the expected return can vary over time. First, if the conditional variance of returns changes over time, we might expect the conditional mean return to vary as well the return can just move in and out along a line of constant Sharpe ratio. This explanation does not seem to help much in the data; variables that forecast means do not seem to forecast variances and vice versa. Unless we want to probe the conditional correlation, predictable",
        "150 9. Factor Pricing Models In any sensible economic model, as well as in the data, consumption is related to returns on broad based portfolios, to interest rates, to growth in GDP, investment, or other macroeconomic variables, and to returns on real production processes. All of these variables can measure the state of the economy. Furthermore, consumption and marginal utility respond to news: if a change in some variable today signals high income in the future, then con sumption rises now, by permanent income logic. This fact opens the door to forecasting variables: any variable that forecasts asset returns changes in the investment opportunity set or that forecasts macroeconomic variables is a candidate factor. Variables such as the term premium, dividend price ratio, stock returns, etc., can be defended as pricing factors on this logic. Though they are not direct measures of aggregate good or bad times, they forecast such times. Should factors be unpredictable over time? The answer is, sort of. If there is a constant real interest rate, then marginal utility growth should be unpredictable. Consumption is a random walk in the quadratic utility permanent income model. To see this, look at the first order condition with a constant interest rate, u ct RfEt u ct 1 , or in a more time series notation, u ct 1 u ct Rf 1 t 1, Et t 1 0. The real risk free rate is not constant, but it does not vary a lot, especially compared to asset returns. Measured consumption growth is not exactly unpredictable but it is the least predictable macroeconomic time series, especially if one accounts properly for temporal aggregation consumption data are quarterly averages . Thus, factors that proxy for marginal utility growth, though they do not have to be totally unpredictable, should not be highly predictable. If one chooses highly predictable factors, the model will counterfactually predict large interest rate variation. In practice, this consideration means that you should choose the right units: Use GNP growth rather than level, portfolio returns rather than prices or price dividend ratios, etc. However, unless you want to impose an exactly constant risk free rate, you do not have to filter or prewhiten factors to make them exactly unpredictable. Furthermore, we often apply factor pricing models to excess returns in a way that the conditional mean of the discount factor is not identified, and hence has no effect on the results.",
        "172 9. Factor Pricing Models will thus prefer stocks that do well on such news, hedging the reinvest ment risk. Demanding more of such stocks, investors raise their prices and depress their expected returns for a given market beta. Thus, equilibrium expected returns depend on covariation with news of future returns, as well as covariation with the current market return. The ICAPM remained on the theoretical shelf for 20 years mostly because it took that long to accumulate empirical evidence that returns are, in fact, predictable. Most current theorizing and empirical work, while citing the ICAPM, really considers another source of additional risk factors: Investors have jobs. Or they own houses and shares of small businesses. The CAPM and ICAPM simplify matters by assuming pure retired investors who sit on a pile of wealth, all invested in stocks or bonds. Alternatively, these models assume leisure and consumption are separable and that all sources of income includ ing labor income correspond to traded securities. For this reason, the only risk in the CAPM is the market return, and the only state variables in the ICAPM are those that forecast future market returns. People with jobs will prefer stocks that don t fall in recessions. Demand ing such stocks, they drive up prices and drive down expected returns. Thus, expected returns may depend on additional betas that capture labor market conditions, house values, fortunes of small businesses, or other non marketed assets. Yet these state variables need not forecast returns on any traded assets this is not the ICAPM. Much current empirical work seems to be headed towards additional state variables of this type for distress, recession, etc. However, I know of no famous paper or name to cite for this idea, perhaps because at this point its theoretical content is so obvious. It is crucial that the extra factors affect the average investor. If an event makes investor A worse off and investor B better off, then investor A buys assets that do well when the event happens, and investor B sells them. They transfer the risk of the event, but the price or expected return of the asset is unaffected. For a factor to affect prices or expected returns, the average investor must be affected by it. We should expect many factors, common movements in returns, that do not carry risk prices. Industry portfolios seem to be an example; industries move together but average returns do not vary by industry once you control for other betas. As you can see, this traditional intuition is encompassed by consump tion, or marginal utility more generally. Bad labor market outcomes or bad news about future returns are bad news that raise the marginal utility of wealth, which equals the marginal utility of consumption. The promise of the consumption based model was that it would give us a single indi cator that captures all of these general equilibrium determinants. It still does, theoretically, but not yet in empirical practice.",
        "This part surveys some of the empirical issues that are changing our theoretical understanding of the nature of risk and risk premia. This part draws heavily on two previous review articles, Cochrane 1997 and 1999a and on Cochrane and Hansen 1992 . Fama s 1970 and 1991 efficient market reviews are classic and detailed reviews of much of the underlying empirical literature, focusing on cross sectional questions. Campbell 1999, 2000 and Kocherlakota 1996 are good surveys of the equity premium literature. 387",
        "21 Equity Premium Puzzle and Consumption Based Models The original specification of the consumption based model was not a great success, as we saw in Chapter 2. Still, it is in some sense the only model we have. The central task of financial economics is to figure out what are the real risks that drive asset prices and expected returns. Something like the consumption based model investors first order conditions for savings and portfolio choice has to be the starting point. Rather than dream up models, test them, and reject them, financial economists since the work of Mehra and Prescott 1985 and Hansen and Jagannathan 1991 have been able to work backwards to some extent, char acterizing the properties that discount factors must have in order to explain asset return data. Among other things, we learned that the discount factor had to be extremely volatile, while not too conditionally volatile; the risk free rate or conditional mean had to be pretty steady. This knowledge is now lead ing to a much more successful set of variations on the consumption based model. 455",
        "456 21. Equity Premium Puzzle and Consumption Based Models 21.1 Equity Premium Puzzles The Basic Equity Premium Risk Free Rate Puzzle The postwar U.S. market Sharpe ratio is about 0.5 an 8 return and 16 standard deviation. The basic Hansen Jagannathan bound E Re  m  c  Re E m implies  m 50 on an annual basis, requiring huge risk aversion or consumption growth volatility. The average risk free rate is about 1 , so E m 0.99. High risk aversion with power utility implies a very high risk free rate, or requires a negative subjective discount factor. Interest rates are quite stable over time and across countries, so Et m varies little. High risk aversion with power utility implies that interest rates are very volatile. In Chapter 1, we derived the basic Hansen Jagannathan 1991 bounds. These are characterizations of the discount factors that price a given set of asset returns. Manipulating 0 E mR e , we found  m E Re . 21.1 time separable utility implies  c where  cu u is the local curvature of the utility function, and risk aversion coefficient for the power case. Equity Premium Puzzle The postwar mean value weighted NYSE is about 8 per year over the T bill rate, with a standard deviation of about 16 . Thus, the market Sharpe ratio E R e  R e is about 0.5 for an annual investment horizon. If there were a constant risk free rate, E m 1 Rf would nail down E m . The T bill rate is not very risky, so E m is not far from the inverse of the mean T bill rate, or about E m 0.99. Thus, these basic E m  Re In continuous time, or as an approximation in discrete time, we found that E Re  Re , 21.2",
        "21.1. Equity Premium Puzzles 457 facts about the mean and variance of stocks and bonds imply  m 0.5. The volatility of the discount factor must be about 50 of its level in annual data! Per capita consumption growth has standard deviation about 1 per year. With log utility, that implies  m 0.01 1 which is off by a factor of 50. To match the equity premium we need  50, which seems a huge level of risk aversion. Equivalently, a log utility investor with consumption growth of 1 and facing a 0.5 Sharpe ratio should be investing dramatically more in the stock market, borrowing to do so. He should invest so much that his wealth and hence consumption growth does vary by 50 each year. Correlation Puzzle The bound takes the extreme possibility that consumption and stock returns are perfectly correlated. They are not, in the data. Correlations are hard to measure, since they are sensitive to data definition, timing, time aggregation, and so forth. Still, the correlation of annual stock returns and nondurable plus services consumption growth in postwar U.S. data is no more than about 0.2. If we use this information as well if we characterize the mean and standard deviation of all discount factors that have correlation less than 0.2 with the market return, the calculation becomes  m 1 E Re 1 0.5 2.5 0.2 E m m,Re  Re with  m  c ; we now need a risk aversion coefficient of 250! Here is a classier way to state the correlation puzzle. Remember that proj m X should price assets just as well as m itself. Now, m proj m X  and 2 m 2 proj m X 2  . Some of the early resolutions of the equity premium puzzle ended up adding noise uncorrelated with asset payoffs to the discount factor. This modification increased discount factor volatility and satisfied the bound. But as you can see, adding  increases  2 m with no effect whatsoever on the model s ability to price assets. As you add , the correlation between m and asset returns declines. A bound with correlation, or equivalently comparing 2 proj m X rather than 2 m to the bound, avoids this trap. Average Interest Rates and Subjective Discount Factors It has been traditional to use risk aversion numbers of 1 to 5 or so, but perhaps this is tradition, not fact. What is wrong with  50 to 250? The most basic piece of evidence for low  comes from the relation between consumption growth and interest rates: 1 Et mt 1 Et  Ct 1  Rf Ct t",
        "458 21. Equity Premium Puzzle and Consumption Based Models or, in continuous time, rf  E c 1  1 2 c . 21.3 tt2t Real interest rates are typically quite low, about 1 . However, with a 1 mean and 1 standard deviation of consumption growth, the predicted interestraterisesquicklyasweraise.Forexample,with 50andatypical 1  0.01,wepredictrf 0.01 50 0.01 1 50 51 0.012 0.38 2 or 38 . To get a reasonable 1 real interest rate, we have to use a subjective discount factor of negative 37 . That is not impossible an economic model can be well specified, and in particular present values can converge, with negative discount rates Kocherlakota 1990 but it doesn t seem very reasonable. People prefer earlier utility. The second term in 21.3 opens another possibility. As risk aversion increases, this precautionary saving term starts to offset the first, intertem poral substitution term. At an extreme value of risk aversion,  199 still using E c 0.01, c 0.01 , they exactly offset, leaving rf . The discrete time formula behaves similarly, though at a somewhat different very high value of  . Interest Rate Variation and the Conditional Mean of the Discount Factor Again, however, maybe we are being too doctrinaire. What evidence is there against 50with 0.38or 199with 0.01? Real interest rates are not only low on average, they are also relatively stable over time and across countries.  50 in equation 21.3 means that a country or a boom time with consumption growth 1 percentage point higher than normal must have real interest rates 50 percentage points higher than normal, and consumption 1 percentage point lower than normal should be accompanied by real interest rates of 50 percentage points lower than normal you pay them 48 to keep your money. We do not see anything like this.  50 to 250 in a time separable utility function implies that consumers are essentially unwilling to substitute expected consumption over time, so huge interest rate variation must force them to make the small variations in consumption growth that we do see. This level of aversion to intertemporal substitution is too large. For example, think about what interest rate you need to convince someone to skip a vacation. Take a family with 50,000 per year consumption, which spends 2,500 5 on an annual vacation. If interest rates are good enough, though, the family can be persuaded to skip this year s vacation and go on a much more lavish vacation next year. The required interest rate is 52,500 47,500  1. For  250, that is an interest rate of 3 1011! For  50, we still need an interest rate of 14,800 . I think most of us would give in and defer the vacation",
        "21.1. Equity Premium Puzzles 459 for somewhat lower interest rates! A reasonable willingness to substitute intertemporally is central to most macroeconomic models that try to capture the dynamics of output, investment, consumption, etc. As always, we can express the observation as a desired characteristic of the discount factor. Though mt 1 must vary a lot, its conditional mean E m 1 R f must not vary much. You can get variance in two ways tt 1 t variance in the conditional mean and variance in the unexpected compo nent; var x var Et x var x Et x . The fact that interest rates are stable means that almost all of the 50 or more unconditional discount factor variance must come from the second term. The power functional form is really not an issue. To get past the equity premium and these related puzzles, we will have to introduce other arguments to the marginal utility function some nonseparability. One important key will be to introduce some nonseparability that distinguishes intertemporal substitution from risk aversion. Variations Is the Interest Rate Too Low ? A large literature has tried to explain the equity premium puzzle by intro ducing frictions that make treasury bills money like and so argues that the short term interest rate is artificially low. Aiyagari and Gertler 1991 is an example . However, high historical Sharpe ratios are pervasive in finan cial markets. Portfolios long small stocks and short big stocks, or long value high book market and short growth stocks, give Sharpe ratios of 0.5 or more as well. Individual Shocks Maybe we should abandon the representative agent assumption. Individual income shocks are not perfectly insured, so individual income and con sumption is much more volatile than aggregate consumption. Furthermore, through most of the sample, only a small portion of the population held any stocks at all. Just raising the interest rate will not help, as all stock portfolios have high Sharpe ratios too. Uninsured individual risk is not an obvious solution. Individual consump tion is not volatile enough to satisfy the bounds, and is less correlated with stock returns than aggregate consumption. The average return in postwar data may overstate the true expected return; a target of 3 4 is not unreasonable.",
        "460 21. Equity Premium Puzzle and Consumption Based Models This line of argument faces a steep uphill battle. The basic pricing equation applies to each investor. Individual income growth may be more volatile than the aggregate, but it is not credible that any individual s consumption growth varies by 50 250 per year! Keep in mind, this is non durable and services consumption and the flow of services from durables, not durables purchases. Furthermore, individual consumption growth is likely to be less cor related with stock returns than is aggregate consumption growth, and the more volatile it is, the less correlated. As a simple example, write individ ual consumption growth equal to aggregate consumption growth plus an idiosyncratic shock, uncorrelated with economywide variables, Hence, ci ca i. ttt cov ci,r cov ca i,r cov ca,r . tt ttt tt As we add more idiosyncratic variation, the correlation of consumption with any aggregate such as stock returns declines in exact proportion. Follow ing correlation puzzle logic, the asset pricing implications are completely unaffected. Luck and a Lower Target One nagging doubt is that a large part of the U.S. postwar average stock return may represent good luck rather than ex ante expected return. First of all, the standard deviation of stock returns is so high that standard errors are surprisingly large. Using the standard formula  T, the standard error of average stock returns in 50 years of data is about 16 50 2.3. This fact means that a two standard error confidence interval for the expected return extends from about 3 to about 13 ! This is a pervasive, simple, but surprisingly underappreciated prob lem in empirical asset pricing. In 20 years of data, 16 20 3.6 so we can barely say that an 8 average return is above zero. Five year perfor mance averages of something like a stock return are close to meaningless on a statistical basis, since 16 5 7.2. This is one reason that many funds are held to tracking error limits relative to a benchmark. You may be able to measure performance relative to a benchmark, even if your return and the benchmark are both very volatile. If  Ri Rm is small, then  Ri Rm T can be small, even if  Ri and  Rm are large. However, large standard errors can argue that the equity premium is really higher than the postwar return. Several other arguments suggest a bias that a substantial part of the 8 average excess return of the last 50 years was good luck, and that the true equity premium is more like 3 4 .",
        "21.1. Equity Premium Puzzles 461 Brown, Goetzmann, and Ross 1995 suggest that the U.S. data suffer from selection bias. One of the reasons that I write this book in the United States, and that the data has been collected from the United States, is pre cisely because U.S. stock returns and growth have been so good for the last 50 100 years. One way to address this question is to look at other samples. Average returns were a lot lower in the United States before WWII. In Shiller s 1989 annual data from 1871 1940, the S P500 average excess return was only 4.1 . However, Campbell 1999, table 1 looks across countries for which we have stock market data from 1970 1995, and finds the average equity premium practically the same as that for the United States in that period. The other countries averaged a 4.6 excess return while the United States had a 4.4 average excess return in that period. On the other hand, Campbell s countries are Canada, Japan, Australia, and Western Europe. These probably shared a lot of the U.S. good luck in the postwar period. There are lots of countries for which we do not have data, and usually because returns were very low in those countries. As Brown, Goetzmann, and Ross 1995 put it, Looking back over the history of the London or the New York stock markets can be extraordinarily comforting to an investor equities appear to have provided a substantial premium over bonds, and markets appear to have recovered nicely after huge crashes. . . . Less comforting is the past history of other major markets: Russia, China, Germany and Japan. Each of these markets has had one or more major interruptions that prevent their inclusion in long term studies my emphasis . Think of the things that did not happen in the last 50 years. We had no banking panics, and no depressions; no civil wars, no constitutional crises; we did not lose the Cold War, no missiles were fired over Berlin, Cuba, Korea, or Vietnam. If any of these things had happened, we might well have seen a calamitous decline in stock values, and I would not be writing about the equity premium puzzle. A view that stocks are subject to occasional and highly non normal crashes world wars, great depressions, etc. makes sampling uncertainty even larger, and means that the average return from any sample that does not include a crash will be larger than the actual average return the Peso Problem again Reitz 1988 . Fama and French 2000 notice that the price dividend ratio is low at the beginning of the sample and high at the end. Much of that is luck the dividend yield is stationary in the very long run, with slow moving vari ation through good and bad times. We can understand their alternative calculation most easily using the return linearization, rt 1 dt 1 dt pt  dt 1 pt 1 .",
        "462 21. Equity Premium Puzzle and Consumption Based Models Then, imposing the view that the dividend price ratio is stationary, we can estimate the average return as E rt 1 E dt 1 1  E dt pt . The right hand expression gives an estimate of the unconditional average return on stocks equal to 3.4 . This differs from the sample average return of 9 , because the d p ratio declined dramatically in the postwar sample. Here is the fundamental issue: Was it clear to people in 1947 or 1871, or whenever one starts the sample and throughout the period that the aver age return on stocks would be 8 greater than that of bonds, subject only to the 16 year to year variation? Given that knowledge, would investors have changed their portfolios, or would they have stayed pat, patiently explain ing that these average returns are earned in exchange for risk that they are not prepared to take? If people expected these mean returns, then we face a tremendous challenge of explaining why people did not buy more stocks. This is the basic assumption and challenge of the equity premium puzzle. But phrased this way, the answer is not so clear. I do not think it was obvious in 1947 that the United States would not slip back into depres sion, or another world war, but would instead experience a half century of economic growth and stock returns never before seen in human history. Eight percent seems like an extremely maybe even irrationally exuberant expectation for stock returns as of 1947, or 1871. You can ask the same ques tion, by the way, about value effects, market timing, or other puzzles we try to explain. Only if you can reasonably believe that people understood the average returns and shied away because of the risks does it make sense to explain the puzzles by risk rather than luck. Only in that case will the return premia continue! This consideration mitigates, but cannot totally solve, the equity pre mium puzzle. Even a 3 equity premium is tough to understand with 1 consumption volatility. If the premium is 3 , the Sharpe ratio is 3 16 0.2, so we still need risk aversion of 20, and 100 if we include correlation. Twenty to 100 is a lot better than 50 250, but is still quite a challenge. Predictability and the Equity Premium The Sharpe ratio varies over time. This means that discount factor volatil ity must vary over time. Since consumption volatility does not seem to vary over time, this suggests that risk aversion must vary over time a conditional equity premium puzzle.",
        "21.1. Equity Premium Puzzles 463 Conventional portfolio calculations suggest that people are not terri bly risk averse. These calculations implicitly assume that consumption moves proportionally to wealth, and inherits the large wealth volatility. If stock returns mean revert, E Re  Re and hence  m E m rises faster than the square root of the horizon. Consumption growth is roughly i.i.d., so  c rises about with the square root of horizon. Thus, mean reversion means that the equity premium puzzle is even worse for long horizon investors and long horizon returns. We have traced the implications of the unconditional Sharpe ratio, and of low and relatively constant interest rates. The predictability of stock returns also has important implications for discount factors. Heteroskedasticity in the Discount Factor Conditional Equity Premium Puzzle The Hansen Jagannathan bound applies conditionally of course, E Re  m t t 1  Re,m t t 1.  Re tt 1t 1E m t t 1 t t 1 Mean returns are predictable, and the standard deviation of returns varies over time. So far, however, the two moments are forecasted by different sets of variables and at different horizons d p, term premium, etc. forecast the mean at long horizons; past squared returns and implied volatility fore cast the variance at shorter horizons and these variables move at different times. Hence, it seems that the conditional Sharpe ratio on the left hand side moves over time. Glosten, Jagannathan, and Runkle 1993 , French, Schwert, and Stambaugh 1987 , and Yan 2000 find some co movements in conditional mean and variance, but do not find that all movement in one moment is matched by movement in the other. On the right hand side, the conditional mean discount factor equals the risk free rate and so must be relatively stable over time. Time varying conditional correlations are a possibility, but hard to interpret. Thus, the predictability of returns strongly suggests that the discount factor must be conditionally heteroskedastic t mt 1 must vary through time. Certainly the discount factors on the volatility bound, or the mimicking portfolios for discount factors must have time varying volatility, since both of them have  1. In the standard time separable model, t mt 1 tt ct 1 . Thus, we need either time varying consumption risk or time varying curvature; loosely speaking, a time varying risk aversion. The data do not show much evidence of conditional heteroskedasticity in consumption growth, leading one to favor a time varying curvature. However, this is a case in which high curvature helps: if  is sufficiently high, a small and perhaps statistically",
        "464 21. Equity Premium Puzzle and Consumption Based Models hard to measure amount of consumption heteroskedasticity can generate a lot of discount factor heteroskedasticity. Kandel and Stambaugh 1990 follow this approach to explain predictability. CAPM, Portfolios, and Consumption The equity premium puzzle is centrally about the smoothness of consump tion. This is why it was not noticed as a major puzzle in the early development of financial theory. In turn, the smoothness of consumption is centrally related to the predictability of returns. In standard portfolio analyses, there is no puzzle that people with nor mal levels of risk aversion do not want to hold far more stocks. From the usual first order condition and with VW W , we can also write the Hansen Jagannathan bound in terms of wealth, analogously to 21.2 , E r rf  r VW WVWW  w . 21.4 Thequantity WWWW VW isinfactthemeasureofriskaversioncorrespond ing to most survey and introspection evidence, since it represents aversion to bets on wealth rather than to bets on consumption. For an investor who holds the market portfolio of stocks,  w is the standard deviation of that return, about 16 . With a market Sharpe ratio of 0.5, we find the lower bound on risk aversion, WVWW 0.5 3. VW 0.16 Furthermore, the correlation between wealth and the stock market is one in this calculation, so no correlation puzzle crops up to raise the required risk aversion. This is the heart of the oft cited Friend and Blume 1975 calculation of risk aversion, one source of the idea that 3 5 is about the right level of risk aversion rather than 50 or 250. The Achilles heel is the hidden simplifying assumption that returns are independent over time, and the investor has no other source of income, so no variables other than wealth show up in its marginal value VW . In such an i.i.d. world, consumption moves one for one with wealth, and  c  w . If your wealth doubles and nothing else has changed, you double consumption. This calculation thus hides a consumption based model, and the model has the drastically counterfactual implication that consumption growth has a 16 standard deviation! All this calculation has done is say that in a model in which consump tion has a 16 volatility like stock returns, we do not need high risk aversion to explain the equity premium. Hence the central point the equity pre mium is about consumption smoothness. Just looking at wealth and portfolios, you do not notice anything unusual.",
        "21.2. New Models 465 In the same way, retreating to the CAPM or factor models does not solve the puzzle either. The CAPM is a specialization of the consumption based model, not an alternative to it, and thus hides an equity premium puzzle. Most implementations of the CAPM take the market premium as given, ignoring the link to consumption in the model s derivation, and estimate the market premium as a free parameter. The equity premium puzzle asks whether the market premium itself makes any sense. The Long Run Equity Premium Puzzle The fact that annual consumption is much smoother than wealth is an important piece of information. In the long run, consumption must move one for one with wealth, so consumption and wealth volatility must be the same. Therefore, we know that the world is very far from i.i.d., so predictability will be an important issue in understanding risk premia. Predictability can imply mean reversion and Sharpe ratios that rise faster than the square root of horizon. Thus, ERe m t t k t t k  ct t k.  Rte t k E mt t k If stocks do mean revert, then discount factor volatility must increase faster than the square root of the horizon. Consumption growth is close to i.i.d., so the volatility of consumption growth only increases with the square root of horizon. Thus mean reversion implies that the equity premium puzzle is even worse at long investment horizons. 21.2 New Models We want to end up with a model that explains a high market Sharpe ratio, and the high level and volatility of stock returns, with low and relatively con stant interest rates, roughly i.i.d. consumption growth with small volatility, and that explains the predictability of excess returns the fact that high prices today correspond to low excess returns in the future. Eventually, we would like the model to explain the predictability of bond and foreign exchange returns as well, the time varying volatility of stock returns and the cross sectional variation of expected returns, and it would be nice if in addi tion to fitting all of the facts, people in the models did not display unusually high aversion to wealth bets. I start with a general outline of the features shared by most models that address these puzzles. Then, I focus on two models, the Campbell Cochrane 1999 habit persistence model and the Constantinides and Duffie 1996 model with uninsured idiosyncratic risks. The mechanisms we uncover in",
        "466 21. Equity Premium Puzzle and Consumption Based Models these models apply to a large class. The Campbell Cochrane model is a representative from the literature that attacks the equity premium by modi fying the representative agent s preferences. The Constantinides and Duffie model is a representative of the literature that attacks the equity premium by modeling uninsured idiosyncratic risks, market frictions, and limited participation. Outlines of New Models Additional state variables are the natural route to solving the empirical puz zles. Investors must not be particularly scared of the wealth or consumption effects of holding stocks, but of the fact that stocks do badly at particular times, or in particular states of nature. Broadly speaking, most solutions introduce something like a recession state variable. This fact makes stocks different, and more feared, than pure wealth bets, whose risk is unrelated to the state of the economy. In the ICAPM view, we get models of this sort by specifying things so there is an additional recession state variable z in the value function V W , z . Then, expected returns are u C,z , so E r rf CuCC uC cov C,r zuCz uC cov z,r . 21.6 WVWW VW zVWz VW E r rf In a utility framework, we add other arguments to the utility function cov W,r cov z,r . 21.5 The extra utility function arguments must enter nonseparably. If u C,z f C g z , then uCz 0. All utility function modifications are of this sort they add extra goods like leisure, nonseparability over time in the form of habit persistence, or nonseparability across states of nature so that consumption if it rains affects marginal utility if it shines. The lesson of the equity premium literature is that the second term must account for essentially all of the market premium. Since the cross sectional work surveyed in Chapter 20 seemed to point to something like a recession factor as the primary determinant of cross sectional vari ation in expected returns, and since the time series work pointed to a recession related time varying risk premium, a gratifying unity seems close at hand and a fundamental revision of the CAPM i.i.d. view of the source of risk prices. The predictability of returns suggests a natural source of state variables. Unfortunately, the sign is wrong. The fact that stocks go up when their",
        "21.2. New Models 467 expected subsequent returns are low means that stocks, like bonds, are good hedges for shocks to their own opportunity sets. Therefore, adding the effects of predictability typically lowers expected returns. The typically in this sentence is important. The sign of this effect the sign of zVWz does depend on the utility function and environment. For example, there is no risk premium for log utility. Thus, we need an additional state variable, and one strong enough to not only explain the equity premium, given that the first terms in 21.5 and 21.6 are not up to the job, but one stronger still to overcome the effects of predictability. Recessions are times of low prices and high expected returns. We want a model in which recessions are bad times, so that investors fear bad stock returns in recessions. But high expected returns are good times for a pure Merton investor. Thus, the other state variable s that describe a recession high risk aversion, low labor income, high labor income uncer tainty, liquidity, etc. must overcome the good times of high expected returns and indicate that times really are bad after all. Habits A natural explanation for the predictability of returns from price dividend ratios is that people get less risk averse as consumption and wealth increase in a boom, and more risk averse as consumption and wealth decrease in a recession. We cannot tie risk aversion to the level of consumption and wealth, since that increases over time while equity premia have not declined. Thus, to pursue this idea, we must specify a model in which risk aversion depends on the level of consumption or wealth relative to some trend or the recent past. Following this idea, Campbell and Cochrane 1999 specify that people slowly develop habits for higher or lower consumption. Thus, the habits form the trend in consumption. The idea is not implausible. Anyone who has had a large pizza dinner or smoked a cigarette knows that what you consumed yesterday can have an impact on how you feel about more consumption today. Might a similar mechanism apply for consumption in general and at a longer time horizon? Perhaps we get used to an accustomed standard of living, so a fall in consumption hurts after a few years of good times, even though the same level of consumption might have seemed very pleasant if it arrived after years of bad times. This thought can at least explain the perception that recessions are awful events, even though a recession year may be just the second or third best year in human history rather than the absolute best. Law, custom, and social insurance also insure against falls in consumption as much as low levels of consumption. We specify an external, or keep up with the Joneses form of habit formation, following Abel 1990 . In the model, this is primarily a technical",
        "468 21. Equity Premium Puzzle and Consumption Based Models convenience, and we argue that it does not make much difference to the results for aggregate consumption and asset prices. See problem 2. It does seem to capture much interesting behavior, however. Many investors seem more concerned about staying ahead of their colleagues that they are in absolute performance. They demand low tracking error of their investments, meaning that they give up average return opportunities such as value to make sure that their investments do not fall behind as the market rises. We also argue that this specification may be crucial to reconcile strong habits in the aggregate with microeconomic data. Given a windfall, most people spend it quickly. This behavior is consistent with an internal habit, but if each person s habit were driven by his own consumption, consumption would ramp up slowly following a windfall. The Model We model an endowment economy with i.i.d. consumption growth: ct 1 g vt 1, vt 1 i.i.d.N 0,2 . We replace the utility function u C with u C X , where X denotes the level of habits: 1  Habits should move slowly in response to consumption, something like Ct Xt 1  1 Et . or, equivalently, t 0 xt  xt xt 1 ct. 21.8 j 0 jct j 21.7 Small letters denote the logs of large letters throughout this section; ct lnCt, etc. Rather than letting habit itself follow an AR 1 , we let the surplus consumption ratio of consumption to habit follow an AR 1 : St Ct Xt , Ct st 1 1  s st  st ct 1 ct g . 21.9 Since s contains c and x, this equation also specifies how x responds to c, and it is locally the same as 21.7 . We also allow consumption to affect habit",
        "470 21. Equity Premium Puzzle and Consumption Based Models Means and standard deviations of simulated and historical data Consumption Dividend Postwar claim claim data 0.50 0.50 6.64 6.52 6.69 15.2 20.0 15.7 18.3 18.7 24.7 0.27 0.29 0.26 Table 21.1. Statistic E R R  R R E r rf  r r f exp E p d  p d The model is simulated at a monthly frequency; statistics are calculated from artificial time averaged data at an annual frequency. Asterisks denote statis tics that model parameters were chosen to replicate. All returns are annual percentages. Equity Premium and Predictability We choose parameters, simulate 100,000 artificial data points, and report standard statistics and tests in artificial data. The parameters g 1.89,  1.50, r f 0.94 match their values in postwar data. The parameter  0.87 matches the autocorrelation of the price dividend ratio and the choice  2.00 matches the postwar Sharpe ratio.  0.89, S 0.057 follow from the model. Table 21.1 presents means and standard deviations predicted by the model. The model replicates the postwar Sharpe ratio, with a constant 0.94 risk free rate and a reasonable subjective discount factor  1. Of course, we picked the parameters to do this, but given the equity premium discussion it is already an achievement that we are able to pick any parameters to hit these moments. Some models can replicate the Sharpe ratio, but do not replicate the level of expected returns and return volatility. E 1 and  2 will give an 0.5 Sharpe ratio. This model predicts the right levels as well. The model also gets the level of the price dividend ratio about right. Table 21.2 shows how the artificial data match the predictability of returns from price dividend ratios. The paper goes on, and shows how the model matches the volatility test result that almost all return variation is due to variation in expected excess returns, the leverage effect of higher volatility after a big price decline, and several related phenomena. How Does It Work? How does this model get around all the equity premium risk free rate difficulties described above, and explain predictability as well?",
        "21.2. New Models Horizon Years 1 2 3 5 7 471 Table 21.2. Long horizon return regressions Consumption claim 10 coef. R2 2.0 0.13 3.7 0.23 5.1 0.32 7.5 0.46 9.4 0.55 Postwar data 10 coef. R2 2.6 0.18 4.3 0.27 5.4 0.37 9.0 0.55 12.1 0.65 When a consumer has a habit, local curvature depends on how far consumption is above the habit, as well as the power  , Ct ucc Ct Xt  t . uc Ct Xt St As consumption falls toward habit, people become much less willing to tolerate further falls in consumption; they become very risk averse. Thus a low power coefficient  can still mean a high, and time varying curvature. Recall our fundamental equation for the Sharpe ratio, E r r f t r High curvature t means that the model can explain the equity premium, and curvature t that varies over time as consumption rises in booms and falls toward habit in recessions means that the model can explain a time varying and countercyclical high in recessions, low in booms Sharpe ratio, despite constant consumption volatility t c and correlation corrt c,r . So far so good, but did we not just learn that raising curvature implies high and time varying interest rates? This model gets around interest rate problems with precautionary saving. Suppose we are in a bad time, in which consumption is low relative to habit. People want to borrow against future, higher, consumption, and this force should drive up interest rates. In fact, many habit models have very volatile interest rates. However, people are also much more risk averse when consumption is low. This consideration induces them to save more, in order to build up assets against the event that tomorrow might be even worse. This precautionary desire to save drives down interest rates. Our  s specification makes these two forces exactly offset, leading to constant real rates. t t tt c corrt c,r .",
        "472 21. Equity Premium Puzzle and Consumption Based Models The precautionary saving motive also makes the model more plausibly consistent with variation in consumption growth across time and countries. Adding 21.11 to 21.12 , we can write The power coefficient  2 controls the relation between consumption growth and interest rates, while the curvature coefficient  St controls the risk premium. Thus this habit model allows high risk aversion with low aversion to intertemporal substitution, and it is consistent with the consumption and interest rate data. As advertised, this model explains the equity premium and predictabil ity by fundamentally changing the story for why consumers are afraid of holding stocks. The k period stochastic discount factor is Mt t k k St k Ct k St Ct  While Ct k Ct the volatility of Ct k Ct  rf  g 12 2S 2. Covariances with S shocks now drive average returns as well as covariances with C shocks. S C X C is a recession indicator it is low after several quarters of consumption declines and high in booms. and St k St enter symmetrically in the formula,  with  2 is so low that it accounts for essen tially no risk premia. Therefore, it must be true, and it is, that variation in  St k St At short horizons, shocks to St 1 and Ct 1 move together, so the dis tinction between a recession state variable and consumption risk is minor; one can regard S as an amplification mechanism for consumption risks in marginal utility. dS C 50, so this amplification generates the required volatility of the discount factor. At long horizons, however, St k becomes less and less conditionally correlated with Ct k . St k depends on Ct k relative to its recent past, but the overall level of consumption may be high or low. Therefore, investors fear stocks because they do badly in occasional serious recessions, times of recent belt tightening. These risks are at the long run unrelated to the risks of long run average consumption growth. is much larger, and accounts for nearly all risk premia. In the Merton language of 21.5 and 21.6 , variation across assets in expected returns is driven by variation across assets in covariances with recessions far more than by variation across assets in covariances with consumption growth.  .",
        "21.2. New Models 473 As another way to digest how this model works, we can substitute in the s process from 21.9 and write the marginal rate of substitution as Mt 1  St 1Ct 1 , lnMt 1 ln  st 1 st  ct 1 ct  St Ct ln  1  s 1 st gst st 1ct 1 ct a b st d st ct 1 ct . Up to the question of logs versus levels, this is a scaled factor model of the form we studied in Chapter 8. It still is a consumption based model, but the sensitivity of the discount factor to consumption changes over time. The long run equity premium is even more of a puzzle. Most recession state variables, such as GDP growth, labor, and instruments for time varying expected returns shifts in the investment opportunity set , are station ary. Hence, the standard deviation of their growth rates eventually stops growing with horizon. At a long enough horizon, the standard deviation of the discount factor is dominated by the standard deviation of the consump tion growth term, and we return to the equity premium puzzle at a long enough run. Since this model produces predictability of the right sign, it produces a long run equity premium puzzle. How it manages this feat with a stationary state variable St is subtle and we did not notice it until the penultimate  draft! . The answer is that while St is stationary, St is not. St has a fat tail  approaching zero so the conditional variance of St k grows without bound.  This model does have high risk aversion. The utility curvature and value function curvature are both high. Many authors require that a solution of the equity premium puzzle display low risk aversion. This is a laudable goal, and no current model has attained it. No current model generates the equity premium with a low and relatively constant interest rate, low risk aversion, and the right pattern of predictability high prices forecast low returns, not high returns, and consumption is roughly a random walk. Constantinides 1990 and Boldrin, Christiano, and Fisher 2001 are habit models with a large equity premium and low risk aversion, but they do not get the pattern of predictability right. Boldrin, Christiano, and Fisher have highly variable While the distinction between stationary S and nonstationary S initially minor, it is in fact central. Any model that wishes to explain the equity premium at long and short runs by means of an additional, stationary state variable must find some similar transformation so that the volatility of the stochastic discount factor remains high at long horizons. seems",
        "474 21. Equity Premium Puzzle and Consumption Based Models interest rates to keep consumption from being predictable. Constantinides 1990 has a constant interest rate, but consumption growth that is serially correlated, so consumption rises to meet i.i.d. wealth growth. The long run equity premium is solved with counterfactually high long run consumption volatility. Heterogeneous Agents and Idiosyncratic Risks A long, increasing, and important literature in the equity premium attacks the problem with relatively standard preferences, but instead adds unin sured idiosyncratic risk. As with the preference literature, this literature is interesting beyond the equity premium. We are learning a lot about who holds stocks and why, what risks they face. We are challenged to think of new assets and creative ways of using existing assets to share risks better. Constantinides and Duffie 1996 provide a very clever and simple model in which idiosyncratic risk can be tailored to generate any pattern of aggregate consumption and asset prices. It can generate the equity premium, predictability, relatively constant interest rates, smooth and unpredictable aggregate consumption growth, and so forth. Furthermore, it requires no transactions costs, borrowing constraints, or other frictions, and the individual consumers can have any nonzero value of risk aversion. Of course, we still have to evaluate whether the idiosyncratic risk process we construct to explain asset pricing phenomena are reasonable and consistent with microeconomic data. A Simple Version of the Model I start with a very simplified version of the Constantinides Duffie model. Each consumer i has power utility, U E Individual consumption growth Cit 1 is determined by an independent, idiosyncratic normal 0,1 shock it , ln Cit 1  y 1y2 , 21.13 C where yt 1 is, by construction since it multiplies the shock it, the cross sectional standard deviation of consumption growth. yt 1 is dated t 1 since it is the cross sectional standard deviation given aggregates at t 1. The aggregates are determined first, and then the shocks it 1 are handed out. t e tC1 . it i,t it 1 t 1 2 t 1",
        "476 21. Equity Premium Puzzle and Consumption Based Models where Ct denotes aggregate consumption and mt is a strictly positive discount factor that prices all assets under consideration, pt Et mt 1xt 1 for all xt 1 X. 21.16 By starting with a discount factor that can price a large collection of assets, where I used the discount factor R 1 to price the single return R in t 1 t 1 21.14 , idiosyncratic risk can be constructed to price exactly a large collec tion of assets. We can exactly match the Sharpe ratio, return forecastability, and other features of the data. Then, they let yt 1 is still the conditional standard deviation of consumption growth, given aggregates returns and aggregate consumption. This variation allows uncertainty in aggregate consumption. We can tailor the idiosyncratic risk to and consumption interest rate facts as well. Following exactly the same argument as before, we can now show that 1 Et e  Cit 1 Cit for all the assets priced by m. A Technical Assumption  ln vit 1  y 1y2 , v it 1 t 1 2 t 1 Cit 1 vit 1Ct 1. it Rt 1 Astute readers will notice the possibility that the square root term in 21.14 and 21.15 might be negative. Constantinides and Duffie rule out this possibility by assuming that the discount factor m satisfies lnmt 1  lnCt 1 21.17 Ct in every state of nature, so that the square root term is positive. We can sometimes construct such discount factors by picking parame tersa,binmt 1 max a b xt 1,e Ct 1 Ct  tosatisfy 21.16 .However, neither this construction nor a discount factor satisfying 21.17 is guaran teed to exist for any set of assets. The restriction 21.17 is a tighter form of the familiar restriction that mt 1 0 that is equivalent to the absence of arbitrage in the assets under consideration. Bernardo and Ledoit 2000 show that the restriction m a is equivalent to restrictions on the maximum",
        "478 21. Equity Premium Puzzle and Consumption Based Models consumption growth. In an evaluation in microeconomic data, this makes us look for sources of permanent shocks. This, at a deeper level, is why idiosyncratic consumption shocks have to be uncorrelated with the market. We can give individuals idiosyncratic income shocks that are correlated with the market. Say, agent A gets more income when the market is high, and agent B gets more income when it is low. But then A will short the market, B will go long, and they will trade away any component of the shock that is correlated with the returns on available assets. I argued in Section 21.1 that this effect made idiosyncratic shocks unlikely candidates to explain the equity premium puzzle. Shocks uncorre lated with asset returns have no effect on asset pricing, and shocks correlated with asset returns are quickly traded away. The only way out is to exploit the nonlinearity of marginal utility. We can give people income shocks that are uncorrelated with returns, so they cannot be traded away. Then we have a nonlinear marginal utility function turn these shocks into marginal utility shocks that are correlated with asset returns, and hence can affect pricing implications. This is why Constantinides and Duffie specify that the variance of idiosyncratic risk rises when the market declines. If marginal utility were linear, an increase in variance would have no effect on the average level of marginal utility. Therefore, Constantinides and Duffie specify power utility, and the interaction of nonlinear marginal utility and changing conditional variance produces an equity premium. As a simple calculation that shows the basic idea, start with individuals i with power utility so Ci  0 E t 1 Re . Cti t 1 Now aggregate across people by summing over i, with EN 1 N N Cti t 1 If the cross sectional variation of consumption growth is lognormally distributed, 0 E e E ci 2 2 ci Re Ci  0 EE t 1 Re . N t 1 2 N t 1 t 1 As you see, the economy displays more risk aversion than would a repre sentative agent with aggregate consumption ca E c . That risk t 1 N it 1 aversioncanalsovaryovertimeifN variesovertime,andthisvariationcan generate risk premia. . N , i 1",
        "21.2. New Models 479 Microeconomic Evaluation and Risk Aversion Like the Campbell Cochrane model, this could be either a new view of stock market and macroeconomic risk, or just a clever existence proof for a heretofore troubling class of models. The first question is whether the microeconomic picture painted by this model is correct, or even plausible. Is idiosyncratic risk large enough? Does idiosyncratic risk really rise when the market falls, and enough to account for the equity premium? Are there enough permanent idiosyncratic shocks? Do people really shy away from stocks because stock returns are low at times of high labor market risk? This model does not change the first puzzle. To get power utility consumers to shun stocks, they still must have tremendously volatile con sumption growth or high risk aversion. The point of this model is to show how consumers can get stuck with high consumption volatility in equilibrium, already a difficult task. More seriously than volatility itself, consumption growth variance also represents the amount by which the distribution of individual consumption and income spreads out over time, since the shocks must be permanent and independent across people. The 50 or larger consumption growth volatility that we require to reconcile the Sharpe ratio with risk aversion of one means that the distribution of consumption and income must also spread out by 50 per year. The distribution of consumption does spread out, but not this much. For example, Deaton and Paxson 1994 report that the cross sectional variance of log consumption within an age cohort rises from about 0.2 at age 20 to 0.6 at age 60. This estimate means that the cross sectional stan dard deviation of consumption rises from 0.2 . 45 or 45 at age 20 to 0.6 . 77 or 77 at age 60. 77 means that an individual one standard deviation better off than the mean consumes 77 more than the mean consumer. We are back to about 1 per year. Finally, and most crucially, the cross sectional uncertainty about indi vidual income must not only be large, it must be higher when the market is lower. This risk factor is after all the central element of Constantinides and Duffie s explanation for the market premium. Figure 21.1 shows how the cross sectional standard deviation of consumption growth varies with the market return and risk aversion in my simple version of Constantinides and Duffie s model. If we insist on low  1 to 2 risk aversion, the cross sectional standard deviation of consumption growth must be extremely sensitive to the level of the market return. Looking at the  2 line, for example, is it plausible that a year with 5 market return would show a 10 cross sectional variation in consumption growth, while a mild 5 decline in the market is associated with a 25 cross sectional variation? All of these empirical problems are avoided if we allow high risk aver sion rather than a large risk to drive the equity premium. The  25 line",
        "480 21. Equity Premium Puzzle and Consumption Based Models Figure 21.1. Cross sectional standard deviation of individual consumption growth as a function of the market return in the simple version of the Constantinides Duffie model. The lnCt Ct 1 0.01. in Figure 21.1 looks possible; a  50 line would look even better. With high risk aversion, we do not need to specify highly volatile individual con sumption growth, spreading out of the income distribution, or dramatic sensitivity of the cross sectional variance to the market return. As in any model, a high equity premium must come from a large risk, or from large risk aversion. Labor market risk correlated with the stock market does not seem large enough to account for the equity premium without high risk aversion. The larger set of asset pricing facts has not yet been studied in this model. It is clearly able to generate return predictability, but that requires a pattern of variation in idiosyncratic risk that remains to be characterized and evaluated. It can generate cross sectional patterns such as value premia if value stocks decline at times of higher cross sectional volatility; that too remains to be studied. Summary In the end, the Constantinides Duffie model and the Campbell Cochrane model are quite similar in spirit. First, both models make a similar, funda mental change in the description of stock market risk. Consumers do not fear much the loss of wealth of a bad market return per se. They fear that plotisthevariableyt 2 ln 1  ln Ct .Parametervaluesare 0.05,   1 Rt Ct 1",
        "21.3. Bibliography 481 loss of wealth because it tends to come in recessions, in one case defined as times of heightened idiosyncratic labor market risk, and in the other case defined as a fall of consumption relative to its recent past. This recession state variable or risk factor drives most variation in expected returns. Second, both models require high risk aversion. While Constantinides and Duffie s proof shows that one can dream up a labor income process to rationalize the equity premium for any risk aversion coefficient, we see that even vaguely plausible characterizations of actual labor income uncertainty require high risk aversion to explain the historical equity premium. Third, both models provide long sought demonstrations that it is possi ble to rationalize the equity premium in their respective class of models. This existence proof is particularly stunning in Constantinides and Duffie s case. Many authors myself included had come to the conclusion that the effort to generate an equity premium from idiosyncratic risk was hopeless because any idiosyncratic risk that would affect asset prices would be traded away. 21.3 Bibliography Shiller 1982 made the first calculation that showed either a large risk aversion coefficient or counterfactually large consumption variability was required to explain means and variances of asset returns. Mehra and Prescott 1985 labeled this fact the equity premium puzzle. However, they described these puzzles in the context of a two state Markov model for consumption growth, identifying a stock as a claim to consumption and a risk free bond. Weil 1989 emphasized the interaction between equity pre mium and risk free rate puzzles. Hansen and Jagannathan 1991 sparked the kind of calculations I report here in a simplified manner. Cochrane and Hansen 1992 derived many of the extra discount factor moment restric tions I surveyed here, calculating bounds in each case. Luttmer 1996, 1999 tackled the important extension to transactions costs. Kocherlakota 1996 is a nice summary of equity premium facts and models. Much of the material in this chapter is adapted from a survey in Cochrane 1997 . Campbell 1999 and 2000 are two excellent recent surveys. Ferson 1995 is a nice survey of consumption based model varia tions as well as some of the beta pricing models discussed in the last chapter. The Campbell Cochrane model I presented here is a tip of an iceberg of habit research, including prominent contributions by Constantinides 1990 , Ferson and Constantinides 1991 , Heaton 1995 , and Abel 1990 . Models can be nonseparable across goods as well. Leisure is the most natural extra variable to add to a utility function. It is not clear a priori whether",
        "482 21. Equity Premium Puzzle and Consumption Based Models more leisure enhances the marginal utility of consumption why bother buying a boat if you are at the office all day and cannot use it or vice versa if you have to work all day, it is more important to come home to a really nice big TV . However, we can let the data speak on this matter. Explicit versions of this approach have not been very successful to date Eichenbaum, Hansen, and Singleton 1988 . On the other hand, recent research has found that adding labor income as an extra ad hoc factor can be useful in explaining the cross section of average stock returns, especially if it is scaled by a conditioning variable Jagannathan and Wang 1996 , Reyfman 1997 , Lettau and Ludvigson 2001a . The non state separable utility functions following Epstein and Zin 1989 are a major omission of this presentation. The expectation E in the standard utility function sums over states of nature, e.g., U prob rain u C if it rains prob shine u C if it shines . Separability means one adds across states, so the marginal utility of con sumption in one state is unaffected by what happens in another state. But perhaps the marginal utility of a little more consumption in the sunny state of the world is affected by the level of consumption in the rainy state of the world. Epstein and Zin 1989 , and Hansen, Sargent, and Tallarini 1999 propose recursive utility functions of the form U C1  fEf 1 U . t t t t 1 If f x x, this expression reduces to power utility. These utility functions are not state separable. As with habits, these utility functions distinguish risk aversion from intertemporal substitution one coefficient can be set to capture the consumption interest rate facts, and a completely separate coefficient can be set to capture the equity premium. So far, this style of model as in Epstein and Zin 1989 , Weil 1989 , Kandel and Stambaugh 1991 , and Campbell 1996 does not generate time varying risk aversion, but that modification should not be too difficult, and could lead to a model that works very much like the habit model I surveyed here. Habit persistence is the opposite of durability. If you buy a durable good yesterday, that lowers your marginal utility of an additional purchase today, while buying a habit forming good raises your marginal utility of an additional purchase today. Thus the durability of goods should introduce a non time separability of the form u ct xt , xt f ct 1, ct 2, . . . rather than the habit persistence form u ct  xt . Since goods are durable, and we have a lot of data on durables purchases, it would be good to include both durability and habit persistence in our models. In fact, even nondurables data contain items like clothing; the truly nondurable purchases are such a small fraction of total consumption that we rely on very little data.",
        "21.3. Bibliography 483 One must be careful with the time horizon in such a specification. At a sufficiently small time horizon, all goods are durable. A pizza eaten at noon lowers marginal utility of more pizza at 12:05. Thus, our common continuous time, time separable assumption really cannot be taken liter ally. Hindy and Huang 1992 argue that consumption should be locally substitutable in continuous time models. Heaton 1993 found that at monthly horizons, consumption growth displays the negative autocorrela tion suggestive of durability with constant interest rates, while at longer horizons consumption is nearly unforecastable after accounting for time aggregation. There is also a production first order condition that must be solved, relating asset prices to marginal rates of transformation. The standard here is the q theory of investment, which is based on an adjustment cost. If the stock market is really high, you issue stock and make new investments. The trouble with this view is that f K declines very slowly, so the observed price volatility implies huge investment volatility. The q theory adds adjustment costs to damp the investment volatility. The q theory has had as much trou ble fitting the data as the consumption based model. Cochrane 1991d reports one success when you transform the data to returns high stock returns are associated with high investment growth. The more recent invest ment literature has focused on specifying the adjustment cost problem with asymmetries and irreversibilities, for example, Abel and Eberly 1996 but has not yet been applied to asset pricing puzzles. There is an important literature that puts new utility functions together with production functions, to construct complete explicit economic mod els that replicate the asset pricing facts. Such efforts should also at least preserve if not enhance our ability to understand the broad range of dynamic microeconomic, macroeconomic, international, and growth facts that the standard models were constructed around. Jermann 1998 tried putting habit persistence consumers in a model with a standard technology Y  f K , L from real business cycle models. The easy opportunities for intertemporal transformation provided by that technology meant that the consumers used it to smooth consumption dramatically, destroying the pre diction of a high equity premium. To generate the equity premium, Jermann added an adjustment cost technology, as the production side literature had found necessary. This modification resulted in a high equity premium, but also large variation in risk free rates. Boldrin, Christiano, and Fisher 2001 also added habit persistence preferences to real business cycle models with frictions in the allocation of resources to two sectors. They generate about 1 2 the historical Sharpe ratio. They find some quantity dynamics are improved over the standard model. However, they still predict highly volatile interest rates and persistent consumption growth.",
        "484 21. Equity Premium Puzzle and Consumption Based Models To avoid the implications of highly volatile interest rates, I suspect we will need representations of technology that allow easy transformation across time but not across states of nature, analogous to the need for easy intertem poral substitution but high risk aversion in preferences. Alternatively, the Campbell Cochrane model above already produces the equity premium with constant interest rates, which can be interpreted as a linear production function f K . Models with this kind of precautionary savings motive may not be as severely affected by the presence of inter temporal transformation opportunities in production. Tallarini 1999 uses non state separable preferences similar to those of Epstein and Zin in a general equilibrium model with production. He shows a beautiful observational equivalence result: A model with standard preferences and a model with non state separable preferences can predict the same path of quantity variables output, investment, consumption, etc. but differ dramatically on asset prices. This result offers one explanation of how the real business cycle and growth literature could go on for 25 years examining quantity data in detail and miss all the modifications to prefer ences that we seem to need to explain asset pricing data. It also means that asset price information is crucial to identifying preferences and calculating welfare costs of policy experiments. Finally, it offers hope that adding the deep modifications necessary to explain asset pricing phenomena will not demolish the success of standard models at describing the movements of quantities. The Constantinides and Duffie model has roots in a calculation by Mankiw 1986 that idiosyncratic risk could make the representative con sumer seem more risk averse than the individuals. Work on evaluating the mechanisms in this model in microeconomic data is starting. Heaton and Lucas 1996 calibrate idiosyncratic risk from the PSID, but their model explains at best 1 2 of the sample average stock return, and less still if they allow a net supply of bonds with which people can smooth transitory shocks. More direct tests of these features in microeconomic consumption data are underway, for example Brav, Constantinides, and Geczy 1999 , Storesletten, Telmer, and Yaron 1999 , and Vissing Jorgenson 1999 . Keim and Stambaugh 1986 present a model in which a small amount of time varying consumption volatility and a high risk aversion coefficient generate the large time varying discount factor volatility we need to generate returns predictability. Aiyagari and Gertler 1991 , though aimed at the point that the equity premium might be explained by a too low riskless rate, nonetheless was an important paper in specifying and solving models with unin sured individual risks and transactions costs to keep people from trading them away."
    ],
    "Topic 9": [
        "1.2. Marginal Rate of Substitution Stochastic Discount Factor 7 The term stochastic discount factor refers to the way m generalizes standard discount factor ideas. If there is no uncertainty, we can express prices via the standard present value formula pt 1 xt 1, 1.5 Rf where Rf is the gross risk free rate. 1 Rf is the discount factor. Since gross interest rates are typically greater than one, the payoff xt 1 sells at a discount. Riskier assets have lower prices than equivalent risk free assets, so they are often valued by using risk adjusted discount factors, 1 pi Exi . t Ri t t 1 Here, I have added the i superscript to emphasize that each risky asset i must be discounted by an asset specific risk adjusted discount factor 1 R i . In this context, equation 1.4 is obviously a generalization, and it says something deep: one can incorporate all risk corrections by defining a single stochastic discount factor the same one for each asset and putting it inside the expectation. mt 1 is stochastic or random because it is not known with certainty at time t. The correlation between the random components of the common discount factor m and the asset specific payoff xi generate asset specific risk corrections. mt 1 is also often called the marginal rate of substitution after 1.3 . In that equation, mt 1 is the rate at which the investor is willing to substitute consumption at time t 1 for consumption at time t . mt 1 is sometimes also called the pricing kernel. If you know what a kernel is and you express the expectation as an integral, you can see where the name comes from. It is sometimes called a change of measure or a state price density. For the moment, introducing the discount factor m and breaking the basic pricing equation 1.2 into 1.3 and 1.4 is just a notational conve nience. However, it represents a much deeper and more useful separation. For example, notice that p E mx would still be valid if we changed the utility function, but we would have a different function connecting m to data. All asset pricing models amount to alternative ways of connecting the stochastic discount factor to data. At the same time, we will study lots of alternative expressions of p E mx , and we can summarize many empir ical approaches by applying them to p E mx . By separating our models into these two components, we do not have to redo all that elaboration for each asset pricing model.",
        "26 1. Consumption Based Model and Overview contains a brief introduction to continuous time processes that covers what you need to know for this book. Even if you want to end up with a discrete time representation, manipulations are often easier in continuous time. For example, relating interest rates and Sharpe ratios to consumption growth in the last section required a clumsy lognormal approximation; you will see the same sort of thing done much more cleanly in this section. The choice of discrete versus continuous time is one of modeling con venience. The richness of the theory of continuous time processes often allows you to obtain analytical results that would be unavailable in discrete time. On the other hand, in the complexity of most practical situations, you often end up resorting to numerical simulation of a discretized model anyway. In those cases, it might be clearer to start with a discrete model. But this is all a choice of language. One should become familiar enough with discrete as well as continuous time representations of the same ideas to pick the representation that is most convenient for a particular application. First, we need to think about how to model securities, in place of price pt and one period payoff xt 1. Let a generic security have price pt at any moment in time, and let it pay dividends at the rate Dt . I will continue to denote functions of time as pt rather than p t to maintain continuity with the discrete time treatment, and I will drop the time subscripts where they are obvious, e.g., dp in place of dpt . In an interval dt , the security pays dividendsDt dt.IusecapitalDfordividendstodistinguishthemfromthe differential operator d. The instantaneous total return is dpt Dt dt. pt pt We model the price of risky assets as diffusions, for example, dpt  dt  dz. pt I use the notation dz for increments to a standard Brownian motion, e.g., zt zt N 0, . I use the notation to indicate that the drift and diffusions  and  can be functions of state variables. I limit the discussion to diffusion processes no jumps. What is nice about this diffusion model is that the increments dz are normal. However, the dependence of  and  on state variables means that the finite time distribution of prices f pt It need not be normal. We can think of a risk free security as one that has a constant price equal to 1 and pays the risk free rate as a dividend, p 1 D rf, 1.25 tt",
        "1.5. Discount Factors in Continuous Time 27 or as a security that pays no dividend but whose price climbs deterministically at a rate dpt r f dt. 1.26 pt t Next, we need to express the first order conditions in continuous time. The utility function is t 0 Suppose the investor can buy a security whose price is pt and that pays a dividend stream Dt . As we did in deriving the present value price relation in discrete time, the first order condition for this problem gives us the infinite period version of the basic pricing equation right away,2 U ct E e tu ct dt. ptu ct Et This equation is an obvious continuous time analogue to It turns out that dividing by u ct is not a good idea in continuous time, since the ratio u ct u ct is not well behaved for small time intervals. Instead, we can keep track of the level of marginal utility. Therefore, define the discount factor in continuous time as t e tu ct . Then we can write the pricing equation as 1.28 Some people like to define t u ct , in which case you keep the e t in pt t Et t sDt s ds. s 0 s 0 e su ct s Dt s ds. 1.27 pt Et  Dt j. j u ct j j 1 u ct the equation. Others like to scale t by the risk free rate, in which case you  0 t  get an extra e s r f d  in the equation. The latter procedure makes it look like a risk neutral or present value formula valuation. 2 One unit of the security pays the dividend stream Dt , i.e., Dt dt units of the numeraire consumptiongoodinatimeintervaldt.Thesecuritycostspt unitsoftheconsumptiongood. The investor can finance the purchase of  units of the security by reducing consumption from et to ct et pt dt during time interval dt. The loss in utility from doing so is u ct et ct dt u ct pt . The gain is the right hand side of 1.27 multiplied by .",
        "28 1. Consumption Based Model and Overview The analogue to the one period pricing equation p E mx is 0 Ddt Et d p . 1.29 To derive this fundamental equation, take the difference of equation 1.28 at t and t or, start directly with the first order condition for buying the security at t and selling it at t , t sDt s ds Et t pt . pt t tDt Et t pt . We want to get to d something, so introduce differences by writing pt t tDt Et tpt t pt tpt . Canceling pt t , 0 tDt Et t pt tpt . Taking the limit as 0, 0 tDt dt Et d tpt pt t Et For small the term in the integral can be approximated as s 0 or, dropping time subscripts, equation 1.29 . Equation 1.29 looks different than p E mx because there is no price on the left hand side; we are used to thinking of the one period pricing equation as determining price at t given other things, including price at t 1. But price at t is really here, of course, as you can see from equation 1.30 or 1.31 . It is just easier to express the difference in price over time rather than price today on the left and payoff including price tomorrow on the right. With no dividends and constant , 0 Et dpt Et pt pt says that price should follow a martingale. Thus, Et d p 0 means that marginal utility weighted price should follow a martingale, and 1.29 adjusts for div idends.Thus,itisthesameastheequation 1.21 ,pt Et mt 1 pt 1 dt 1 that we derived in discrete time. Since we will write down price processes for dp and discount factor processes for d , and to interpret 1.29 in terms of expected returns, it is often convenient to break up the d t pt term using Ito s lemma: d p pd dp dpd . 1.32 1.30 1.31",
        "1.5. Discount Factors in Continuous Time 29 Using the expanded version 1.32 in the basic equation 1.29 , and dividing by p to make it pretty, we obtain an equivalent, slightly less compact but slightly more intuitive version, 0 Ddt E d dp d dp . 1.33 ptpp This formula only works when both and p can never be zero. It is often enough the case that this formula is useful. If not, multiply through by and p and keep them in numerators. Applying the basic pricing equations 1.29 or 1.33 to a risk free rate, defined as 1.25 or 1.26 , we obtain rfdt E d t . tt t This equation is the obvious continuous time equivalent to Rf 1. If a risk free rate is not traded, we can use 1.34 to define a shadow risk free rate or zero beta rate. 1.34 t Et mt 1 With this interpretation, we can rearrange equation 1.33 as E dpt Dtdt rfdt E d tdpt . tppttp 1.35 1.36 tttt This is the obvious continuous time analogue to E R Rf Rfcov m,R . The last term in 1.35 is the covariance of the return with the discount factor or marginal utility. Since means are order dt, there is no difference between covariance and second moment in the last term of 1.35 . The interest rate component of the last term of 1.36 naturally vanishes as the time interval gets short. Ito s lemma makes many transformations simple in continuous time. For example, the nonlinear transformation between consumption and the discount factor led us to some tricky approximations in discrete time. This transformation is easy in continuous time diffusions are locally normal, so",
        "52 3. Contingent Claims Markets We can also think of the discount factor m as the derivative or change of measure from the real probabilities  to the subjective probabilities  . The risk neutral probability representation of asset pricing is quite common, especially in derivative pricing where the results are often independent of risk adjustments. The risk neutral representation is particularly popular in continuous time diffusion processes, because we can adjust only the means, leaving the covariances alone. In discrete time, changing the probabilities typically changes both first and second moments. Suppose we start with a process for prices and discount factor dp p dt p dz, p E dp Ddt rf dt E d dp tppt p p D rf  p. p In the risk neutral measure we just increase the drift of each price process by its covariance with the discount factor, and write a risk neutral discount factor, dp p p dt p dz p dt p dz, p d  dt  dz, and suppose that the discount factor prices the assets, d  dt. Under this new set of probabilities, we can just write a risk neutral pricing formula E dp Ddt rfdt 0 t with E dp p p dt. As before, t p p D rf 0. p pp",
        "74 4. The Discount Factor mathematics. If we reinvest dividends for simplicity, then a discount factor must satisfy pt t Et t spt s . Calling pt s xt s , this is precisely the discrete time p E mx that we have studied all along. Thus, the law of one price or absence of arbitrage are equivalent to the existence of a positive t s . The same conditions at all horizons s are thus equivalent to the existence of a discount factor process, orapositivediscountfactorprocess t foralltimet. For calculations it is useful to find explicit formulas for a discount factors. Suppose a set of securities pays dividends and their prices follow Dt dt dpt tdt tdzt, pt wherepandzareN 1vectors,t andt mayvaryovertime, pt,t,other variables , E dz dz Idt, and the division on the left hand side is element tt by element. As usual, I will drop the t subscripts when not necessary for clarity, but everything can vary over time. We can form a discount factor that prices these assets from a linear combination of the shocks that drive the original assets, and d rfdt  rf 1dz, d D p 4.3 where  again is the covariance matrix of returns. You can easily check that this equation solves E dp Ddt rfdt E d dp tpp t p erties. If there is a risk free rate r f also potentially time varying , then that t rate determines r f . If there is no risk free rate, 4.3 will price the risky assets t for any arbitrary or convenient choice of r f . As usual, this discount factor t is not unique; plus orthogonal noise will also act as a discount factor: r f dt, or you can show that this is the only diffusion driven by dz, dt with these prop d d dw; E dw 0; E dzdw 0. Et 4.4",
        "Problems 75 You can see that 4.3 is exactly analogous to the discrete time formula 4.2 . If you do not like answers that pop out of hats, guess a solution of the form d Problems Chapter 4 1. Does the absence of arbitrage imply the law of one price? Does the law of one price imply the absence of arbitrage? 2. If the law of one price or absence of arbitrage hold in population, must they hold in a sample drawn from that population? 3. This problem shows that the growth optimal portfolio introduced in problem 9, Chapter 1 can also serve as a discount factor. a Suppose you have a single return R. x R E R2 is one dis count factor. What about R 1? Certainly E R 1R 1, so what about the theorem that x is unique? Is R 1 always positive? b Let R denote a N 1 vector of asset returns. Show that the portfolio that solves max E ln  R s.t.  1 1 is also a discount factor. Is this discount factor always positive? Is it in the payoff space? Can you find a formula for ? c Find the continuous time counterpart to the discount factor of part b.  dt  dz. Then find  and  to satisfy 4.4 for the risk free and risky assets.",
        "9.1. Capital Asset Pricing Model CAPM 159 Instead solve 9.8 as a functional equation. Guess that the value func tion V Wt 1 is quadratic, with some unknown parameters. Then use the recursive definition of V Wt in 9.8 , and solve a two period problem find the optimal consumption choice, plug it into 9.8 , and calculate the value function V Wt . If the guess was right, you obtain a quadratic function for V Wt , and determine any free parameters. Guess 1 2 u ct ct c . 2  V Wt 1 Wt 1 W 2 2 Let us do it. Specify with  and W parameters to be determined later. Then the problem 9.8 is I do not write the portfolio choice w part for simplicity; it does not change anything 1 V Wt max ct c 2  E Wt 1 W 2 ct 2 2 s.t.W RW W c . t 1 t 1 t t Et is now E since I assumed i.i.d. Substituting the constraint into the objective, V W max c c 2  E RW W c W . 9.9 1  2 t ct 2t 2 t 1 t t The first order condition with respect to ct , using c to denote the optimal value, is Solving for ct , c c   E R W W c W R W . t t 1ttt 1 c c E RW2W cRW2 W RW , t t 1t tt 1 t 1 c 1 E RW2 c E RW2 W W E RW , t t 1 t 1 t t 1 c E RW W E RW2 W t 1 t 1 t. ct 9.10 This is a linear function of Wt . Writing 9.9 in terms of the optimal value of c, we get 1 E RW2 t 1 1  2 V W c c 2  ERW W c W . 9.11 t2t 2t 1tt",
        "9.1. Capital Asset Pricing Model CAPM 163 With log utility, we have u ct 1 ct , so again the value of the market portfolio is exactly proportional to consumption, pW 1 t e sds . ct0  The discount factor is proportional to the inverse of the value of the market portfolio, so or, for a short discrete time interval, E Ri Rf cov Ri ,RW t e t t eu ct e t ptW ct t dt t t . dpW ttt d Now, the basic pricing equation in continuous time for asset i reads 1 dpW 2 pW 2 pW 2 f dpti dptW dpti Et pi Dti dt rt dt Et ttt t t 1 t t t 1 t 1 pi pit pWt  varRW i,W;t t t  W i,W;t t 9.13 Working backwards, equation 9.13 corresponds to a discrete time discount factor that is a linear function of the market return. Normal distributions in discrete time: Stein s lemma The essence of the continuous time approximation is that diffusion pro cesses are locally normally distributed. If we assume that returns are normally distributed in discrete time, we can make the linearization exact in discrete time. Again, the point of the linearization is to give us an expected return beta model with betas calculated against the factors themselves rather than non linear functions of the factors. We need a way to transform from cov g f , R to cov f , R . The central mathematical trick is Stein s lemma: Lemma: If f ,R are bivariate normal, g f is differentiable and E g f , then cov g f ,R E g f cov f ,R . At this point, we re really done. We can substitute covariances and betas with the nonlinear function g f with covariances and betas with f itself.",
        "We value bonds and options with closely related techniques. As you might expect, I present both applications in a discount factor context. Bonds and options are priced with surprisingly simple discount factors. So far, we have focused on returns, which reduce the pricing problem to a one period problem. Bonds and options force us to start thinking about chaining together the one period or instantaneous representations to get a prediction for prices of long lived securities. Taking this step is very impor tant, and I forecast that we will see much more multiperiod analysis in stocks as well, studying prices and streams of payoffs rather than returns. This step rather than the discount factor accounts for the mathematical complexity of some bond and option pricing models. There are two standard ways to go from instantaneous or return rep resentations to prices. First, we can chain the discount factors together. Starting with a one period discount factor mt,t 1, we can find a long term discountfactormt,t j mt,t 1mt 1,t 2...mt j 1,t j thatcanpriceaj period payoff. Starting with the discount factor increment d that satisfies the instantaneous pricing equation 0 Et d P , we can solve its stochas tic differential equation to find the level that prices a j period payoff by Pt Et t j t x t j . Second, we can chain the prices together. Start ing with pT 1 ET 1 mT 1,T xT we can find pT 2 ET 2 mT 2,T 1pT 1 Rt, t 1Rt 1, t 2 . . . Rt j 1, t j . Starting with 0 Et d P , we can find a differ ential equation for the prices, and solve that back. We will use both methods and so forth. Conceptually, this is the same as chaining returns R t,t j to solve interest rate and option pricing models. t j 311",
        "17.2. Black Scholes Formula 321 The call option payoff is CT max ST X,0 , where X denotes the strike price X and ST denotes the stock price on the expiration date T . The underlying stock follows dS dt dz. S There is also a money market security that pays the real interest rate r dt. We want a discount factor that prices the stock and bond. All such discount factors are of the form m x w, E xw 0. In continuous time, all such discount factors are of the form d  r r dt dz w dw; E dw dz 0. You can check that this set of discount factors does in fact price the stock and interest rate, or take a quick look back at Section 4.3. Now we price the call option with this discount factor, and show that the Black Scholes equation results. Importantly, the choice of discount factor via choice of w dw turns out to have no effect on the resulting option price. Every discount factor that prices the stock and interest rate gives the same value for the option price. The option is therefore priced using the law of one price alone. There are two paths to follow. Either we solve the discount factor for ward, and then find the call value by C E mxC , or we characterize the price path and solve it backwards from expiration. Method 1: Price Using Discount Factor Let us use the discount factor to price the option directly:  C E T max S X,0 T max S X,0 df ,S , 00 T TTT 00 where T andST aresolutionsto dS dt dz, S 17.2 d  r r dt dz w dw.  I simplify the algebra by setting w dw to zero, anticipating that it does not matter. You can reason that since S does not depend on dw, CT depends",
        "322 17. Option Pricing only on ST , so C will only depend on S, and dw will have no effect on the answer. If this is not good enough, a problem asks you to include the dw, trace through the remaining steps, and verify that the answer does not in fact depend on dw. Solving a stochastic differential equation such as 17.2 means finding the distribution of the random variables ST and T , using information as of date 0. This is just what we do with difference equations. For example, if wesolvext 1 xt t 1 withnormalforwardtoxT Tx0 T T jj, j 1 wehaveexpressedxT asanormallydistributedrandomvariablewithmean Tx0 andvariance T 2 T j .Inthecontinuous timecase,itturnsoutthat j 1 we can solve some nonlinear specifications as well. Integrals of dz give us shocks, as integrals of dt give us deterministic functions of time. We can find analytical expressions for the solutions of equations of the form 17.2 . Start with the stochastic differential equation Write dY Y dt Y dz. Y dlnY dY 11dY2  12 dt dZ. Y2Y2Y2YY 17.3 17.4 Integrating from 0 to T , 17.3 has solution T1TT dlnY  2 dt  Y2YYt 000 2 lnYT lnY0 Y Y T Y zT z0. 2 zT z0 is a normally distributed random variable with mean zero and variance T . Thus, ln Y is conditionally normal with mean 17.5  2 2 T and variance 2T. YYY Applying the solution 17.4 to 17.2 , we have lnST lnS0  T  T, 1  r 2 ln T ln 0 r T T, 2 2  r 2 dZ ln Y 0 where the random variable  is a standard normal, zT z0  T N 0, 1 .",
        "17.2. Black Scholes Formula 323 Having found the joint distribution of stock and discount factor, we evaluate the call option value by doing the integral corresponding to the expectation, ST X X 2 C0 ST X T ST Xdf T,ST 0 17.6  T ST  X df  . ST X 0 We know the joint distribution of the terminal stock price ST and dis count factor T on the right hand side, so we have all the information we need to calculate this integral. This example has enough structure that we can find an analytical formula. In more general circumstances, you may have to resort to numerical methods. At the most basic level, you can sim ulate the , S process forward and then take the integral by summing over many such simulations. Doing the Integral Start by breaking up the integral 17.6 into two terms, C0 ST X ST and T are both exponential functions of . The normal distribution is also an exponential function of . Thus, we can approach this integral exactly as we approach the expectation of a lognormal; we can merge the two exponentials in  into one term, and express the result as integrals against a normal distribution. Here we go. Plug in 17.5 for ST , T , and simplify the exponentials in terms of ,   T ST  df  T Xdf  . 0 ST X 0 2 C e r 1  r T  r TS e  12 T  Tf  d 22 00 1  r  r e r T Tf  d 2 ST X 2 e  r 1 2  r T   r Tf  d 02 S X ST X Now add the normal distribution formula for f  , 2 ST X 2 e r 1  r T  r Tf  d. 2 f  e . 1 1 2 2",
        "324 17. Option Pricing The result is 2 1  r 1 2  r T   r T 12 C0 S0e2 2d 2 ST X 2 1 r 1  r T  r T 12 Xe2  2d 2 ST X 2 1 S0 e2  d 1    r T 2 ST X 1 rT 1   r T 2 Xe e2  d. 2 ST X Notice that the integrals have the form of a normal distribution with nonzero mean.ThelowerboundST Xis,intermsof, 2 lnX lnST lnS0  lnX lnS0  2 2T  . T Finally, we can express definite integrals against a normal distribution by the cumulative normal, 1 e 1 2   2 d  a , i.e., is the area under the left tail of the normal distribution: 2 T  T, 2 a lnX lnS0  2 2 T  r C0 S0  T T rT lnX lnS0  2 2T  r Xe T. T Simplifying, we get the Black Scholes formula ln S0 X r 2 2T C0 S0  T 17.7 rT ln S0 X r 2 2T Xe . T",
        "17.2. Black Scholes Formula 325 Method 2: Derive Black Scholes Differential Equation Rather than solve the discount factor forward and then integrate, we can solve the price backwards from expiration. The instantaneous or expected return formulation of a pricing model amounts to a differential equation for prices. Guess that the solution for the call price is a function of stock price and time to expiration, C S , t . Use Ito s lemma to find derivatives of C S , t , dC Ct dt CS dS 12 CSS dS 2 Ct CSS 12CSSS22 dt CSSdz. Plugging into the basic asset pricing equation 0 Et d C CEt d Et dC Et d dC, using Et d r dt and canceling dt , we get or, 1 0 rC Ct CSS CSSS22 S rCS 2 0 rC Ct SrCS 12CSSS22. This is the Black Scholes differential equation for the option price. We now know a differential equation for the price function C S , t . We knowthevalueofthisfunctionatexpiration,C ST,T max ST X,0 . The remaining task is to solve this differential equation backwards through time. Conceptually, and numerically, this is easy. Express the differential equation as C S,t t C S,t 1 2C S,t rC S , t Sr S 2  2 . 17.8 At any point in time, you know the values of C S , t for all S for example, you can store them on a grid for S . Then, you can take the first and second derivatives with respect to S and form the quantity on the right hand side at each value of S. Now, you can find the option price at any value of S, one instant earlier in time. S 2 S2",
        "326 17. Option Pricing This differential equation, solved with boundary condition C max ST X , 0 , has an analytic solution the familiar formula 17.7 . One standard way to solve differential equations is to guess and check; and by taking derivatives you can check that 17.7 does satisfy 17.8 . Black and Scholes solved the differential equation with a fairly complicated Fourier transform method. The more elegant Feynman Kac solution amounts to showing that solu tions of the partial differential equation 17.8 can be represented as integrals of the form that we already derived independently as in 17.6 . See Duffie 1992, p. 87 . Problems Chapter 17 1. We showed that you should never exercise an American call early if there are no dividends. Is the same true for American puts, or are there circumstances in which it is optimal to exercise American puts early? 2. Retrace the steps in the integral derivation of the Black Scholes formula and show that the dw does not affect the final result.",
        "18.3. Multiple Periods and Continuous Time 337 To see that the bounds are recursive, consider a two period version of the problem, C min E m m xc s.t. m1,m2 0 1 2 2 t t t 1t 1 t t 1 t 0 p E m p ,E m2 A2, m 0, t 1 t 0,1. This two period problem is equivalent to a series of one period problems, 18.15 in which the C0 problem finds the lowest price of the C1 lower bound, C minE mxc , C minE mC , 1 m2 1 2 2 0 m1 0 1 1 subject to 18.15 . Why? The solution to the two period problem minE0 m1E1 m2xc must minimize E1 m2xc in each state of nature at time 1. If not, you could lower E1 m2xc without affecting the constraints, and lower the objective. Note that this recursive property only holds if we impose m 0. If m1 0 were possible, we might want to maximize E1 m2xc in some states of nature. Basis Risk and Real Options The general case leads to some dense formulas, so a simple example will let us understand the idea most simply. Let us value a European call option on an event V that is not a traded asset, but is correlated with a traded asset that can be used as an approximate hedge. This situation is common with real options and nonfinancial options and describes some financial options on illiquid assets. The terminal payoff is xc max V K,0 . TT Model the joint evolution of the traded asset S and the event V on which the option is written as dS S dt S dz, S dV V dt Vzdz Vwdw. V The dw risk cannot be hedged by the S asset, so the market price of dw risk its correlation with the discount factor will matter to the option price. We are looking for a discount factor that prices S and r f , has instanta neous volatility A, and generates the largest or smallest price for the option.",
        "338 18. Option Pricing without Perfect Replication Hence it will have the largest loading on dw possible. By analogy with the one period case 18.5 , you can quickly see that the discount factor will have the form d d A2 hS2dw, r dt hS dz, S r hS . d  S is the familiar analogue to x that prices stock and bond. We add a d loading on the orthogonal shock dw just sufficient to satisfy the constraint Et d 2 2 A2. One of will generate the upper bound, and one will generate the lower bound. Now that we have the discount factor, the good deal bound is given by C Et T max VT K . t t St , ST , VT , and defining the expectation is straightforward to perform, and works very sim ilarly to the integral we evaluated to solve the Black Scholes formula in Section 16.2.1. If you get stuck, see Cochrane and Sa Requejo 2000 for the algebra. The result is Vt , and t are all diffusions T are jointly lognormally distributed, so the double integral with constant coefficients. Therefore, 1 1 CorC V0eT d V T Ke rT d V T , 18.16 22 where  denotes the left tail of the normal distribution and dV2 V tV2 Vz Vw ln V0 K  r T d , V T  hV hS  a S r hS , hV 2 E 2 2 , A2 hS2 V r , 1 1 2 V,  SV",
        "18.3. Multiple Periods and Continuous Time 339 dV dS Vz  corr , , V S V 1 upper bound, a This expression is exactly the Black Scholes formula with the addition oftheterm.V enterstheformulabecausetheeventV maynotgrowatthe same rate as the asset S . Obviously, the correlation  between V shocks and asset shocks enters the formula, and as this correlation declines, the bounds widen. The bounds also widen as the volatility constraint A becomes larger relative to the asset Sharpe ratios hS . Market Prices of Risk 1 lower bound. Continuous time pricing problems are often specified in terms of market prices of risk rather than discount factors. This is the instantaneous Sharpe ratio that an asset must earn if it loads on a specific shock. If an asset has a price process P that loads on a shock  dw, then its expected return must be with Sharpe ratio EdP rfdt E d dw tPt  Et dP P rfdt E d dw. t I have introduced the common notation  for the market price of risk. Thus, problems are often attacked by making assumptions about  directly and then proceeding from E dP r f dt  . tP Inthislanguage,themarketpriceofstockriskishS andcanbemeasured by observing the stock, and does not matter when you can price by arbitrage notice it is missing from the Black Scholes formula . Our problem comes down to choosing the market price of dw risk, which cannot be measured by observing a traded asset, in such a way as to minimize or maximize the option price, subject to a constraint that the total price of risk h2 2 A. S Continuous Time Now, I give a more systematic expression of the same ideas in continuous time. As in the option pricing case in the last chapter and the term structure",
        "340 18. Option Pricing without Perfect Replication case in the next chapter, we will obtain a differential characterization. To actually get prices, we have either to solve the discount factor forward, or to find a differential equation for prices which we solve backward. Basis Assets In place of E x , E xx , etc., model the price processes of an nS dimensional vector of basis assets by a diffusion, dS S S,V,t dt S S,V,t dz, E dzdz I. 18.17 S Rather than complicate the notation, understand division to operate ele ment by element on vectors, e.g., dS S dS1 S1 dS2 S2 . The basis assets may pay dividends at rate D S, V , t dt. V represents an nV dimensional vector of additional state variables that follow dV V S,V,t dt Vz S,V,t dz Vw S,V,t dw, E dw dw I, E dw dz 0. 18.18 This could include a stochastic stock volatility or stochastic interest rate classic cases in which the Black Scholes replication breaks down. Again, I keep it simple by assuming there is a risk free rate. The Problem We want to value an asset that pays continuous dividends at rate xc S,V,t dt andhasaterminalpaymentxc S,V,T .Nowwemustchoose T a discount factor process to minimize the asset s value subject to the constraints that 1 the discount factor prices the basis assets S , r at each moment in time, 2 the instantaneous volatility of the discount factor process is less than a prespecified value A2, and 3 the discount factor ispositive s 0,t s T. One Period at a Time; Differential Statement Since the problem is recursive, we can study how to move one step back in time, or, for small time intervals, T C min E sxcds E Txc 18.19 t s,t s T t s t T s tt t Ct minEt t s t t s t sxcsds Et t tC t t Ct minEt xct C C t . t t",
        "18.3. Multiple Periods and Continuous Time Letting t 0, we can write the objective in differential form, 341 18.20 18.21 and hence, using 18.22 , vv A2 dt 2 1 d 2 1 . d rdt  1 dz, SSS xct Et d C 0 dt min C d C , subject to the constraints. We can also write 18.20 as EdC xct dt rfdt minE d dC . tCC d t C This condition sensibly tells us to find the lowest value C by maximizing the drift of the bound at each date. You should recognize the form of 18.20 and 18.21 as the basic pricing equations in continuous time, relating expected returns to covariance with discount factors. Constraints Now we express the constraints. As in the discrete time case, we orthogonal ize the discount factor in m x  form, and then the solution pops out. Any discount factor that prices the basis assets is of the form where d d vdw, 18.22   D r ,   , SSSSSS and v is a 1 nV but they will have no effect on the answer; the minimization will set such loadings to zero. matrix. We can add shocks orthogonal to dw if we like, The volatility constraint is 1d2 Et A2, E dt t 2 A2  S S S 18.23 By expressing the constraints via 18.22 and 18.23 , we have again reduced the problem of choosing the stochastic process for to the choice of loadings v on the noises dw with unknown values, subject to a quadratic constraint on vv . Since we are picking differentials and have ruled out jumps, the positivity constraint is slack so long as 0.",
        "342 18. Option Pricing without Perfect Replication Market Prices of Risk Using equation 18.22 , v is the vector of market prices of risks of the dw shocks the expected return that any asset must offer if its shocks are dw: 1E d dw v. dt Thus, the problem is equivalent to: find at each date the assignment of market prices of risk to the dw shocks that minimizes maximizes the focus payoff value, subject to the constraint that the total sum of squared market price of risk is bounded by A2. Now, we are ready to follow the usual steps. We can characterize a differ ential equation for the option price bound that must be solved back from expiration, or we can try to solve the discount factor forward and take an expectation. Solutions: The Discount Factor and Bound Drift at Each Instant We can start by characterizing the bound s process, just as the basis assets follow 18.17 . This step is exactly the instantaneous analogue of the one period bound without a positivity constraint, so remember that logic if the equations start to get a bit forbidding. Guess that lower bound C follows a diffusion process, and figure out what the coefficients must look like. Write dC C S,V,t dt Cz S,V,t dz Cw S,V,t dw. 18.24 C Cz and Cw capture the stochastic evolution of the bound over the next instant the analogues to E xxc , etc. that were inputs to the one period problem. Therefore, a differential or moment to moment characterization of the bound will tell us C and d in terms of Cz and Cw. 18.25 18.26 18.27 Theorem: The lower bound discount factor t follows andC,Cz,andCw satisfytherestriction where d d v dw xc 1d  r E  dz v , CCdtt Cz Cw 1d 2 Cw v A2 Et . dt 2   Cw Cw",
        "18.3. Multiple Periods and Continuous Time 343 The upper bound process C and discount factor have the same representation with v v. This theorem has the same geometric interpretation as shown in Figure 18.1 d is the combination of basis asset shocks that prices the basis assets by construction, in analogy to x . The term Cw dw corresponds to the error w, and   corresponds to E w2 . The proposition looks Cw Cw a little different because now we choose a vector v rather than a number. We could define a residual Cw dw and then the problem would reduce to choosing a number, the loading of d on this residual. It is not convenient todosointhiscasesinceCw potentiallychangesovertime.Inthegeometry of Figure 18.1, the w direction may change over time. The algebraic proof just follows this logic. Proof: Substituting equation 18.22 into the problem 18.20 in order to impose the pricing constraint, the problem is 0 xc d C dC dt Et min vEt dw C C v C 2 1 d 2 s.t.vv A Et 2 . tt dt Using equation 18.24 for d C C in the last term, the problem is 0 xc 1 d C E min v  s.t. Cdtt C v Cw 2 1 d 2 vv A Et 2 . dt 18.28 This is a linear objective in v with a quadratic constraint. Therefore, as longasCw 0,theconstraintbindsandtheoptimalvisgivenby 18.27 . v v gives the maximum since   0. Plugging the optimal value Cw Cw for v in 18.28 gives 0 E v . xc 1 d C Cdtt C Cw For clarity, and exploiting the fact that d middle term as does not load on dw, write the 1 d C Et 1 d C r Et Czdz . dt C dt If Cw 0, any v leads to the same price bound. In this case we can most simply take v 0. P",
        "344 18. Option Pricing without Perfect Replication dw, 18.29 As in the discrete time case, we can plug in the definition of explicit, if less intuitive, expressions for the optimal discount factor and the resulting lower bound, d Cw S S S S S S   rdt  1 dz A2  1 to obtain Cw Cw xc  r  1 A2  1   . CC SSSCz SSSCwCw A Partial Differential Equation 18.30 Now we are ready to apply the standard method; find a partial differen tial equation and solve it backwards to find the price at any date. The method proceeds exactly as for the Black Scholes formula: Guess a solu tion C S,V,t . Use Ito s lemma to derive expressions for C and Cz,Cw in terms of the partial derivatives of C S , V , t . Substitute these expressions into restriction 18.30 . The result is ugly, but straightforward to evalu ate numerically. Just like the Black Scholes partial differential equation, it expresses the time derivative C t in terms of derivatives with respect to state variables, and thus can be used to work back from a terminal period. Theorem:The lower bound C S,V,t is the solution to the partial differential equation C 1 2C xc rC SS      S  i,j i j t 2 S SijSiSj i,j i j 1 2C 2 V V Vzi Vzj Vwj Vwj i,j i j 2C S V i Si Vzj D r S C  1    C S A 2  1  C   C , S S S V Vw Vw V subject to the boundary conditions provided by the focus asset payoff xcT . CV denotes the vector with typical element C Vj and S C element Si C Si. Replacing with before the square root gives the partial differential equation satisfied by the upper bound. The Discount Factor In general, the process 18.25 or 18.29 depends on the parameters Cw. Hence, without solving the above partial differential equation we do S SSSVzVV S denotes the vector with typical",
        "19.5. Three Linear Term Structure Models 373 Interchanging the order of the first integral, evaluating the easy ds integrals, and rearranging, and simplifying, t   r  ttt  dzs r s 0 e  s u ds dzu u 0 s u 1t r 2t r r e sds, 2 0 u 0 19.28 1 2 r  t r r . 1 e  t u dzu 1 e t 2 0 s 0 The first integral includes a deterministic function of time u times dzu . This gives rise to a normally distributed random variable it is just a weighted sum of independent normals dzu : tt f u dzu N 0, f2 u du . u 0 Thus, ln t ln 0 is normally distributed with mean given by the second u 0 set of terms in 19.28 and variance var0 ln t ln 0 2 t u 0   r 1 e  t u du 19.29  t 2 2  r 2 r  r e  t u r e 2 t u du u 0    2 2 2  r t 2r r 1 e t r1 e 2t.  2  23 Sincewehavethedistributionof N wearereadytotaketheexpectation: lnP N,0 lnE0 eln N ln 0 E0 ln N ln 0 12 ln ln . 20N0",
        "Appendix Continuous Time This appendix is a brief introduction to the mechanics of continuous time stochastic processes; i.e., how to use dz and dt . I presume the reader is famil iarwithdiscrete timeARMAmodels,i.e.,modelsofthesortxt xt 1 t. I draw analogies of continuous time constructs to those models. The formal mathematics of continuous time processes are a bit impos ing. For example, the basic random walk zt is not time differentiable, so one needs to rethink the definition of an integral and differential to write obvious things like z t t dzs . Also, since z t is a random variable, one s 0 has to specify not only the usual measure theoretic foundations of random variables, but their evolution over a continuous time index. However, with a fewbasic,intuitiveruleslikedz2 dt,youcanusecontinuous timeprocesses quite quickly, and that is the aim of this chapter. A.1 Brownian Motion Diffusion models are a standard way to represent random variables in con tinuous time. The ideas are analogous to discrete time stochastic processes. We start with a simple shock series, t in discrete time and dzt in contin uous time. Then we build up more complex models by building on this foundation. The basic building block is a Brownian motion, which is the natural generalization of a random walk in discrete time. For a random walk zt zt 1 t, 489 zt,dzt aredefinedbyzt zt N 0, .",
        "490 Appendix. Continuous Time thevariancescaleswithtime;var zt 2 zt 2var zt 1 zt .Thus,define aBrownianmotionasaprocesszt forwhich zt zt N 0, . A.1 We have added the normal distribution to the usual definition of a ran dom walk. As E tt 1 0 in discrete time, increments to z for nonover lapping intervals are also independent. I use the notation zt to denote z as a function of time, in conformity with discrete time formulas; many people prefer to use the standard representation of a function z t . It is natural to want to look at very small time intervals. We use the notationdzt torepresentzt zt forarbitrarilysmalltimeintervals ,and we sometimes drop the subscript when it is obvious we are talking about time t. Conversely, the level of zt is the sum of its small differences, so we can write the stochastic integral t s 0 The variance of a random walk scales with time, so the standard devi ation scales with the square root of time. The standard deviation is the zt z0 dzs. typical size of a movement in a normally distributed random variable, so the typical size of z t z t in time interval is . This fact means that zt zt has typical size 1 , so though the sample path of zt is continuous, zt is not differentiable. For this reason, it is important to be a little careful with notation. dz , dz t , or dz t mean zt zt for arbitrarily small . We are used to thinking about dz as the derivative of a function, but since a Brownian motion is not a differentiable function of time, dz dz t dt dt makes no sense. From A.1 , it is clear that Et dz t 0. Again, the notation is initially confusing how can you take an expectation attofarandomvariabledatedt?Keepinmind,however,thatdzt zt zt is the forward difference. The variance is the same as the second moment, so we write it as E dz2 dt. tt Itturnsoutthatnotonlyisthevarianceofdzt equaltodt,but dz2 dt. t As one simple way to see this point, since z t z t is normal, this means that zt zt 2 is a 2 random variable, z z N 0, 2. t t 1 zt zt 2",
        "A.2. Diffusion Model Taking moments, 491 E z z 2 E 2 t t 1 var z z 2 2var 2 2 2. t t 1 Before, wehadvar z z O so z t t t now var zt zt 2 O 2 so  zt zt 2 order dt. A.2 Diffusion Model z O . But O , it vanishes as t I form more complicated time series processes by adding drift and diffusion terms, dxt  dt  dzt. I introduce some common examples, Random walk with drift: AR 1 : Square root process: dxt  dt  dzt , dxt  x  dt dzt, dxt  x  dt  xt dzt , Priceprocess: dpt dt dzt. pt You can simulate a diffusion process by approximating it for a small time interval, xt xt  t  tt , t N 0,1 . As we add up serially uncorrelated shocks t to form discrete time ARMAmodels,webuildontheshocksdzt toformdiffusionmodels.Iproceed by example, introducing some popular examples in turn. Random walk with drift. In discrete time, we model a random walk with drift as xt  xt 1 t. The obvious continuous time analogue is dxt dt dzt.",
        "492 Appendix. Continuous Time Integrating both sides from 0 to t , we can find the implications of this process for discrete horizons, or This is a random walk with drift. xt x0 t  zt z0 xt x0 t t, t N 0,2t . AR 1 . The simplest discrete time process is an AR 1 , xt 1   xt 1 t or dxt  xt  dt dzt. This is known as the Ohrnstein Uhlenbeck process. The mean or drift is Et dxt  xt  dt. Thisforcepullsxbacktoitssteady statevalue,buttheshocksdzt move it around. Square root process. Like its discrete time counterpart, the continuous time AR 1 ranges over the whole real numbers. It would be nice to have a process that was always positive, so it could capture a price or an interest rate. An extension of the continuous time AR 1 is a workhorse of such applications, xt xt 1 1  xt 1  t. The continuous time analogue is analogue xt 1   xt 1 xtt dxt  xt  dt  xt dzt. Now, volatility also varies over time, E dx2 2x dt; ttt as x approaches zero, the volatility declines. At x 0, the volatility is entirely turned off, so x drifts up towards . This is a nice example because it is decidedly nonlinear. Its discrete time is not a standard ARMA model, so standard linear time series tools would fail us. We could not, for example, give a pretty equation for the distribu tion of xt s for finite s. It turns out that we can do this in continuous time. Thus, one advantage of continuous time formulations is that they give rise",
        "A.2. Diffusion Model 493 to a toolkit of interesting nonlinear time series models for which we have closed form solutions. Price processes. A modification of the random walk with drift is the most common model for prices. We want the return or proportional increase in price to be uncorrelated over time. The most natural way to do this is to specify or more simply, dpt ptdt ptdzt, dpt dt dzt. pt Diffusion models more generally. A general picture should emerge. We form more complex models of stochastic time series by changing the local mean and variance of the underlying Brownian motion: dxt  xt dt  xt dzt. More generally, we can allow the drift  and diffusion to be a function of other variables and of time explicitly. We often write dxt  dt  dzt to remind us of such possible dependence. There is nothing mysterious about this class of processes; they are just like easily understandable discrete time processes xt xt  t  tt , t N 0,1 . A.2 In fact, when analytical methods fail us, we can figure out how diffusion models work by simulating the discretized version A.2 for a fine time interval . The local mean of a diffusion model is and the local variance is Et dx t  dt dx2 E dx2 2 dt. ttt Variance is equal to second moment because means scale linearly with time interval , so mean squared scales with 2, while the second moment scales with . Stochastic integrals. For many purposes, simply understanding the dif ferential representation of a process is sufficient. However, we often want",
        "494 Appendix. Continuous Time tounderstandtherandomvariablext atlongerhorizons.Forexample,we might want to know the distribution of xt s given information at time t. Conceptually, what we want to do is to think of a diffusion model as a stochastic differential equation and solve it forward through time to obtain the finite time random variable x t s . Putting some arguments in for  and  for concreteness, we can think of evaluating the integral ttt xt x0 dxs  xs,s,... ds  xs,s,... dzs. 000 We have already seen how zt z0 t dzs generates the random variable 0 zt N 0,t ,soyoucanseehowexpressionslikethisonegeneraterandom variables x t . The objective of solving a stochastic differential equation is thus to find the distribution of x at some future date, or at least some character izations of that distribution such as conditional mean, variance, etc. Some authors dislike the differential characterization and always write processes in terms of stochastic integrals. A.3 Ito s Lemma You often have a diffusion representation for one variable, say dxt x dt x dzt. Then you define a new variable in terms of the old one, yt f xt . A.3 Do second order Taylor expansions; keep only dz, dt, and dz2 dt terms: dy f x dx 1f x dx2, 2 dy f x  1f x 2 dt f x  dz. x2x x Naturally, you want a diffusion representation for yt . Ito s lemma tells you how to get it. It says, Use a second order Taylor expansion, and think of dz as dt; thus as t 0,keeptermsdz,dt,anddz2 dt,buttermsdt dz,dt2,and higher go to zero.",
        "Problems 495 Applying these rules to A.3 , start with the second order expansion dy Expanding the second term, dx df x dx 1 d2f x 2 dx2 dx2. dx2  dt  dz 2 2 dt2 2 dz2 2  dt dz. xxxxxx Nowapplytheruledt2 0,dz2 dt,anddtdz 0.Thus, dx2 2dt. x Substituting for dx and dx2, df x 1 d2f x dy  dt  dz dxx x 2dx2x 2dt df x 1 d2f x df x  2dt dz. dx x 2 dx2 x dx x Thus, Ito s lemma: df x 1 d2f x dy  df x dxx2dx2x dxx 2 dt  dz. The surprise here is the second term in the drift. Intuitively, this term captures a Jensen s inequality effect. If a is a mean zero random variable and b f a with f a 0, then the mean of bis higher than the mean of a. The more variance of a, and the more concave the function, the higher the mean of b. 1. If Problems Appendix dp dt dz p find the diffusion followed by the log price, y ln p . 2. Find the diffusion followed by xy. 3. Suppose y f x, t . Find the diffusion representation for y. Follow the obvious multivariate extension of Ito s lemma."
    ],
    "Topic 1": [
        "3 Contingent Claims Markets Our first task is to understand the p E mx representation a little more deeply. In this chapter I introduce a very simple market structure, contingent claims. This leads us to an inner product interpretation of p E mx which allows an intuitive visual representation of most of the theorems. We see that discount factors exist, are positive, and the pricing function is linear, just starting from prices and payoffs in a complete market. We don t need any utility functions. The next chapter shows that these properties can be built up in incomplete markets as well. 3.1 Contingent Claims Suppose that one of S possible states of nature can occur tomorrow, i.e., specialize to a finite dimensional state space. Denote the individual states bys.Forexample,wemighthaveS 2ands rainors shine. A contingent claim is a security that pays one dollar or one unit of the consumption good in one state s only tomorrow. pc s is the price today of the contingent claim. I write pc to specify that it is the price of a contingent claim and s to denote in which state s the claim pays off. In a complete market investors can buy any contingent claim. They do not necessarily have to trade explicit contingent claims; they just need enough other securities to span or synthesize all contingent claims. For example, if the possible states of nature are rain, shine , one can span or synthesize I describe contingent claims. I interpret the stochastic discount factor m as contingent claims prices divided by probabilities, and p E mx as a bundling of contingent claims. 49",
        "50 3. Contingent Claims Markets any contingent claim, or any portfolio that can be achieved by combining contingent claims, by forming portfolios of a security that pays 2 dollars if it rainsandoneifitshines,orx1 2,1 ,andarisk freesecuritywhosepayoff patternisx2 1,1 . Now, we are on a hunt for discount factors, and the central point is: If there are complete contingent claims, a discount factor exists, and it is equal to the contingent claim price divided by probabilities. Let x s denote an asset s payoff in state of nature s. We can think of the asset as a bundle of contingent claims x 1 contingent claims to state 1, x 2 claims to state 2, etc. The asset s price must then equal the value of the contingent claims of which it is a bundle, s I denote the price p x to emphasize that it is the price of the payoff x. Where the payoff in question is clear, I suppress the x . I like to think of equation 3.1 as a happy meal theorem: the price of a happy meal in a frictionless market should be the same as the price of one hamburger, one small fries, one small drink, and the toy. It is easier to take expectations rather than sum over states. To this end, multiply and divide the bundling equation 3.1 by probabilities, p x pc s x s . 3.1 pc s p x where  s is the probability that state s occurs. Then define m as the ratio of contingent claim price to probability,  s m s x s E mx . Thus, in a complete market, the stochastic discount factor m in p E mx exists, and it is just a set of contingent claims prices, scaled by probabilities. As a result of this interpretation, the combination of discount factor and probability is sometimes called a state price density. The multiplication and division by probabilities seems very artificial in this finite state context. In general, we posit states of nature  that can take continuous uncountably infinite values in a space . In this case, the sums become integrals, and we have to use some measure to integrate over . . Now we can write the bundling equation as an expectation, s m s pc s  s  s x s ,  s s p",
        "3.2. Risk Neutral Probabilities 51 Thus, scaling contingent claims prices by some probability like object is unavoidable. 3.2 Risk Neutral Probabilities Another common transformation of p E mx results in risk neutral probabilities. Define  s Rf m s  s Rf pc s , where Rf 1 pc s 1 E m . The  s are positive, less than or equal to one and sum to one, so they are a legitimate set of probabilities. Now we can rewrite the asset pricing formula as I interpret the discount factor m as a transformation to risk neutral probabilities such that p E x Rf . p x 1 E x pc s x s  s x s . Rf Rf s Thus, we can think of asset pricing as if agents are all risk neutral, but with probabilities  in the place of the true probabilities . The proba bilities  give greater weight to states with higher than average marginal utility m. There is something very deep in this idea: risk aversion is equivalent to paying more attention to unpleasant states, relative to their actual prob ability of occurrence. People who report high subjective probabilities of unpleasant events like plane crashes may not have irrational expectations; they may simply be reporting the risk neutral probabilities or the product m  . This product is after all the most important piece of information for many decisions: pay a lot of attention to contingencies that are either highly probable or that are improbable but have disastrous consequences. The transformation from actual to risk neutral probabilities is given by to remind us that the expectation uses the risk neutral probabilities  instead of the real probabilities . I use the notation E m s  s E m  s .",
        "3.5. State Diagram and Price Function 57 State 2 Payoff p x pc s x s pc x pc proj x pc pc x cos  , pc Riskfree rate Price 1 returns Price 2 State 1 Payoff State 1 contingent claim Price 0 excess returns Figure 3.2. Contingent claims prices pc and payoffs. Interpreting pc and x as vectors, this means that the price is given by the inner product of the contingent claim price and the payoff. If two vectors are orthogonal if they point out from the origin at right angles to each other then their inner product is zero. Therefore, the set of all zero price payoffs must lie on a plane orthogonal to the contingent claims price vector, as shown in Figure 3.2. More generally, the inner product of two vectors x and pc equals the product of the magnitude of the projection of x onto pc times the magnitude of pc. Using a dot to denote inner product, s where x means the length of the vector x and  is the angle between the vectors pc and x. Since all payoffs on planes such as the price planes in Figure 3.2 that are perpendicular to pc have the same projection onto pc, they must have the same inner product with pc and hence the same price. Only the price 0 plane is, strictly speaking, orthogonal to pc. Lack ing a better term, I have called the nonzero price planes perpendicular to pc. When vectors are finite dimensional, the prime notation is com monly used for inner products, pc x. This notation does not extend well to infinite dimensional spaces. The notation pc x is also often used for inner products.",
        "58 3. Contingent Claims Markets Planes of constant price move out linearly, and the origin x 0 must have a price of zero. If payoff y 2x, then its price is twice the price of x, p y pc s y s pc s 2x s 2 p x . ss Similarly, a payoff of zero must have a price of zero. We can think of p x as a pricing function, a map from the state space or payoff space in which x lies RS in this case to the real line. We have just deduced from the definition 3.3 that p x is a linear function, i.e., that p ax by ap x bp y . The constant price lines in Figure 3.2 are of course exactly what one expects fromalinearfunctionfromRS toR. Onemightdrawthepriceonthezaxis coming out of the page. Then the price function would be a plane going through the origin and sloping up with iso price lines as given in Figure 3.2. Figure 3.2 also includes the payoffs to a contingent claim to the first state. This payoff is one in the first state and zero in other states and thus located on the axis. The plane of price 1 payoffs is the plane of asset returns; the plane of price 0 payoffs is the plane of excess returns. A risk free unit payoff the payoff to a risk free pure discount bond lies on the 1, 1 point in Figure 3.2; the risk free return lies on the intersection of the 45 line same payoff in both states and the price 1 plane the set of all returns . Geometry with m in Place of pc The geometric interpretation of Figure 3.2 goes through with the discount factor m in the place of pc. We can define an inner product between the random variables x and y by x y E xy , and retain all the mathematical properties of an inner product. For this reason, random variables for which E xy 0 are often called orthogonal. This language may be familiar from linear regressions. When we run a regression of y on x, y b x , we find the linear combination of x that is closest to y, by minimizing the variance or size of the residual . We do this by forcing the residual to be orthogonal to the right hand variable E x 0. The projection of y on x is defined as the fitted value, proj y x b x E xx 1E yx x. This idea is often illustrated by a residual vector  that is perpendicular to a plane defined by the right hand variables x . Thus, when the inner product is",
        "4 The Discount Factor Now we look more closely at the discount factor. Rather than derive a specific discount factor as with the consumption based model in the first chapter, I work backwards. A discount factor is just some random variable that generates prices from payoffs, p E mx . What does this expression mean? Can one always find such a discount factor? Can we use this con venient representation without implicitly assuming all the structure of the investors, utility functions, complete markets, and so forth? The chapter focuses on two famous theorems. The law of one price states that if two portfolios have the same payoffs in every state of nature , then they must have the same price. A violation of this law would give rise to an immediate kind of arbitrage profit, as you could sell the expensive version and buy the cheap version of the same portfolio. The first theorem states that there is a discount factor that prices all the payoffs by p E mx if and only if this law of one price holds. In finance, we reserve the term absence of arbitrage for a stronger idea, that if payoff A is always at least as good as payoff B, and sometimes A is better, then the price of A must be greater than the price of B. The second theorem is that there is a positive discount factor that prices all the payoffs by p E mx if and only if there are no arbitrage opportunities, so defined. These theorems are useful to show that we can use stochastic discount factors without implicitly assuming anything about utility functions, aggre gation, complete markets, and so on. All we need to know about investors in order to represent prices and payoffs via a discount factor is that they will not leave law of one price violations or arbitrage opportunities on the table. These theorems can be used to describe aspects of a payoff space such as law of one price, absence of arbitrage by restrictions on the discount factor such as it exists and it is positive . Chapter 18 shows how it can be more conve nient to impose and check restrictions on a single discount factor than it is to check the corresponding restrictions on all possible portfolios. Chapter 7 discusses these and other implications of the existence theorems. 61",
        "62 4. The Discount Factor The theorems are credited to Ross 1978 , Rubinstein 1976 , and Harrison and Kreps 1979 . My presentation is a simplified version of Hansen and Richard 1987 which contains rigorous proofs and some important technical assumptions. 4.1 Law of One Price and Existence of a Discount Factor So far we have derived the basic pricing relation p E mx from envi ronments with a lot of structure: either the consumption based model or complete markets. Suppose we observe a set of prices p and payoffs x, and that markets either the markets faced by investors or the markets under study in a particular application are incomplete, meaning they do not span the entire set of contingencies. In what minimal set of circumstances does some discount factor exist which represents the observed prices by p E mx ? This section and the following answer this important question. Payoff Space The payoff space X is the set of all the payoffs that investors can purchase, or it is a subset of the tradeable payoffs that is used in a particular study. For example, if there are complete contingent claims to S states of nature, then X RS . But the whole point is that markets are as in real life incomplete, so we will generally think of X as a proper subset of complete markets RS . The payoff space includes some set of primitive assets, but investors can also form new payoffs by forming portfolios of the primitive assets. I assume that investors can form any portfolio of traded assets: A1 Portfolioformation x1,x2 X ax1 bx2 X foranyreala,b. Of course, X RS for complete markets satisfies the portfolio forma tion assumption. If there is a single underlying, or basis payoff x, then the payoff space must be at least the ray from the origin through x. If there are two basis payoffs in R3, then the payoff space X must include the plane The definition of the law of one price; price is a linear function. p E mx implies the law of one price. The law of one price implies that a discount factor exists: There exists a unique x in X such that p E x x for all x X space of all available payoffs. Furthermore, for any valid discount factor m, x proj m X .",
        "4.1. Law of One Price and Existence of a Discount Factor 63 Figure 4.1. Payoff spaces X generated by one top and two bottom basis payoffs. defined by these two payoffs and the origin. Figure 4.1 illustrates these possibilities. The payoff space is not the space of returns. The return space is a subset of the payoff space; if a return R is in the payoff space, then you can pay a price 2 to get a payoff 2R , so the payoff 2R with price 2 is also in the payoff space. Also, R is in the payoff space. Free portfolio formation is in fact an important and restrictive simplify ing assumption. It rules out short sales constraints, bid ask spreads, leverage limitations, and so on. The theory can be modified to incorporate these frictions, but it is a substantial modification. If investors can form portfolios of a vector of basic payoffs x say, the returns on the NYSE stocks , then the payoff space consists of all portfolios or linear combinations of these original payoffs X c x where c is a vector of portfolio weights. We also can allow truly infinite dimensional payoff spaces. For example, investors might be able to trade nonlinear functions of a basis",
        "64 4. The Discount Factor payoff x, such as call options on x with strike price K, which have payoff max x s K,0 . The Law of One Price A2 Law of one price, linearity p ax1 bx2 ap x1 bp x2 . It does not matter how one forms the payoff x. The price of a burger, shake, and fries must be the same as the price of a Happy Meal. Graphically, if the iso price curves were not planes, then one could buy two payoffs on the same iso price curve, form a portfolio whose payoff is on the straight line connecting the two original payoffs, and sell the portfolio for a higher price than it cost to assemble it. The law of one price basically says that investors cannot make instanta neous profits by repackaging portfolios. If investors can sell securities, this is a very weak characterization of preferences. It says there is at least one investor for whom marketing does not matter, who values a package by its contents. The law is meant to describe a market that has already reached equilib rium. If there are any violations of the law of one price, traders will quickly eliminate them so they cannot survive in equilibrium. A1 and A2 also mean that the 0 payoff must be available, and must have price 0. The Theorem The existence of a discount factor implies the law of one price. This is obvious to the point of triviality: if x y z, then E mx E m y z . The hard, and interesting part of the theorem reverses this logic. We show that the law of one price implies the existence of a discount factor. Theorem: Given free portfolio formation A1, and the law of one price A2, there exists a unique payoff x X such that p x E x x for all x X. Proof 1: Geometric We have established that the price is a linear func tion as shown in Figure 4.2. Figure 4.2 can be interpreted as the plane X of a larger dimensional space as in the bottom panel of Figure 4.1, laid flat on the page for clarity . Now we can draw a line from the origin per is a discount factor. A1 and A2 imply that the price function on X looks like Figure 3.2: parallel hyperplanes marching out from the origin. The only difference is that X may be a subspace of the original state space, as shown in Figure 4.1. The essence of the proof, then, is that any linear function on a space X can be represented by inner products with a vector that lies in X . x pendicular to the price planes. Choose a vector x line is orthogonal to the price zero plane we have 0 p x E x x for on this line. Since the",
        "4.1. Law of One Price and Existence of a Discount Factor 65 Figure 4.2. Existence of a discount factor x . price zero payoffs x immediately. The inner product between any payoff x on the price 1 plane and x is proj x x x . Thus, every payoff on the price 1 plane has the same inner product with x . All we have to do is pick x to have the right length, and we obtain p x 1 E x x for every x on the price 1 plane. Then, of course we have p x E x x for payoffs x on the other planes as well. Thus, the linear pricing function implied by the law of one price can be represented by inner products with x . P The basic mathematical point is just that any linear function can be represented by an inner product. The Riesz representation theorem extends the proof to infinite dimensional payoff spaces. See Hansen and Richard 1987 . Proof 2: Algebraic We can prove the theorem by construction when the payoff space X is generated by portfolios of N basis payoffs for example, N stocks . This is a common situation, so the formulas are also useful in practice. Organize the basis payoffs into a vector x x1 x2 xN and similarly their prices p. The payoff space is then X c x . We want a discount factor that is in the payoff space, as the theorem requires. Thus, it must be of the form x c x. Construct c so that x prices the basis assets. We want p E x x E xx c . Thus we need c E xx 1p. If E xx is nonsingular, this c exists and is unique. A2 implies that E xx is nonsingular",
        "66 4. The Discount Factor after pruning redundant rows of x . Thus, x p E xx 1x 4.1 is our discount factor. It is a linear combination of x so it is in X . It prices the basis assets x by construction. It prices every x X: E x x c E p E xx 1xx c p c. By linearity, p c x c p. P What the Theorem Does and Does Not Say The theorem says there is a unique x in X. There may be many other discount factors m not in X . In fact, unless markets are complete, there are an infinite number of random variables that satisfy p E mx . If p E mx , then p E m  x for any  orthogonal to x, E x 0. Not only does this construction generate some additional discount fac tors, it generates all of them: Any discount factor m any random variable that satisfies p E mx can be represented as m x  with E x 0. Figure 4.3 gives an example of a one dimensional X in a two dimensional state space, in which case there is a whole line of possible discount factors m. If markets are complete, there is nowhere to go orthogonal to the payoff space X , so x is the only possible discount factor. Figure 4.3. Many discount factors m can price a given set of assets in incomplete markets. Reversing the argument, x m on the space X of payoffs. This is a very important fact: the pricing implications of any discount factor m for a set of payoffs X are the same as those of the projection of m on X. This discount factor is known as the mimicking portfolio for m. is the projection of any stochastic discount factor",
        "4.2. No Arbitrage and Positive Discount Factors 67 Algebraically, p E mx E proj m X  x E proj m X x . Let me repeat and emphasize the logic. Above, we started with investors or a contingent claim market, and derived a discount factor. p E mx implies the linearity of the pricing function and hence the law of one price, a pretty obvious statement in those contexts. Here we work backwards. Markets are incomplete in that contingent claims to lots of states of nature are not available. We found that the law of one price implies a linear pricing function, and a linear pricing function implies that there exists at least one and usually many discount factors even in an incomplete market. We do allow arbitrary portfolio formation, and that sort of com pleteness is important to the result. If investors cannot form a portfolio ax by, they cannot force the price of this portfolio to equal the price of its constituents. The law of one price is not innocuous; it is an assumption about preferences, albeit a weak one. The point of the theorem is that this is just enough information about preferences to deduce the existence of a discount factor. 4.2 No Arbitrage and Positive Discount Factors No arbitrage is another, slightly stronger, implication of marginal utility, that can be reversed to show that there is a positive discount factor. We need to start with the definition of arbitrage: Definition Absence of arbitrage : A payoff space X and pricing function p x leave no arbitrage opportunities if every payoff x that is always nonnegative, x 0 almost surely , and positive, x 0, with some positive probability, has positive price, p x 0. No arbitrage says that you cannot get for free a portfolio that might pay off positively, but will certainly never cost you anything. This definition is different from the colloquial use of the word arbitrage. Most people use arbitrage to mean a violation of the law of one price a riskless way of buying something cheap and selling it for a higher price. Arbitrages here The definition of the absence of arbitrage: positive payoff implies positive price. There is a strictly positive discount factor m such that p E mx if and only if there are no arbitrage opportunities and the law of one price holds.",
        "68 4. The Discount Factor might pay off, but then again they might not. The word arbitrage is also widely abused. Risk arbitrage is a Wall Street oxymoron that means making specific kinds of bets. An equivalent statement is that if one payoff dominates another, then its price must be higher if x y, then p x p y . Or, a bit more carefully but more long windedly, if x y almost surely and x y with positive probability, then p x p y . You cannot forget that x and y are random variables. m 0 Implies No Arbitrage The absence of arbitrage opportunities is clearly a consequence of a positive discount factor, and a positive discount factor naturally results from any sort of utility maximization. Recall, u c s u c It is a sensible characterization of preferences that marginal utility is always positive. Few people are so satiated that they will throw away money. Therefore, the marginal rate of substitution is positive. The marginal rate of substitution is a random variable, so positive means positive in every state of nature or in every possible realization. Now, if contingent claims prices are all positive, a bundle of positive amounts of contingent claims must also have a positive price, even in incomplete markets. A little more formally, Theorem: p E mx and m s 0 imply no arbitrage. Proof: We have m 0; x 0 and there are some states where x 0. Thus, in some states with positive probability mx 0 and in other states mx 0. Therefore, E mx 0. P No Arbitrage and the Law of One Price Imply m 0 Now we turn the observation around, which is again the hard and interesting part. As the law of one price property guaranteed the existence of a discount factor m, no arbitrage and the law of one price guarantee the existence of a positive m. The basic idea is pretty simple. No arbitrage means that the prices of any payoff in the positive orthant except zero, but including the axes must be strictly positive. The price 0 plane divides the region of positive prices from the region of negative prices. Thus, if the region of negative prices is not to intersect the positive orthant, the iso price lines must march up and to the right, and the discount factor m must point up and to the right. This is how we have graphed it all along, most recently in Figure 4.2. m s  0.",
        "4.2. No Arbitrage and Positive Discount Factors 69 Figure 4.4. Example for the theorem relating positive discount factors to the absence of arbi trage. The payoff x is an arbitrage opportunity: the payoff is positive, but it has a negative price. The discount factor is not strictly positive. Figure 4.4 illustrates the case that is ruled out: a whole region of negative price payoffs lies in the positive orthant. For example, the payoff x is strictly positive, but has a negative price. As a result, the unique, since this market is complete discount factor m is negative in the y axis state. The theorem is easy to prove in complete markets. There is only one Theorem: In complete markets, no arbitrage and the law of one price imply that there exists a unique m 0 such that p E mx . Proof: From the law of one price, there is an x such that p E x x . In a complete market this is the unique discount factor. Suppose that x 0 for some states. Then, form a payoff x that is 1 in those states, and zero elsewhere. This payoff is strictly positive, but its price,  s x s is s:x s 0 negative, negating the assumption of no arbitrage. P The tough part comes if markets are incomplete. There are now many discount factors that price assets. Any m of the form m x , with E x 0, will do. We want to show that at least one of these is positive. But m,x . If it is not positive in some state, then the contingent claim in that state has a positive payoff and a negative price, which violates no arbitrage. More formally, 1",
        "70 4. The Discount Factor thatonemaynotbex .Sincethediscountfactorsotherthanx arenotinthe payoff space X , we cannot use the construction of the last argument, since that construction may yield a payoff that is not in X , and hence to which we do not know how to assign a price. To handle this case, I adopt a different strategy of proof. This strategy is due to Ross 1978 . Duffie 1992 has a more formal textbook treatment. The basic idea is another to every plane there is a perpendicular line argument, but applied to a space that includes prices and payoffs together. The price 0 plane is a separating hyperplane between the positive orthant and the negative payoffs, and the proof builds on this idea. Theorem: No arbitrage and the law of one price imply the existence of a strictly positive discount factor, m 0, p E mx x X. Proof: Join p x , x together to form vectors in RS 1 . Call M the set of all p x ,x pairs, M p x ,x ;x X . Given the law of one price, M is still a linear space: m1 M,m2 M am 1 bm 2 M . No arbitrage means that elements of M cannot consist entirely of positive elements. If x is positive, p x must be negative. Thus, M is a hyperplane that only intersects the positive orthant RS 1 at the point 0. WecanthencreatealinearfunctionF:RS 1 RsuchthatF p,x 0for p,x M, and F p,x 0 for p,x RS 1 except the origin. Since we can represent any linear function by a perpendicular vector, there is a vector 1,m suchthatF p,x 1,m p,x p m x or p E mx using the second moment inner product. Finally, since F p , x is positive for p, x 0, m must be positive. P In a larger space than RS 1 , as generated by continuously valued random variables, the separating hyperplane theorem assures us that there is a linear function that separates the two convex sets M and the equivalent of RS 1, and the Riesz representation theorem tells us that we can represent F as an inner product with some vector by F p, x p m x. What the Theorem Does and Does Not Say The theorem says that a discount factor m 0 exists, but it does not say that m 0 is unique. The top panel of Figure 4.5 illustrates the situation. The theorem says that a positive m exists, but it also does not say that every discount factor m must be positive. The discount factors in the top panel Any m on the line through x p E m  x if E x 0. All of these discount factors that lie in the positive orthant are positive, and thus satisfy the theorem. There are lots of them! In a complete market, m is unique, but not otherwise. perpendicular to X also prices assets. Again,",
        "4.2. No Arbitrage and Positive Discount Factors 71 Figure 4.5. Existence of a discount factor and extensions. The top graph shows that the positive discount factor is not unique, and that discount factors may also exist that are not strictly positive. In particular, x choice of m 0 induces an arbitrage free extension of the prices on X to all contingent claims. need not be positive. The bottom graph shows that each particular of Figure 4.5 outside the positive orthant are perfectly valid they satisfy p E mx , and the prices they generate on X are arbitrage free, but they This theorem shows that we can extend the pricing function defined on X to all possible payoffs RS , and not imply any arbitrage opportunities on that larger space of payoffs. It says that there is a pricing function p x defined over all of RS , that assigns the same correct, or observed prices on are not positive in every state of nature. In particular, the discount factor x in the payoff space is still perfectly valid p x E x x but it need not be positive.",
        "72 4. The Discount Factor X and that displays no arbitrage on all of RS . Graphically, it says that we can drawparallelplanestorepresentpricesonallofRS insuchawaythatthe planes intersect X in the right places, and the price planes march up and to the right so the positive orthant always has positive prices. Any positive discount factor m generates such a no arbitrage extension, as illustrated in the bottom panel of Figure 4.5. In fact, there are many ways to do this. Each different choice of m 0 generates a different extension of the pricing function. We can think of strictly positive discount factors as possible contingent claims prices. We can think of the theorem as answering the question: is it possible that an observed and incomplete set of prices and payoffs is gen erated by some complete markets, contingent claim economy? The answer is, yes, if there is no arbitrage on the observed prices and payoffs. In fact, since there are typically many positive discount factors consistent with a X , p x , there exist many contingent claims economies consistent with our observations. Finally, the absence of arbitrage is another very weak characterization of preferences and market equilibrium. The theorem tells us that this is enough to allow us to use the p E mx formalism with m 0. As usual, this theorem and proof do not require that the state space is RS . State spaces generated by continuous random variables work just as well. 4.3 An Alternative Formula, and x in Continuous Time In terms of the covariance matrix of payoffs, x E x p E x E x 1 x E x . Analogously, prices assets by construction in continuous time. d D p rfdt  r 1dz A Formula that Uses Covariance Matrices E xx in our previous formula 4.1 is a second moment matrix. We typ ically summarize data in terms of covariance matrices instead. Therefore, Being able to compute x gives an alternative formula in discrete time, and the continuous time counterpart. is useful in many circumstances. This section",
        "Implications of Existence and Equivalence Theorems 123 any set of probabilities. Thus, the existence and equivalence theorems work equallywellexanteasexpost:E mx ,,E R ,andsoforthcanrefertoagents subjective probability distributions, objective population probabilities, or to the moments realized in a given sample. This observation points to a great danger in the widespread exer cise of searching for and statistically evaluating ad hoc asset pricing models. Such models are guaranteed empirical success in a sample if one places little enough structure on what is included in the discount factor or reference portfolios. The only reason the model does not work perfectly is the restrictions the researcher has imposed on the number or identity of the factors included in m, or the parameters of the function relating the factors to m. Since these restrictions are the entire content of the model, they had better be interesting, carefully described, and well motivated! Obviously, this is typically not the case or I would not be making such a fuss about it. Most empirical asset pricing research posits an ad hoc pond of factors, fishes around a bit in that pond, and reports statistical measures that show success, in that the model is not statistically rejected in pricing a set of portfolios. The discount factor pond is usually not large enough to give the zero pricing errors we know are possible, yet the boundaries are not clearly defined. Discipline What is wrong, you might ask, with finding an ex post efficient port The mistake is that a portfolio that is ex post efficient in one sample, and hence prices all assets in that sample, is unlikely to be mean variance effi cient, ex ante or ex post, in the next sample, and hence is likely to do a poor job of pricing assets in the future. Similarly, the portfolio x p E xx 1x using the sample second moment matrix that is a discount factor by con struction in one sample is unlikely to be a discount factor in the next sample; the required portfolio weights p E xx 1 change, often drastically, from sample to sample. Thus, if the law of one price holds in a sample, one may form an x from sample moments that satisfies p x E x x , exactly, in that sample, where p x refers to observed prices and E x x refers to the sample average. Equivalently, if the sample covariance matrix of a set of returns is nonsingular, there exists an ex post mean variance efficient portfolio for which sample average returns line up exactly with sample regression betas. folio or x that prices assets by construction? Perhaps the lesson we should learn from the existence theorems is to forget about economics, the CAPM, marginal utility, and all that, and simply price assets with ex post mean variance efficient portfolios that we know set pricing errors to zero!",
        "Implications of Existence and Equivalence Theorems 127 But surely markets can be irrational or inefficient without requiring arbitrage opportunities? Yes, they can, if and only if the discount factors that generate asset prices are disconnected from marginal rates of substitution or trans formation in the real economy. But now we are right back to specifying and testing economic models of the discount factor! At best, an asset pricing puzzle might be so severe that the required discount factors are completely unreasonable by some standard measures of real marginal rates of sub stitution and or transformation, but we still have to say something about what a reasonable marginal rate looks like. In sum, the existence theorems mean that there are no quick proofs of rationality or irrationality. The only game in town for the purpose of explaining asset prices is thinking about economic models of the discount factor. The Number of Factors Many asset pricing tests focus on the number of factors required to price a cross section of assets. The equivalence theorems imply that this is a silly question. A linear factor model m b f or its equivalent expected return betamodelE Ri    arenotuniquerepresentations.Inparticular, if f given any multiple factor or multiple beta representation, we can easily find a single beta representation. The single factor m b f will price assets just aswellastheoriginalfactorsf,aswillx proj b f X orthecorresponding R . All three options give rise to single beta models with exactly the same pricing ability as the multiple factor model. We can also easily find equivalent representations with different numbers greater than one of factors. For example, write m a bf bf bf a bf b f b3f a bf bf 112233 1122b3 1122 2 to reduce a three factor model to a two factor model. In the ICAPM language, consumption itself could serve as a single state variable, in place of the S state variables presumed to drive it. There are times when one is interested in a multiple factor representa tion. Sometimes the factors have an economic interpretation that is lost on taking a linear combination. But the pure number of pricing factors is not a meaningful question. Discount Factors vs. Mean, Variance, and Beta Chapter 6 showed how the discount factor, mean variance, and expected return beta models are all equivalent representations of asset pricing. It seems a good moment to contrast them as well; to understand why the",
        "9.4. Arbitrage Pricing Theory APT 173 9.4 Arbitrage Pricing Theory APT The APT: If a set of asset returns are generated by a linear factor model, Ri E Ri E i E i f 0.  f i, j 1 j N ij j Then with additional assumptions there is a discount factor m linear in the factors m a b f that prices the returns. The APT, developed by Ross 1976a , starts from a statistical characteri zation. There is a big common component to stock returns: when the market goes up, most individual stocks also go up. Beyond the market, groups of stocks move together, such as computer stocks, utilities, small stocks, value stocks, and so forth. Finally, each stock s return has some completely idiosyn cratic movement. This is a characterization of realized returns, outcomes, or payoffs. The point of the APT is to start with this statistical characterization of outcomes, and derive something about expected returns or prices. The intuition behind the APT is that the completely idiosyncratic move ments in asset returns should not carry any risk prices, since investors can diversify idiosyncratic returns away by holding portfolios. Therefore, risk prices or expected returns on a security should be related to the security s covariance with the common components or factors only. The job of this section is then 1 to describe a mathematical model of the tendency for stocks to move together, and thus to define the factors and residual idiosyncratic components, and 2 to think carefully about what it takes for the idiosyncratic components to have zero or small risk prices, so that only the common components matter to asset pricing. There are two lines of attack for the second item. 1 If there were no residual, then we could price securities from the factors by arbitrage really, by the law of one price . Perhaps we can extend this logic and show that if the residuals are small, they must have small risk prices. 2 If investors all hold well diversified portfolios, then only variations in the factors drive consumption and hence marginal utility. Much of the original appeal of the APT came from the first line of attack, the idea that we could derive pricing implications without the economic structure required of the CAPM, ICAPM, or any other model derived as a specialization of the consumption based model. In this section, I will first try to see how far we can in fact get with purely law of one price arguments. I will",
        "174 9. Factor Pricing Models conclude that the answer is, not very far, and that the most satisfactory argument for the APT must rely on some economic restrictions. Factor Structure in Covariance Matrices The APT models the tendency of asset payoffs returns to move together via a statistical factor decomposition I define and examine the factor decomposition xi a  f i, E i 0, E fi 0. The factor decomposition is equivalent to a restriction on the payoff covari ance matrix. ii M i ijj ii  f i a  f i. 9.17 xi a The fj are the factors, the ij are the betas or factor loadings, and the i are residuals. As usual, I use the same letter without subscripts to denote a vector, j 1 for example f f1 f2 ... fK . A discount factor m, pricing factors f in m b f , and this factor decomposition or factor structure for returns are totally unrelated uses of the word factor. The APT is conventionally written with xi returns, but it ends up being much less confusing to use prices and payoffs. It is a convenient and conventional simplification to fold the factor means into the first, constant, factor and write the factor decomposition withzero meanfactorsf f E f , xi E xi Remember that E xi is still just a statistical characterization, not a predic tion of a model. We can construct the factor decomposition as a regression equation. Define the ij as regression coefficients, and then the i are uncorrelated with the factors by construction, E i 0; E  f 0. ij M ij j j 1  f i. 9.18",
        "9.4. Arbitrage Pricing Theory APT 175 The content the assumption that keeps 9.18 from describing any arbi trary set of payoffs is an assumption that the i are uncorrelated with each other , E ij 0. More general versions of the model allow some limited correlation across the residuals but the basic story is the same. The factor structure is thus a restriction on the covariance matrix of payoffs. For example, if there is only one factor, then cov xi,xj E if i jf j ij2 f 2 ifi j, i Thus,withN numberofsecurities,theN N 1 2elementsofavariance covariance matrix are described by N betas and N 1 variances. A vector version of the same thing is 2 0 0 1 cov x,x  2 f 0 2 0 . 2 0 0 ... With multiple orthogonalized factors, we obtain cov x,x   2 f   2 f diagonalmatrix . 111222 In all these cases, we describe the covariance matrix as a singular matrix  or a sum of a few such singular matrices plus a diagonal matrix. If we know the factors we want to use ahead of time, say the market value weighted portfolio and industry portfolios, or size and book market portfolios, we can estimate a factor structure by running regressions. Often, however, we do not know the identities of the factor portfolios ahead of time. In this case we have to use one of several statistical techniques under the broad heading of factor analysis that is where the word factor came from in this context to estimate the factor model. One can estimate a factor structure quickly by simply taking an eigenvalue decomposition of the covariance matrix, and then setting small eigenvalues to zero. 0 if i j. With no error term, implies Exact Factor Pricing xi E xi 1  f i p x i E x i p 1  p f i",
        "176 9. Factor Pricing Models and thus m a b f, p xi E mxi , E Ri Rf  , i using only the law of one price. Suppose that there are no idiosyncratic terms i . This is called an exact factor model. Now look again at the factor decomposition, xi E xi 1  f . 9.19 i Itstartedasastatisticaldecomposition.Butitalsosaysthatthepayoffxi can be synthesized as a portfolio of the factors and a constant risk free payoff . Thus, the price of xi can only depend on the prices of the factors f , p x i E x i p 1  p f . 9.20 i The law of one price assumption lets you take prices of right and left sides. If the factors are returns, their prices are 1. If the factors are not returns, their prices are free parameters which can be picked to make the model fit as well as possible. Since there are fewer factors than payoffs, this procedure is not vacuous. We are really done, but the APT is usually stated as there is a discount factorlinearinf thatpricesreturnsRi, or thereisanexpectedreturn beta representationwithf asfactors. Therefore,weshouldtakeaminutetoshow that the rather obvious relationship 9.20 between prices is equivalent to discount factor and expected return statements. Assuming only the law of one price, we know there is a discount factor here to remind us that it prices the factors. If the discount factor prices m linear in factors that prices the factors. We usually call it x , but call it f the factors, it must price any portfolio of the factors; hence f prices all payoffs xi that follow the factor structure 9.19 . To see the point explic  itly, denote f 1 f the factors including the constant. As with x , f p f E ff 1f a b f satisfiesp f E f f andp 1 E f . We could now go from m linear in the factors to an expected return beta model using the connections between the two representations outlined in Chapter 6. But there is a more direct and elegant connection. Start with 9.20 , specialized to returns x i R i and of course p R i 1. Use p 1 1 R f and solve for expected return as E Ri Rf  Rfp f Rf  . ii The last equality defines . Expected returns are linear in the betas, and the constants  are related to the prices of the factors. This is the same definition of  that we arrived at in Chapter 6 connecting m b f to expected return beta models.",
        "9.4. Arbitrage Pricing Theory APT 177 Approximate APT Using the Law of One Price Attempts to extend the exact factor pricing model to an approximate factor pricing model when errors are small, or markets are large, still only using law of one price. For fixed m, the APT gets better and better as R2 or the number of assets increases. However, for any fixed R 2 or size of market, the APT can be arbitrarily bad. These observations mean that we must go beyond the law of one price to derive factor pricing models. Actual returns do not display an exact factor structure. There is some idiosyncratic or residual risk; we cannot exactly replicate the return of a given stock with a portfolio of a few large factor portfolios. However, the idiosyncratic risks are often small. For example, factor model regressions of the form 9.17 often have very high R2, especially when portfolios rather than individual securities are on the left hand side. And the residual risks are still idiosyncratic: Even if they are a large part of an individual security s vari ance, they should be a small contribution to the variance of well diversified portfolios. Thus, there is reason to hope that the APT holds approximately, especially for reasonably large portfolios. Surely, if the residuals are small and or idiosyncratic, the price of an asset cannot be too different from the price predicted from its factor content? To think about these issues, start again from a factor structure, but this time put in a residual, xi E xi 1  f i. i Again take prices of both sides, p xi E xi p 1  p f E mi . i Now, what can we say about the price of the residual p i E mi ? Figure 9.1 illustrates the situation. Portfolios of the factors span a payoff space,thelinefromtheoriginthrough f inthefigure.Thepayoffwewant i to price, xi, is not in that space, since the residual i is not zero. A discount factor f factorsthatpricethefactorsisthelinemperpendiculartof .Theresidual i is orthogonal to the factor space, since it is a regression residual, and to f in particular, E f i 0. This means that f assigns zero price to the residual. But the other discount factors on the m line are not orthogonal to i , so generate nonzero price for the residual i . As we sweep along the line of discount factors m that price the f , in fact, we generate every price that is in the f payoff space prices the factors. The set of all discount",
        "178 9. Factor Pricing Models Figure 9.1. Approximate arbitrage pricing. from to for the residual. Thus, the law of one price does not nail down the price of the residual i and hence does not determine the price or expected return of x i . Limiting Arguments We would like to show that the price of xi has to be close to the price of  f . One notion of close to is that in some appropriate limit the price of i xi converges to the price of  f . Limit means, of course, that you can get i arbitrarily good accuracy by going far enough in the direction of the limit for every  0 there is a  . . . . Thus, establishing a limit result is a way to argue for an approximation. Here is one theorem that seems to imply that the APT should be a good approximation for portfolios that have high R2 on the factors. I state the argument for the case that there is a constant factor, so the constant is in the f space and E i 0. The same ideas work in the less usual case that there is no constant factor, using second moments in place of variance. Theorem: Fix a discount factor m that prices the factors. Then, as var i 0, p xi p  f . i This is easiest to see by just looking at the graph. E i 0 so var i E i2 i 2. Thus, as the size of the i vector in Figure 9.1 gets smaller, xi gets closer and closer to  f . For any fixed m, the induced pricing function i linesperpendiculartothechosenm iscontinuous.Thus,asxi getscloser and closer to  f , its price gets closer and closer to p  f . The factor model is defined as a regression, so var xi var  f var i . i ii",
        "9.4. Arbitrage Pricing Theory APT 179 Thus, the variance of the residual is related to the regression R2, var i var xi The theorem says that as R 2 1, the price of the residual goes to zero. We were hoping for some connection between the fact that the risks are idiosyncratic and factor pricing. Even if the idiosyncratic risks are a large part of the payoff at hand, they are a small part of a well diversified portfolio. The next theorem shows that portfolios with high R 2 do not have to happen by chance; well diversified portfolios will always have this characteristic. Theorem: As the number of primitive assets increases, the R 2 of well diversified portfolios increases to 1. Proof: Start with an equally weighted portfolio xi, the factor decomposition of xp is 1 R2. 1 N N i 1 xi. 1 N 1 N 1 N 1 N xp Going back to the factor decomposition 9.17 for each individual asset xp a  f i a  f Nii NiNiN i i 1 i . Solongasthevarianceofi isbounded,andgiventhefactorassumption i 1 i 1 ap  f p. i 1 p The last equality defines notation p , p , p . But var p var 1 N N i 1 E ij 0, Obviously, the same idea goes through so long as the portfolio spreads some weight on all the new assets, i.e., so long as it is well diversified. P These two theorems can be interpreted to say that the APT holds approximately in the usual limiting sense for either portfolios that nat urally have high R2, or well diversified portfolios in large enough markets. Chamberlain and Rothschild 1983 is a classic treatment. We have only used the law of one price. lim var p 0. N",
        "180 9. Factor Pricing Models Law of One Price Arguments Fail Now, let me pour some cold water on these results. I fixed m and then let other things take limits. The flip side is that for any nonzero residual i , no matter how small, we can pick a discount factor m that prices the factors and assigns any price to xi! As often in mathematics, the order of for all and there exists matters a lot. Theorem: For any nonzero residual i , there is a discount factor that prices the factors f consistentwiththelawofoneprice andthatassignsanydesiredpricein , to the payoff xi. So long as i 0, as we sweep the choice of m along the dashed line of figure 9.1, the inner product of m with i and hence xi varies from to . Thus, for a given size R2 1, or a given finite market, the law of one price says absolutely nothing about the prices of payoffs that do not exactly follow the factor structure. The law of one price says that two ways of constructing the same portfolio must give the same price. If the residual is notexactlyzero,thereisnowayofreplicatingthepayoffxi fromthefactors and no way to infer anything about the price of xi from the price of the factors. I think the contrast between this theorem and those of the last subsec tion accounts for most of the huge theoretical controversy over the APT. For example, Shanken 1985 , Dybvig and Ross 1985 . If you fix m and take limits of N or , the APT gets arbitrarily good. But if you fix N or , as one does in any application, the APT can get arbitrarily bad as you search over possible m. The lesson I learn is that the effort to extend prices from an original set of securities f in this case to new payoffs that are not exactly spanned by the original set of securities, using only the law of one price, is fundamentally doomed. To extend a pricing function, you need to add some restrictions beyond the law of one price. Beyond the Law of One Price: Arbitrage and Sharpe Ratios The approximate APT based on the law of one price fell apart because we could always choose a discount factor sufficiently far out to gener ate an arbitrarily large price for an arbitrarily small residual. But those We can find a well behaved approximate APT if we impose the law of one price and a restriction on the volatility of discount factors, or, equivalently, a bound on the Sharpe ratio achievable by portfolios of the factors and test assets.",
        "9.4. Arbitrage Pricing Theory APT 181 discount factors are surely unreasonable. Surely, we can rule them out, reestablishing an approximate APT, without jumping all the way to fully specified discount factor models such as the CAPM or consumption based model. A natural first idea is to impose the no arbitrage restriction that m must be positive. Graphically, we are now restricted to the solid m line in Figure 9.1. Since that line only extends a finite amount, restricting us to Alas, in applications of the APT as often in option pricing , the arbitrage bounds are too wide to be of much use. The positive discount factor restriction is equivalent to saying if portfolio A gives a higher pay off than portfolio B in every state of nature, then the price of A must be higher than the price of B. Since stock returns and factors are continu ously distributed, not two state distributions as I have graphed for Figure 9.1, there typically are no strictly dominating portfolios, so adding m 0 does not help. A second restriction does let us derive an approximate APT that is useful in finite markets with R2 1. We can restrict the variance and hence the size m E m2 of the discount factor. Figure 9.1 includes a plot of the discount factors with limited variance, size, or length in the geometry of that figure. The restricted range of discount factors produces a restricted range of prices for x i . The restricted range of discount factors gives us upper and lowerpriceboundsforthepriceofxi intermsofthefactorprices.Precisely, the upper and lower bounds solve the problem min or max p xi E mxi s.t. E mf p f , m 0, 2 m A. m m Limiting the variance of the discount factor is of course the same as lim iting the maximum Sharpe ratio mean standard deviation of excess return available from portfolios of the factors and x i . Recall from Chapter 1 that strictly positive m s gives rise to finite upper and lower arbitrage bounds on thepriceofi andhencexi. Thephrasearbitrageboundscomesfromoption pricing, and we will see these ideas again in that context. If this idea worked, it would restore the APT to arbitrage pricing rather than law of one pricing. E Re  m .  Re E m Though a bound on Sharpe ratios or discount factor volatility is not a totally preference free concept, it clearly imposes a great deal less structure than the CAPM or ICAPM which are essentially full general equilibrium models. Ross 1976a included this suggestion in his original APT paper, though it seems to have disappeared from the literature since then in the failed",
        "182 9. Factor Pricing Models effort to derive an APT from the law of one price alone. Ross pointed out that deviations from factor pricing could provide very high Sharpe ratio opportunities, which seem implausible though not violations of the law of one price. Sa Requejo and I 2000 dub this idea good deal pricing, as an extension of arbitrage pricing. Limiting  m rules out good deals as well as pure arbitrage opportunities. Having imposed a limit on discount factor volatility or Sharpe ratio A, then the APT limit does work, and does not depend on the order of for all and there exists. Theorem: As i 0 and R2 1, the price p xi assigned by any discount factor m that satisfies E mf p f , m 0, 2 m A approaches p  f . i 9.5 APT vs. ICAPM The APT and ICAPM stories are often confused. Factor structure can imply factor pricing APT , but factor pricing does not require a factor structure. In the ICAPM there is no presumption that factors f in a pricing model m b f describe the covariance matrix of returns. The factors do not have to be orthogonal or i.i.d. either. High R2 in time series regressions of the returns on the factors may imply factor pricing APT , but again are not necessary ICAPM . The regressions of returns on factors can have low R2 in the ICAPM. Factors such as industry may describe large parts of returns variances but not contribute to the explanation of average returns. The biggest difference between APT and ICAPM for empirical work is in the inspiration for factors. The APT suggests that one start with a statistical analysis of the covariance matrix of returns and find portfolios that characterize common movement. The ICAPM suggests that one start by thinking about state variables that describe the conditional distribution of future asset returns. More generally, the idea of proxying for marginal utility growth suggests macroeconomic indicators, and indicators of shocks to nonasset income in particular. The difference between the derivations of factor pricing models, and in particular an approximate law of one price basis versus a proxy for marginal A factor structure in the covariance of returns or high R 2 in regressions of returns on factors can imply factor pricing APT but factors can price returns without describing their covariance matrix ICAPM . Differing inspiration for factors. The disappearance of absolute pricing.",
        "184 9. Factor Pricing Models returns on factor portfolios, K N . The regression of returns onto factors is Re   Ref . By construction, we have E  0, E Ref 0 a Show that if the price of the errors  is zero, then the APT conclusion holds,  0, E Re  E Ref . b Find the maximum Sharpe ratio you can obtain from portfolios of R e , R ef , as a function of alphas and the error covariance. Intuitively, if the alphas are large and covariance of the errors is small, you can obtain large Sharpe ratios, though not pure arbitrage law of one price opportunities. We can use this argument to assess how small  should be for a given prior on how large a reasonable Sharpe ratio is. You can approach this question by brute force, but I d rather you use discount factor methods. Exploit the fact that the maximum Sharpe ratio you can obtain from a set of securities is proportional to the minimum discount factor volatility among discount factors that price those portfo lios, and the minimum variance discount factor is the one in the payoff space, that is, E Rei  m  m max min . Rei w Re  Rei m:p E mx E m E m Assume there is a risk free rate R f to nail down E m . This will be easiest if you exploit the fact that the residuals  are, by construction, mean zero and orthogonal to factors. Then you can look for discount factors of the form m 1 f  , Rf thatis,m 1 w Ref E Ref  .Youwillfindtwotermsforthe Rf Sharpe ratio, one involving the Sharpe ratio you can get from trading in the factors alone, and an additional one that comes from the alphas. c Consider one excess return Re in isolation, and treat the CAPM as an APT. If  1,  Rem 16 , and the maximum extra Sharpe ratio is 0.5 the market premium , so the total Sharpe ratio should be less than one, find the maximum alpha given that R 2 in the time series regression is 0.8, 0.9, 0.95, 0.99.",
        "Options are a very interesting and useful set of instruments. In thinking about their value, we will adopt an extremely relative pricing approach. Our objective will be to find out a value for the option, taking as given the values of other securities, and in particular the price of the stock on which the option is written and an interest rate. 17.1 Background Definitions and Payoffs Before studying option prices, we need to understand option payoffs. A call option gives you the right, but not the obligation, to buy a stock or other underlying asset for a specified strike price X on or before the expiration date T . European options can only be exercised on the expiration date. American options can be exercised anytime before as well as on the expiration date. A put option gives the right to sell a stock at a specified strike price on or before the expiration date. I will use the 17 Option Pricing A call option gives you the right to buy a stock for a specified strike price on a specified expiration date. ThecalloptionpayoffisCT max ST X,0 . Portfolios of options are called strategies. A straddle a put and a call at the same strike price is a bet on volatility. Options allow you to buy and sell pieces of the return distribution. 313",
        "316 17. Option Pricing Figure 17.2. Payoff diagram for a straddle. Strategies Portfolios of put and call options are called strategies, and have additional interesting properties. Figure 17.2 graphs the payoff of a straddle, which combines a put and call at the same strike price. This strategy pays off if the stock goes up or goes down. It loses money if the stock does not move. Thus the straddle is a bet on volatility. Of course, everyone else understands this, and will bid the put and call prices up until the straddle earns only an equilibrium rate of return. Thus, you invest in a straddle if you think that stock volatility is higher than everyone else thinks it will be. Options allow efficient markets and random walks to operate on the second and higher moments of stocks as well as their overall direction! You can also see quickly that volatility will be a central parameter in option prices. The higher the volatility, the higher both put and call prices. More generally, by combining options of various strikes, you can buy and sell any piece of the return distribution. A complete set of options call options on every strike price is equivalent to complete markets, i.e., it allows you to form payoffs that depend on the terminal stock price in any way; you can form any payoff of the form f ST . Prices: One Period Analysis I use the law of one price existence of a discount factor and no arbitrage existence of a positive discount factor to characterize option prices. The results are: 1 Put call parity: P C S X Rf . 2 Arbi trage bounds, best summarized by Figure 17.4. 3 The proposition that you",
        "17.1. Background 317 should never exercise an American call option early on a stock that pays no dividends. The arbitrage bounds are a linear program, and this procedure can be used to find them in more complex situations where clever identification of arbitrage portfolios may fail. We have a set of interesting payoffs. Now what can we say about their prices their values at dates before expiration? Obviously, p E mx as always. We have learned about x; now we have to think about m. We can start by imposing little structure the law of one price and the absence of arbitrage, or, equivalently, the existence of some discount factor or a positive discount factor. In the case of options, these two principles tell you a good deal about the option price. Put Call Parity The law of one price, or the existence of some discount factor that prices stock, bond, and a call option, allows us to deduce the value of a put in terms of the price of the stock, bond, and call. Consider the following two strategies: 1 Hold a call, write a put, same strike price. 2 Hold stock, promise to pay the strike price X . The payoffs of these two strategies are the same, as shown in Figure 17.3. Equivalently, the payoffs are related by PT CT ST X. Thus, so long as the law of one price holds, the prices of left and right hand sides must be equal. Applying E m to both sides for any m, P C S X R f . The price of ST is S. The price of the payoff X is X Rf . Figure 17.3. Put call parity.",
        "318 17. Option Pricing Arbitrage Bounds If we add the absence of arbitrage, or equivalently the restriction that the discount factor must be positive, we can deduce bounds on the call option price without needing to know the put price. In this case, it is eas iest to cleverly notice arbitrage portfolios situations in which portfolio A dominates portfolio B. Then, either directly from the definition of no arbitrage or from A B, m 0 E mA E mB , you can deduce that the price of A must be greater than the price of B. The arbitrage portfolios are 1. CT 0 C 0.Thecallpayoffispositivesothecallpricemustbe positive. 2. CT ST X C S X Rf.Thecallpayoffisbetterthanthestock payoff the strike price, so the call price is greater than the stock price less the present value of the strike price. 3. CT ST C S. The call payoff is worse than stock payoff because you have to pay the strike price . Thus, the call price is less than stock price. Figure 17.4 summarizes these arbitrage bounds on the call option value. We have gotten somewhere we have restricted the range of the option prices. However, the arbitrage bounds are too large to be of much practical use. Obviously, we need to learn more about the discount factor than pure arbitrage or m 0 will allow. We could retreat to economic models, e.g., use the CAPM or other explicit discount factor model. Option pricing is famous because we do not have to do that. Instead, if we open up dynamic trading the requirement that the discount factor price the stock and bond at every date to expiration it turns out that we can sometimes determine the discount factor and hence the option value precisely. Figure 17.4. Arbitrage bounds for a call option.",
        "17.1. Background 319 Discount Factors and Arbitrage Bounds This presentation of arbitrage bounds is unsettling for two reasons. First, you may worry that you will not be clever enough to dream up dominating portfolios in more complex circumstances. Second, you may worry that we have not dreamed up all of the arbitrage portfolios in this circumstance. Perhaps there is another one lurking out there, which would reduce the unsettlingly large size of the bounds. This presentation leaves us hungry for a constructive technique for finding arbitrage bounds that would be guaranteed to work in general situations, and to find the tightest arbitrage bound. WewanttoknowC E m xc ,wherexc max S X,0 denotes t t,TT T T the call payoff, we want to use information in the observed stock and bond prices to learn about the option price, and we want to impose the absence of arbitrage. We can capture this search with the following problem: max C maxCt Et mxcT s.t.m 0, m St Et mST , 1 Et mR f , 17.1 and the corresponding minimization. The first constraint implements absence of arbitrage. The second and third use the information in the stock and bond price to learn what we can about the option price. Write 17.1 out in state notation,  s m s xc s s.t. m s 0, tT m s This is a linear program a linear objective and linear constraints. In sit uations where you do not know the answer, you can calculate arbitrage bounds and know you have them all by solving this linear program Ritchken 1985 . I do not know how you would begin to check that for every portfolio A whose payoff dominates B, the price of A is greater than the price of B. The discount factor method lets you construct the arbitrage bounds. Early Exercise By applying the absence of arbitrage, we can show quickly that you should never exercise an American call option on a stock that pays no dividends St 1 s s s  s m s ST s ,  s m s Rf .",
        "320 17. Option Pricing before the expiration date. This is a lovely illustration because such a simple principle leads to a result that is not initially obvious. Follow the table: Payoffs CT max ST X,0 ST X Price C S X R f Rf 1 C S X S X is what you get if you exercise now. The value of the call is greater than this value, because you can delay paying the strike, and because exer cising early loses the option value. Put call parity lets us concentrate on call options; this fact lets us concentrate on European call options. 17.2 Black Scholes Formula Our objective, again, is to learn as much as we can about the value of an option, given the value of the underlying stock and bond. The one period analysis led only to arbitrage bounds, at which point we had to start thinking about discount factor models. Now, we allow intermediate trading, which means we really are thinking about dynamic multiperiod asset pricing. The standard approach to the Black Scholes formula rests on explicitly constructing portfolios: at each date we cleverly construct a portfolio of stock and bond that replicates the instantaneous payoff of the option; we reason that the price of the option must equal the price of the replicating portfolio. Instead, I follow the discount factor approach. The law of one price is the same thing as the existence of a discount factor. Thus, rather than construct law of one price replicating portfolios, construct at each date a discount factor that prices the stock and bond, and use that discount factor to price the option. The discount factor approach shows how thinking of the world in terms of a discount factor is equivalent in the result and as easy in the calculation as other approaches. This case shows some of the interest and engineering complexity of continuous time models. Though at each instant the analysis is trivial law of one price, chaining that analysis together over time is not trivial. Write a process for stock and bond, then use to price the option. The Black Scholes formula 17.7 results. You can either solve for the finite horizon discount factor T 0 and find the call option price by taking the expectation C E xC , or you can find a differential equation for 00T0T the call option price and solve it backward.",
        "18 Option Pricing without Perfect Replication 18.1 On the Edges of Arbitrage The beautiful Black Scholes formula launched a thousand techniques for option pricing. The principle of no arbitrage pricing is obvious, but its application leads to many subtle and unanticipated pricing relationships. However, in many practical situations, the law of one price arguments that we used in the Black Scholes formula break down. If options really were redundant, it is unlikely that they would be traded as separate assets. It really is easy to synthesize forward rates from zero coupon bonds, and forward rates are not separately traded or quoted. We really cannot trade continuously, and trying to do so would drown a strategy in transactions costs. As a practical example, at the time of the 1987 stock market crash, several prominent funds were trying to follow portfolio insurance strategies, essentially synthesizing put options by systematically selling stocks as prices declined. During the time of the crash, however, they found that the markets just dried up they were unable to sell as prices plummeted. We can model this situation mathematically as a Poisson jump, a discontinuous movement in prices. In the face of such jumps the option payoff is not perfectly hedged by a portfolio of stock and bond, and cannot be priced as such. Generalizations of the stochastic setup lead to the same result. If the interest rate or stock volatility are stochastic, we do not have securities that allow us to perfectly hedge the corresponding shocks, so the law of one price again breaks down. In addition, many options are written on underlying securities that are not traded, or not traded continually and with sufficient liquidity. Real options in particular the option to build a factory in a particular 327",
        "328 18. Option Pricing without Perfect Replication location are not based on a tradeable underlying security, so the logic behind Black Scholes pricing does not apply. Executives are often forbid den to short stock in order to hedge executive options. Furthermore, trading applications of option pricing formulas seem to suffer a strange inconsistency. We imagine that the stock and bond are per fectly priced and perfectly liquid available for perfect hedging. Then, we search for options that are priced incorrectly as trading opportunities. If the options can be priced incorrectly, why cannot the stock and bond be priced incorrectly? Trading opportunities involve risk, and a theory that pretends they are arbitrage opportunities does not help to quantify that risk. In all of these situations, an unavoidable basis risk creeps in between the option payoff and the best possible hedge portfolio. Holding the option entails some risk, and the value of the option depends on the market price of that risk the covariance of the risk with an appropriate discount factor. Nonetheless, we would like not to give up and go back to the consumption based model, factor models, or other absolute methods that try to price all assets. We are still willing to take as given the prices of lots of assets in determining the price of an option, and in particular assets that will be used to hedge the option. We can form an approximate hedge or portfolio of basis assets closest to the focus payoff, and we can hedge most of the option s risk with that approximate hedge. Then, the uncer tainty about the option value is reduced only to figuring out the price of the residual. In addition, since the residuals are small, we might be able to say a lot about option prices with much weaker restrictions on the discount factor than those suggested by absolute models. Many authors simply add market price of risk assumptions. This leaves the questions, how sensitive are the results to market price of risk assumptions? What are reasonable values for market prices of risk? In this chapter, I survey good deal option price bounds, a tech nique that Jesus Sa Requejo and I 2000 advocated for this situation. The good deal bounds amount to systematically searching over all possi ble assignments of the market price of risk of the residual, constraining the total market price of risk to a reasonable value, and imposing no arbitrage opportunities, to find upper and lower bounds on the option price. It is not equivalent to pricing options with pure Sharpe ratio arguments. Good deal bonds are just the beginning. Finding ways to merge no arbitrage and absolute pricing is one of the most exciting new areas of research. The concluding section of this chapter surveys some alternative and additional techniques.",
        "18.2. One Period Good Deal Bounds 329 18.2 One Period Good Deal Bounds We want to price the payoff xc, for example, xc max ST K,0 for a call option. We have in hand an N dimensional vector of basis payoffs x, whose prices p we can observe, for example the stock and bond. The good deal bound finds the minimum and maximum value of xc by searching over all positive discount factors that price the basis assets and have limited volatility: C max E mx c s .t . p E mx , m 0,  2 m h R f . 18.1 m The corresponding minimization yields the lower bound C. This is a one period discrete time problem. The Black Scholes formula does not apply because you cannot trade between the price and payoff periods. The first constraint on the discount factor imposes the price of the basis assets. We want to do as much relative pricing as possible; we want to extend what we know about the prices of x to price xc, without worrying about where the prices of x come from. The second constraint imposes the absence of arbitrage. This problem without the last constraint yields the arbitrage bounds that we studied in Section 17.1. In most situations, the arbitrage bounds are too wide to be of much use. The last is an additional constraint on discount factors, and the extra content of good deal versus arbitrage bounds. It is a relatively weak restric tion. We could obtain closer bounds on prices with more information about the discount factor. In particular, if we knew the correlation of the discount factorwiththepayoffxc wecouldpricetheoptionalotbetter! Discount factor restrictions often have portfolio implications. As m 0 means that no portfolios priced by m may display an arbitrage opportunity, 2 m h R f means that no portfolio priced by m may have a Sharpe ratio greater than h. Recall E mRe 0 implies E m E Re  m  Re and  1. It is a central advantage of a discount factor approach that we can easily impose both the discount factor volatility constraint and positivity, merging the lessons of factor models and option pricing models. The prices and payoffs generated by discount factors that satisfy both m 0 and  m h Rf domorethanruleoutarbitrageopportunitiesandhighSharperatios. I will treat the case that there is a risk free rate, so we can write E m 1 R f . In this case, it is more convenient to express the volatility constraint as a second moment, so the bound 18.1 becomes C minE m xc s.t. p E mx , E m2 A2, m 0, 18.2 m where A2 1 h2 R f 2. The problem is a standard minimization with two inequality constraints. Hence we find a solution by trying all the",
        "330 18. Option Pricing without Perfect Replication combinations of binding and nonbinding constraints, in order of their ease of calculation. 1 Assume the volatility constraint binds and the positivity constraint is slack. This one is very easy to calculate, since we will find analytic formulas for the solution. If the resulting discount factor m is nonnegative, this is the solution. If not, 2 assume that the volatility constraint is slack and the positivity constraint binds. This is the classic arbitrage bound. Find the minimum variance discount factor that generates the arbitrage bound. If this discount factor satisfies the volatility constraint, this is the solution. If not, 3 solve the problem with both constraints binding. Volatility Constraint Binds, Positivity Constraint Is Slack If the positivity constraint is slack, the problem reduces to C min E m xc s.t. p E mx , E m2 A2. 18.3 m We could solve this problem directly, choosing m in each state with Lagrange multipliers on the constraints. But as with the mean variance frontier, it is much more elegant to set up orthogonal decompositions and then let the solution pop out. Figure 18.1 describes the idea. X denotes the space of payoffs of port folios of the basis assets x, a stock and a bond in the classic Black Scholes setup. Though graphed as a line, X is typically a larger space. We know all pricesinX,butthepayoffxc thatwewishtovaluedoesnotlieinX. Startbydecomposingthefocuspayoffxc intoanapproximatehedgexc and a residual w, x c x c w , xc proj xc X E xcx E xx 1x, 18.4 w x c x c . We know the price of xc . We want to bound the price of the residual w to learn as much as we can about the price of xc. All discount factors that price x that satisfy p E mx lie in the plane through x . As we sweep through these discount factors, we gen erate any price from to for the residual w and hence payoff xc. All positive discount factors m 0 lie in the intersection of the m plane and the positive orthant the triangular region. Discount factors m in this range generate a limited range of prices for the focus payoff the arbi trage bounds. Since second moment defines distance in Figure 18.1, the set of discount factors that satisfies the volatility constraint E m2 A2 lies inside a sphere around the origin. The circle in Figure 18.1 shows the inter section of this sphere with the set of discount factors. This restricted range of discount factors will produce a restricted range of values for the residual w",
        "18.2. One Period Good Deal Bounds 331 Figure 18.1. Construction of a discount factor to solve the one period good deal bound when the positivity constraint is slack. and hence a restricted range of values for the focus payoff x c . In the situation I have drawn, the positivity constraint is slack, since the E m2 A2 circle lies entirely in the positive orthant. We want to find the discount factors in the circle that minimize or max imize the price of the residual w. The more a discount factor m points in the w direction, the larger a price E mw it assigns to the residual. Obvi ously, the discount factors that maximize or minimize the price of w point as much as possible towards and away from w. If you add any movement  orthogonal to w, this increases discount factor volatility without changing the price of w. where m x vw, 18.5 18.6 Hence, the discount factor that generates the lower bound is v A2 E x 2 E w2",
        "332 18. Option Pricing without Perfect Replication is picked to just satisfy the volatility constraint. The bound is C E mxc E x xc vE w2 . 18.7 The upper bound is given by v v. The first term in equation 18.7 is the value of the approximate hedge portfolio, and can be written several ways, including E x xc E x xc E mxc 18.8 for any discount factor m that prices basis assets. To derive 18.8 , remem ber that E xy E x proj y X . The second term in equation 18.7 is the lowest possible price of the residual w consistent with the discount factor volatility bound: vE w2 E vw w E x vw w E mw . For calculations, you can substitute the definitions of x and w in equation 18.7 to obtain an explicit, if not very pretty, formula: C p E xx 1E xxc A2 p E xx 1p E xc2 E xcx E xx 1E xxc . 18.9 The upper bound C is the same formula with a sign in front of the square root. Using 18.5 , check whether the discount factor is positive in every state of nature. If so, this is the good deal bound, and the positivity constraint is slack. If not, proceed to the next step. If you prefer an algebraic and slightly more formal argument, start by noticing that any discount factor that satisfies p E mx can be decomposed as m x vw , where E x w E x  E w . Check these properties from the defi nition of w and ; this is just like R R wRe n. Our minimization problem is then minE mxc s.t. E m2 A2, v, minE x vw  xc w s.t.E x 2 v2E w2 E 2 A2, v, minE x xc vE w2 s.t. E x 2 vE w2 E 2 A2. v, The solution is  0 and v A2 E x 2 E w2 .",
        "334 18. Option Pricing without Perfect Replication The inner minimization yields the same first order conditions 18.10 . Plug ging those first order conditions into the outer maximization of 18.12 and simplifying, we obtain  xc  x 2 C maxE   p A2. 18.13 2 You can search numerically over ,  to find the solution to this problem. The upper bound is found by replacing max with min and replacing  0 with  0. Positivity Binds, Volatility Is Slack If the volatility constraint is slack and the positivity constraint binds, the problem reduces to C minE mxc s.t. p E mx , m 0. 18.14 m These are the arbitrage bounds. We found these bounds in Chapter 17 for a call option by being clever. If you cannot be clever, 18.14 is a linear program. We still have to check that the discount factor volatility constraint can be satisfied at the arbitrage bound. Denote the lower arbitrage bound by Cl . The minimum variance second moment discount factor that generates thearbitrageboundCl solves E m2 min minE m2 s.t p E m x , m 0. m Cl xc Using the same conjugate method, this problem is equivalent to E m2 min max E xc v x 2 2v p 2Cl . v, Again, search numerically for v,  to solve this problem. If E m2 min A, Cl isthesolutiontothegood dealbound;ifnot,weproceedwiththecase that both constraints are binding described above. Application to the Black Scholes Environment with No Dynamic Hedging The natural first exercise with this technique is to see how it applies in the Black Scholes world. Keep in mind, this is the Black Scholes world , 0 2",
        "18.2. One Period Good Deal Bounds 335 Figure 18.2. Good deal option price bounds as a function of stock price. Options have three months to expiration and strike price K 100. The bounds assume no trading until expira tion, and a discount factor volatility bound h 1.0 corresponding to twice the market Sharpe ratio. The stock is lognormally distributed with parameters calibrated to an index option. with no intermediate trading; compare the results to the arbitrage bounds, not to the Black Scholes formula. Figure 18.2, taken from Cochrane and Sa Requejo 2000 , presents the upper and lower good deal bounds for a call option on the S P500 index with strike price K 100, and three months to expiration. We used parameter values E R 13 ,  R 16 for the stock index return and a risk free rate Rf 5 . The discount factor volatility constraint is twice the historical market Sharpe ratio, h 2 E R R f  R 1.0. To take the expectations required in the formula, we evaluated integrals against the lognormal stock distribution. The figure includes the lower arbitrage bounds C 0, C K R f . The upper arbitrage bound states that C S, but this 45 line is too far up to fit on the vertical scale and still see anything else. As in many practical situations, the arbitrage bounds are so wide that they are of little use. The upper good deal bound is much tighter than the upper arbitrage bound. For example, if the stock price is 95, the entire range of option prices between the upper bound of 2 and the upper arbitrage bound of 95 is ruled out. The lower good deal bound is the same as the lower arbitrage bound for stock prices less than about 90 and greater than about 110. In this range, the positivity constraint binds and the volatility constraint is slack.",
        "336 18. Option Pricing without Perfect Replication This range shows that it is important to impose both volatility and positiv ity constraints. Good deal bounds are not just the imposition of low Sharpe ratios on options. I emphasize it because this point causes a lot of confusion. The volatility bound alone admits negative prices. A free out of the money call option is like a lottery ticket: it is an arbitrage opportunity, but its expected return standard deviation ratio is terrible, because the stan dard deviation is so high. A Sharpe ratio criterion alone will not rule it out. In between 90 and 110, the good deal bound improves on the lower arbitrage bound. It also improves on a bound that imposes only the volatility constraint. In this region, both positivity and volatility con straints bind. This fact has an interesting implication: Not all values outside the good deal bounds imply high Sharpe ratios or arbitrage opportunities. Such values might be generated by a positive but highly volatile discount fac tor, and generated by another less volatile but sometimes negative discount factor, but no discount factor generates these values that is simultaneously nonnegative and respects the volatility constraint. It makes sense to rule out these values. If we know that an investor will invest in any arbitrage opportunity or take any Sharpe ratio greater than h, then we know that his unique marginal utility satisfies both restrictions. He would find a utility improving trade for values outside the good deal bounds, even though those values may not imply a high Sharpe ratio, an arbitrage opportunity, or any other simple portfolio interpretation. The right thing to do is to intersect restrictions on the discount factor. Simple portfolio interpretations, while historically important, are likely to fall by the wayside as we add more discount factor restrictions or intersect simple ones. 18.3 Multiple Periods and Continuous Time Now, on to the interesting case. Option pricing is all about dynamic hedging, even if imperfect dynamic hedging. Good deal bounds would be of little use if we could only apply them to one period environments. The Bounds Are Recursive The central fact that makes good deal bounds tractable in dynamic environ ments is that the bounds are recursive. Today s bound can be calculated as the minimum price of tomorrow s bound, just as today s option price can be calculated as the value of tomorrow s option price.",
        "18.4. Extensions, Other Approaches, and Bibliography 345 not know how to spread the loading of d across the multiple sources of risk dw whose risk prices we do not observe. Equivalently, we do not know how to optimally spread the total market price of risk across the elements of dw. Thus, in general we cannot use the integration approach solve the discount factor forward to find the bound by T C Et sxscds Et TxTc . t s tt t However, if there is only one shock dw, then we do not have to worry about how the loading of d spreads across multiple sources of risk. v can be determined simply by the volatility constraint. In this special case, dw and Cw arescalars.Henceequation 18.25 simplifiesasfollows: Theorem: In the special case that there is only one extra noise dw driving the V process, we can find the lower bound discount factor directly from d I used this characterization to solve for the case of a nontraded underly ing in the last section. In some applications, the loading of d on multiple shocks dw may be constant over time. In such cases, one can again construct the discount factor and solve for bounds by possibly numerical integration, avoiding the solution of a partial differential equation. 18.4 Extensions, Other Approaches, and Bibliography The roots of the good deal idea go a long way back. Ross 1976a bounded APT residuals by assuming that no portfolio can have more than twice the market Sharpe ratio, and I used the corresponding idea that discount fac tor volatility should be bounded to generate a robust approximate APT in Section 9.4. Good deal bounds apply the same idea to option pay offs. However, the good deal bounds also impose positive discount factors, and this constraint is important in an option pricing context. We also study dynamic models that chain discount factors together as in the option pricing literature. The one period good deal bound is the dual to the Hansen Jagannathan 1991 bound with positivity Hansen and Jagannathan study the minimum variance of positive discount factors that correctly price a given set of assets. The good deal bound interchanges the position of the option pricing equation and the variance of the discount factor. The rdt  1 dz A2  1 dw. 18.31 SSS SSS",
        "346 18. Option Pricing without Perfect Replication techniques for solving the bound, therefore, are exactly those of the Hansen Jagannathan bound in this one period setup. There is nothing magic about discount factor volatility. This kind of problem needs weak but credible discount factor restrictions that lead to tractable and usefully tight bounds. Several other similar restrictions have been proposed in the literature. 1 Levy 1985 and Constantinides 1998 assume that the discount fac tor declines monotonically with a state variable; marginal utility should decline with wealth. 2 The good deal bounds allow the worst case that marginal utility growth is perfectly correlated with a portfolio of basis and focus assets. In many cases one could credibly impose a sharper limit than 1  1 on this correlation to obtain tighter bounds. 3 Bernardo and Ledoit 2000 use the restriction a m b to sharpen the no arbitrage restriction m 0. They show that this restriction has a beautiful portfolio interpretation a m b corresponds to limited gain loss ratios just as  m E m corresponds to limited Sharpe ratios. Define Re max Re,0 and Re min Re,0 asthe gainsand losses of an excess return R e . Then, Re sup m max min . 18.32 Re Re Re m: 0 E mRe inf m The sup and inf ignore measure zero states. This is exactly analogous to E Re  m max min Re Re  Re m: 0 E mRe E m and hints at an interesting restatement of asset pricing theory in L1 with sup norm rather than L2 with second moment norm. Since m a, the call option price generated by this restriction in a one period model is strictly greater than the lower arbitrage bound generated by m 0; as in this case, the gain loss bound can improve on the good deal bound. 4 Bernardo and Ledoit also suggest a m y b, where y is an explicit discount factor model such as the consumption based model or CAPM, as a way of imposing a weak implication of that particular model. These alternatives are really not competitors. Add all the discount factor restrictions that are appropriate and useful for a given problem. This exercise seems to me a strong case for discount factor methods as opposed to portfolio methods. The combination of positivity and volatil ity constraints on the discount factor leads to a sharper bound than the intersection of no arbitrage and limited Sharpe ratios. I do not know of a",
        "Problems 347 simple portfolio characterization of the set of prices that are ruled out by the good deal bound when both constraints bind. The same will be true as we add, say, gain loss restrictions, monotonicity restrictions, etc. In continuous time, option pricing and term structure problems increasingly feature assumptions about the market price of risk of the non traded shocks. The good deal bounds treat these rather formally; they choose the market prices of risks at each instant to minimize or maximize the option price subject to a constraint that the total market price of risk is less than a reasonable value, compared to the Sharpe ratios of other trading opportunities. One need not be this formal. Many empirical implementa tions of option pricing and term structure models feature unbelievable sizes and time variation in market prices of risk. Just imposing sensible values for the market prices of risk and trying on a range of sensible values may be good enough for many practical situations. The continuous time treatment has not yet been extended to the impor tant case of jumps rather than diffusion processes. With jumps, both the positivity and volatility constraints will bind. 1. Prove 18.32 , Problems Chapter 18 Re sup m max min . Re Re Re m:0 E mRe inf m Start with a finite state space. 2. Binomial models are very popular in option pricing. This simple prob lem illustrates the technique. A stock currently selling at S will either rise to ST uS with probability u or decline to ST dS with probability d , paying no dividends in the interim. There is a constant gross interest rate R f . a Find a discount factor that prices stock and bond. This means, find its value in each state of nature. b Use the discount factor to price a call option one step before expiration. Express your results as an expected value using risk neutral probabilities. c Do the same thing two steps before expiration. d Cox, Ross, and Rubinstein 1979 derive these formulas by setting up a hedge portfolio of stocks and bonds, and finding portfolio weights to exactly synthesize the option. Rederive your result with this method.",
        "21.2. New Models 477 gain loss ratio available from the set of assets under consideration. Thus, the theorem really does not apply to any set of arbitrage free payoffs. The example m 1 R is a positive discount factor that prices a single asset return 1 E R 1 R , but does not necessarily satisfy restriction 21.17 . For high R, we can have very negative ln1 R. This example only works if the distribution of R is limited to R e. How the Model Works As the Campbell Cochrane model is blatantly and proudly reverse engineered to surmount and here, to illustrate the known pitfalls of representative consumer models, the Constantinides Duffie model is reverse engineered to surmount the known pitfalls of idiosyncratic risk models. Idiosyncratic risk stories face two severe challenges, as explained in Section 21.1. First, the basic pricing equation applies to each individual. If we are to have low risk aversion and power utility, the required huge volatility of consumption is implausible for any individual. Second, if you add idiosyncratic risk uncorrelated with asset returns, it has no effect on pricing implications. Constantinides and Duffie s central contribution is very cleverly to solve the second problem. In idiosyncratic risk models, we cannot specify individual consumption directly as we do in representative agent endowment economies, and go straight to finding prices. The endowment economy structure says that aggregate consumption is fixed, and prices have to adjust so that consumers are happy consuming the given aggregate consumption stream. However, individuals can always trade consumption with each other. The whole point of assets is that one individual can sell another some consumption, in exchange for the promise of some consumption in return in the next period. We have to give individuals idiosyncratic income shocks, and then either check that they do not want to trade away the idiosyncratic shock, or find the equilibrium consumption after they do so. Early idiosyncratic risk papers found quickly how clever the consumers could be in getting rid of the idiosyncratic risks by trading the existing set of assets. Telmer 1993 and Lucas 1994 found that if you give people transitory but uninsured income shocks, they respond by borrowing and lending or by building up a stock of savings. As in the classic permanent income model, consumption then only responds by the interest rate times the change in permanent income, and at low enough interest rates, not at all. Self insurance through storage removes the extra income volatility and we are back to smooth individual consumption and an equity premium puzzle. Constantinides and Duffie get around this problem by making the idiosyncratic shocks permanent. The normal it shocks determine"
    ],
    "Topic 7": [
        "Our first task in bringing an asset pricing model to data is to estimate the free parameters: the  and  in m  ct 1 ct  , or the b in m b f . Then we want to evaluate the model. Is it a good model or not? Is another model better? Statistical analysis helps us to evaluate a model by providing a distribution theory for numbers such as parameter estimates that we create from the data. A distribution theory pursues the following idea: Suppose that we generate artificial data over and over again from a statistical model. For example, we could specify that the market return is an i.i.d. normal random variable, and a set of stock returns is generated by Rei   Rem i. After picking tiitt valuesforthemeanandvarianceofthemarketreturnandthei,i,2 i , we could ask a computer to simulate many artificial data sets. We can repeat our statistical procedure in each of these artificial data sets, and graph the distribution of any statistic which we have estimated from the real data, i.e., the frequency that it takes on any particular value in our artificial data sets. In particular, we are interested in a distribution theory for the esti mated parameters, to give us some sense of how much the data really has to say about their values; and for the pricing errors, which helps us to judge whether pricing errors we observe are just bad luck of one particular his torical accident or if they indicate a failure of the model. We also will want to generate distributions for statistics that compare one model to another, or that provide other interesting evidence, to judge how much sample luck affects those calculations. The statistical methods I discuss in this part achieve these ends. They give methods for estimating free parameters, they provide a distribution theory for those parameters, and they provide distributions for statistics that we can use to evaluate models, most often a quadratic form of pricing errors in the form  V 1 . I start by focusing on the GMM approach. The GMM approach is a natu ral fit for a discount factor formulation of asset pricing theories, since we just use sample moments in the place of population moments. As you will see, there is no singular GMM estimate and test. GMM is a large canvas and a big set of paints and brushes; a flexible tool for doing all kinds of sensible and, unless you are careful, not so sensible things to the data. Then I con sider traditional regression tests, naturally paired with expected return beta statements of factor models, and their maximum likelihood formalization. I emphasize the fundamental similarities between these three methods, as I emphasized the similarity between p E mx , expected return beta models, and mean variance frontiers. A concluding chapter highlights some of the differences between the methods, as I contrasted p E mx and beta or mean variance representations of the models. 187",
        "i.e., to calculate 1 T T t 1 pt and 1 T T m datat 1, parameters xt 1 . 10.2 t 1 10 GMM in Explicit Discount Factor Models The basic idea in the GMM approach is very straightforward. The asset pricing model predicts E pt E m datat 1, parameters xt 1 . 10.1 The most natural way to check this prediction is to examine sample averages, GMM estimates the parameters by making the sample averages in 10.2 as close to each other as possible. It seems natural, before evaluating a model, to pick parameters that give it its best chance. GMM then works out a distribution theory for the estimates. This distribution theory is a generalization of the simplest exercise in statistics: the distribution of the sample mean. Then, it suggests that we evaluate the model by looking at how close the sample averages of price and discounted payoff are to each other, or equivalently by looking at the pricing errors. GMM gives a sta tistical test of the hypothesis that the underlying population means are in fact zero. 189",
        "10.1. The Recipe 191 Given values for the parameters b, we could construct a time series on ut and look at its mean. Define g b as the sample mean of the ut errors, when the parameter T vector is b in a sample of size T: 1 T g b ut b ET ut b ET mt 1 b xt 1 pt . T T t 1 The second equality introduces the handy notation ET for sample means, 1 T T t 1 It might make more sense to denote these estimates E and g. However, Hansen s T subscript notation is so widespread that doing so would cause more confusion than it solves. The first stage estimate of b minimizes a quadratic form of the sample mean of the errors, b argmin g b Wg b 1 b T T for some arbitrary matrix W often, W I . This estimate is consistent and asymptotically normal. You can and often should stop here, as I explain below. Using b , form an estimate S of 1 2 bTT b isaconsistent,asymptoticallynormal,andasymptoticallyefficientesti mate of the parameter vector b. Efficient means that it has the smallest variance covariance matrix among all estimators that set different linear combinations of g b to zero or all choices of weighting matrix W . The T ET . j Eut b ut j b . mate. Formasecond stageestimateb usingthematrixS inthequadratic form, S Below I discuss various interpretations of and ways to construct this esti 2  b2 argmin g b S 1g b . variance covariance matrix of b var b 1 d S 1d 1, 2T 2 is 10.5",
        "10.2. Interpreting the GMM Procedure 193 Pricing Errors The moment conditions are b ET mt 1 b xt 1 ET pt . Thus, each moment is the difference between actual ET p and predicted ET mx price, or pricing error. What could be more natural than to pick parameters so that the model s predicted prices are as close as possible to the actual prices, and then to evaluate the model by how large these pricing errors are? In the language of expected returns, the moments g b are propor T tional to the difference between actual and predicted returns: Jensen s alphas, or the vertical distance between the points and the line in a graph of actual vs. predicted average returns such as Figure 2.4. To see this fact, recall that 0 E mR e can be translated to a predicted expected return, . g T E R Therefore, we can write the pricing error as e cov m,Re E m g b E mRe E m E Re cov m,Re E m 1 actual mean return predicted mean return . Rf If we express the model in expected return beta language, E Rei   , then the GMM objective is proportional to the Jensen s alpha measure of mis pricing, g b 1i. Rf First Stage Estimates If we could, we would pick b to make every element of g b 0 to have T the model price assets perfectly in sample. However, there are usually more moment conditions returns times instruments than there are parameters. There should be, because theories with as many free parameters as facts moments arevacuous.Thus,wechoosebtomakethepricingerrorsg b T as small as possible, by minimizing a quadratic form, min g b Wg b . 10.6 TT W is a weighting matrix that tells us how much attention to pay to each moment, or how to trade off doing well in pricing one asset or linear b ii",
        "194 10. GMM in Explicit Discount Factor Models combination of assets versus doing well in pricing another. In the com mon case W I , GMM treats all assets symmetrically, and the objective is to minimize the sum of squared pricing errors. Thesamplepricingerrorg b maybeanonlinearfunctionofb.Thus, T you may have to use a numerical search to find the value of b that minimizes the objective in 10.6 . However, since the objective is locally quadratic, the search is usually straightforward. Second Stage Estimates: Why S 1? What weighting matrix should you use? The weighting matrix directs GMM to emphasize some moments or linear combinations of moments at the expense of others. You might start with W I , i.e., try to price all assets equally well. A W that is not the identity matrix can be used to offset dif ferences in units between the moments. You might also start with different elementsonthediagonalofW ifyouthinksomeassetsaremoreinteresting, more informative, or better measured than others. The second stage estimate picks a weighting matrix based on statisti cal considerations. Some asset returns may have much more variance than others. For those assets, the sample mean g ET mt Rt 1 will be a much T less accurate measurement of the population mean E mR 1 , since the sam ple mean will vary more from sample to sample. Hence, it seems like a good idea to pay less attention to pricing errors from assets with high variance of mt Rt 1. One could implement this idea by using a W matrix composed of inverse variances of ET mt Rt 1 on the diagonal. More generally, since asset returns are correlated, one might think of using the covariance matrix of ET mt Rt 1 . This weighting matrix pays most attention to linear combinations of moments about which the data set at hand has the most information. This idea is exactly the same as heteroskedasticity and cross correlation corrections that lead you from OLS to GLS in linear regressions. T T t 1 t t AsT , T j T 1,so t t 1 t t 1 ET ut 1 is the variance of a sample The covariance matrix of g mean. Exploiting the assumption that E ut 0, and that ut is stationary so E u1u2 E utut 1 depends only on the time interval between the two u s, we have 1 T var g var ut 1 T 1 T2 TE uu T 1 E uu E uu . var g 1 T E uu 1S. T tt j T j",
        "10.2. Interpreting the GMM Procedure 195 The last equality denotes S, known for other reasons as the spectral density matrix at frequency zero of ut . Precisely, S so defined is the variance covariance matrix of the gT for fixed b. The actual variance covariance matrix of gT must take into account the fact that we chose b to set a linear combination of the gT to zero in each sample. I give that formula below. The point here is heuristic. This fact suggests that a good weighting matrix might be the inverse of S. In fact, Hansen 1982 shows formally that the choice W S 1, S is the statistically optimal weighting matrix, meaning that it produces estimates with lowest asymptotic variance. You may be familiar with the formula  u T for the standard devi ation of a sample mean. This formula is a special case that holds when the u s are uncorrelated over time. If E u u 0, j 0, then the previous t equation reduces to var ut 1 t t t j 1 T Tt 1 T T t t j j E u u , 1 var u E uu . This is probably the first statistical formula you ever saw the variance of the sample mean. In GMM, it is the last statistical formula you will ever see as well. GMM amounts to just generalizing the simple ideas behind the distribution of the sample mean to parameter estimation and general statistical contexts. The first and second stage estimates should remind you of standard linear regression models. You start with an OLS regression. If the errors are not i.i.d., the OLS estimates are consistent, but not efficient. If you want efficient estimates, you can use the OLS estimates to obtain a series of residuals, estimate a variance covariance matrix of residuals, and then do GLS. GLS is also consistent and more efficient, meaning that the sampling variation in the estimated parameters is lower. Standard Errors The formula for the standard error of the estimate, var b 1 d S 1d 1, 10.7 can be understood most simply as an instance of the delta method that the asymptotic variance of f x is f x 2 var x . Suppose there is only one 2T",
        "10.3. Applying GMM 197 can capture a lot. We often test asset pricing models using returns, in which case the moment conditions are Emt 1 b Rt 1 1 0. It is common to add instruments as well. Mechanically, you can multiply both sides of by any variable zt observed at time t before taking unconditional expecta tions, resulting in Expressing the result in E 0 form, or, emphasizing notation, E mt 1 b Rt 1 1 zt 0, the managed portfolio interpretation E mt 1 b Rt 1 zt 1 zt 0. and p 10.9 E mx 1 Et mt 1 b Rt 1 E zt E mt 1 b Rt 1zt . 0 E mt 1 b Rt 1 1 zt . 10.8 We can do this for a whole vector of returns and instruments, multiplying each return by each instrument. For example, if we start with two returns R RaRb and one instrument z, equation 10.8 looks like mt 1 b Ra 1 0 t 1 m b Rb 1 0 E t 1 t 1 . a m t 1 b R t 1 z t z t 0 m b Rbz z 0 t 1 t 1 t t Using the Kronecker product meaning multiply every element by every other element and including the constant 1 as the first element of the instrument vector zt , we can denote the same relation compactly by Forecast Errors and Instruments The asset pricing model says that, although expected returns can vary across time and assets, expected discounted returns should always be the same, 1. The error ut 1 mt 1 Rt 1 1 is the ex post discounted return. ut 1 mt 1Rt 1 1 represents a forecast error. Like any forecast error, ut 1 should be conditionally and unconditionally mean zero. In an econometric context, z is an instrument because it should be uncorrelated with the error ut 1. E ztut 1 is the numerator of a regres sion coefficient of ut 1 on zt ; thus adding instruments basically checks that the ex post discounted return is unforecastable by linear regressions.",
        "198 10. GMM in Explicit Discount Factor Models If an asset s return is higher than predicted when zt is unusually high, butnotonaverage,scalingbyzt willpickupthisfeatureofthedata.Then, the moment condition checks that the discount rate is unusually low at such times, or that the conditional covariance of the discount rate and asset return moves sufficiently to justify the high conditionally expected return. As in Section 8.1, the addition of instruments is equivalent to adding the returns of managed portfolios to the analysis, and is in principle able to capture all of the model s predictions. Stationarity and Distributions The GMM distribution theory does require some statistical assumptions. Hansen 1982 and Ogaki 1993 cover them in depth. The most impor tant assumption is that m,p, and x must be stationary random variables. Stationary is often misused to mean constant, or i.i.d. The statistical def initionofstationarityisthatthejointdistributionofxt,xt j dependsonlyon j and not on t . Sample averages must converge to population means as the sample size grows, and stationarity is necessary for this result. Assuring stationarity usually amounts to a choice of sensible units. For example, though we could express the pricing of a stock as pt Et mt 1 dt 1 pt 1 , it would not be wise to do so. For stocks, p and d rise over time and so are typically not stationary; their unconditional means are not defined. It is bettertodividebypt andexpressthemodelas 1 Em dt 1 pt 1 Em R . tt 1p tt 1t 1 t The stock return is plausibly stationary. Dividing by dividends is an alternative and, I think, underutilized way to achieve stationarity at least for portfolios, since many individual stocks do not pay regular dividends : pt Et mt 1 1 pt 1 dt 1 . dt dt 1 dt Now we map 1 pt 1 dt 1 dt 1 dt into xt 1 and pt dt into pt . This formulation allows us to focus on prices rather than one period returns. Bonds are a claim to a dollar, so bond prices and yields do not grow over time. Hence, it might be all right to examine with no transformations. pb E m 1 t t 1",
        "10.3. Applying GMM 199 Stationarity is not always a clear cut question in practice. As variables become less stationary, as they experience longer swings in a sample, the asymptotic distribution can become a less reliable guide to a finite sample distribution. For example, the level of nominal interest rates is surely a stationary variable in a fundamental sense: we have observations near 6 as far back as ancient Babylon, and it is about 6 again today. Yet it takes very long swings away from this unconditional mean, moving slowly up or down for even 20 years at a time. Therefore, in an estimate and test that uses the level of interest rates, the asymptotic distribution theory might be a bad approximation to the correct finite sample distribution theory. This is true even if the number of data points is large. Ten thousand data points measured every minute are a smaller data set than 100 data points measured every year. In such a case, it is particularly important to develop a finite sample distribution by simulation or bootstrap, which is easy to do given today s computing power. It is also important to choose test assets in a way that is stationary. For example, individual stocks change character over time, increasing or decreasing size, exposure to risk factors, leverage, and even nature of the business. For this reason, it is common to sort stocks into portfolios based on characteristics such as betas, size, book market ratios, industry, and so forth. The statistical characteristics of the portfolio returns may be much more con stant than the characteristics of individual securities, which float in and out of the various portfolios. One can alternatively include the characteristics as instruments. Many econometric techniques require assumptions about distributions. As you can see, the variance formulas used in GMM do not include the usual assumptions that variables are i.i.d., normally distributed, homoskedastic, etc. You can put such assumptions in if you want to we will see how below, and adding such assumptions simplifies the formulas and can improve the small sample performance when the assumptions are justified but you do not have to add these assumptions.",
        "202 11. GMM: General Formulas and Applications 11.1 General GMM Formulas The general GMM estimate: aTg b 0. T Distribution of b: Distribution of g T cov b ad 1aSa ad 1 . b : Tcov g b I d ad 1a S I d ad 1a . T T The optimal estimate uses a d S 1. In this case, and Tcov b d S 1d 1, Tcovg b S d d S 1d 1d , T TJT Tg b S 1g b 2 moments parameters . TT An analogue to the likelihood ratio test, TJ restricted TJ unrestricted 2 . T T Number of restrictions GMM procedures can be used to implement a host of estimation and testing exercises. Just about anything you might want to estimate can be written as a special case of GMM. To do so, you just have to remem ber or look up a few very general formulas, and then map them into your case. Express a model as E f xt,b 0. Everything is a vector: f can represent a vector of L sample moments, xt can be M data series, b can be N parameters. f x t , b generalizes the errors ut b in the last chapter. Definition of the GMM Estimate We estimate parameters b to set some linear combination of sample means of f to zero, b:setaTg b 0, 11.1 T",
        "11.1. General GMM Formulas where 203 by min g b Wg TT b , the first order conditions are g b T g T b Wg b 0, T 1 T T t 1 f xt,b , andaT isamatrixthatdefineswhichlinearcombinationofg b willbeset T to zero. This defines the GMM estimate. If there are as many moments as parameters, you will set each moment to zero; when there are fewer parameters than moments, 11.1 captures the natural idea that you will set some moments, or some linear combination of moments, to zero in order to estimate the parameters. The minimization of the last chapter is a special case. If you estimate b which is of the form 11.1 with aT gT bW . The general GMM proce dure allows you to pick arbitrary linear combinations of the moments to set to zero in parameter estimation. Standard Errors of the Estimate Hansen 1982, Theorem 3.1 tells us that the asymptotic distribution of the GMM estimate is where f g b T b b N 0, ad 1aSa ad 1 , 11.2 d E xt,b T b b precisely, d is defined as the population moment in the first equality, which we estimate in sample by the second equality , where and where S In practical terms, this means to use a plimaT, var b 1 ad 1aSa ad 1 T j Ef xt,b ,f xt j,b . 11.3 11.4 as the covariance matrix for standard errors and tests. As in the last chapter, you can understand this formula as an application of the delta method.",
        "204 11. GMM: General Formulas and Applications Distribution of the Moments Hansen sLemma4.1givesthesamplingdistributionofthemomentsg b : T Tg b N 0, I d ad 1a S I d ad 1a . 11.5 T As we have seen, S would be the asymptotic variance covariance matrix of sample means, if we did not estimate any parameters. The I d ad 1a terms accountforthefactthatineachsamplesomelinearcombinationsofgT are set to zero in order to estimate parameters. Thus, this variance covariance matrix is singular. 2 Tests It is natural to use the distribution theory for gT to see if the gT too big. Equation 11.5 suggests that we form the statistic A sum of squared standard normals is distributed 2. So this statistic should have a 2 distribution. It does, but with a hitch: The variance covariance matrix is singular, so you have to pseudo invert it. For example, you can perform an eigenvalue decomposition Q Q and then invert only the nonzero eigenvalues. Also, the 2 distribution has degrees of freedom given by the number of nonzero linear combinations of gT , the number of moments less the number of estimated parameters. Efficient Estimates The theory so far allows us to estimate parameters by setting any linear combination of moments to zero. Hansen shows that one particular choice is statistically optimal, a d S 1. 11.7 This choice is the first order condition to min b g b S 1g b that we stud TT ied in the last chapter. With this weighting matrix, the standard error formula 11.4 reduces to T b b N0, d S 1d 1 . 11.8 This is Hansen s Theorem 3.2. The sense in which 11.7 is efficient is that the sampling variation of the parameters for arbitrary a matrix, 11.4 , equals the sampling variation of the efficient estimate in 11.8 plus a positive semidefinite matrix. Thus, it is efficient in the class of estimates thatsetdifferentlinearcombinationsofthemomentsgT tozero.Estimators based on other moments may be more efficient still. 1  1 1  are jointly 11.6 Tg b I d ad a S I d ad a g b TT",
        "11.1. General GMM Formulas 205 With the optimal weights 11.7 , the variance of the moments 11.5 simplifies to 1 S d d S 1d 1d . 11.9 cov g T T We can use this matrix in a test of the form 11.6 . However, Hansen s Lemma 4.2 tells us that there is an equivalent and simpler way to construct this test, Tg b S 1g b 2 moments parameters . 11.10 TT This result is nice since we get to use the already calculated and nonsingular S 1. To derive 11.10 , factor S CC and then find the asymptotic b using 11.5 . The result is var TC 1g b I C 1d d S 1d 1d C 1 . covariance matrix of C 1g T T This is an idempotent matrix of rank moments parameters, so 11.10 follows. Alternatively, note that S 1 is a pseudo inverse of the second stage cov g a pseudo inverse times cov g should result in an idempotent matrixofthesamerankascov g , T TT I S 1d d S 1d 1d . I S 1d d S 1d 1d I S 1d d S 1d 1d I S 1d d S 1d 1d . This derivation not only verifies that JT has the same distribution as g cov g 1g , but that they are numerically the same in every sample. I emphasize that 11.8 and 11.10 only apply to the optimal choice of weights, 11.7 . If you use another set of weights, as in a first stage estimate, you must use the general formulas 11.4 and 11.5 . Model Comparisons You often want to compare one model to another. If one model can be expressed as a special or restricted case of the other or unrestricted model, we can perform a statistical comparison that looks very much like a likelihood ratio test. If we use the same S matrix usually that of the unrestrictedmodel therestrictedJT mustrise.Butiftherestrictedmodel is really true, it should not rise much. How much? TJT restricted TJT unrestricted 2 of restrictions . S 1cov g S 1 S d d S 1d 1d T Then, check that the result is idempotent, TTT",
        "206 11. GMM: General Formulas and Applications This is a 2 difference test, due to Newey and West 1987a , who call it the D test. 11.2 Testing Moments You may want to see how well a model does on particular moments or particular pricing errors. For example, the celebrated small firm effect states that an unconditional CAPM m a bR W , no scaled factors does badly in pricing the returns on a portfolio that always holds the smallest 1 10th or 1 20th of firms in the NYSE. You might want to see whether a new model prices the small firm returns well. The standard error of pricing errors also allows you to add error bars to a plot of predicted versus actual mean returns such as Figure 2.4, or to compute standard errors for other diagnostics based on pricing errors. WehavealreadyseenthatindividualelementsofgT measurethepricing errors. Thus, the sampling variation of gT given by 11.5 provides exactly the standard error we are looking for. You can use the sampling distribution of gT to evaluate the significance of individual pricing errors, to construct a t test for a single gT , such as small firms or a 2 test for groups of gT , such as small firms instruments . Alternatively, you can use the 2 difference approach. Start with a gen eral model that includes all the moments, and form an estimate of the spectral density matrix S . Now set to zero the moments you want to test, and denote g b the vector of moments, including the zeros s for smaller . sT Choose bs to minimize g bs S 1 g bs using the same weighting matrix S . sT sT The criterion will be lower than the original criterion g b S 1g b , since TT there are the same number of parameters and fewer moments. But, if the moments we want to test truly are zero, the criterion should not be that much lower. The 2 difference test applies, Tg b S 1g b Tg b S 1g b 2 eliminated moments . T T sTs sTs Of course, do not fall into the obvious trap of picking the largest of 10 pricing errors and noting it is more than two standard deviations from zero. The distribution of the largest of 10 pricing errors is much wider than the distribution of a single one. To use this distribution, you have to pick which pricing error you are going to test before you look at the data. How to test one or a group of pricing errors. 1 Use the formula for var g . 2 A 2 difference test. T",
        "11.4. Using GMM for Regressions 207 11.3 Standard Errors of Anything by Delta Method One quick application illustrates the usefulness of the GMM formulas. Often, we want to estimate a quantity that is a nonlinear function of sample means, b  E xt   . In this case, the formula 11.2 reduces to . var b 1 d cov x,x d 11.11 TTd tt jd j The formula is very intuitive. The variance of the sample mean is the covari ance term inside. The derivatives just linearize the function  near the true b. For example, a correlation coefficient can be written as a function of sample means as E xtyt E xt E yt corr xt,yt . E x2t E xt 2 E yt2 E yt 2 Thus, take A problem at the end of the chapter asks you to take derivatives and derive the standard error of the correlation coefficient. One can derive standard errors for impulse response functions, variance decompositions, and many other statistics in this way. 11.4 Using GMM for Regressions  E x E x2 E y E y2 E x y . tttttt By mapping OLS regressions in to the GMM framework, we derive formu las for OLS standard errors that correct for autocorrelation and conditional heteroskedasticity of the errors. The general formula is E xx  E xx 1, t t t j t j t t var  1E xx 1 T and it simplifies in special cases. t t j Mapping any statistical procedure into GMM makes it easy to develop an asymptotic distribution that corrects for statistical problems such as serial",
        "208 11. GMM: General Formulas and Applications correlation, and conditional heteroskedasticity. To illustrate, as well as to develop the very useful formulas, I map OLS regressions into GMM. Correcting OLS standard errors for econometric problems is not the same thing as GLS. When errors do not obey the OLS assumptions, OLS is consistent, and often more robust than GLS, but its standard errors need to be corrected. OLS picks parameters  to minimize the variance of the residual: minET yt  xt 2 .  We find  from the first order condition, which states that the residual is orthogonal to the right hand variable: g  E x y x  0. 11.12 T This condition is exactly identified the number of moments equals the number of parameters. Thus, we set the sample moments exactly to zero and there is no weighting matrix a I . We can solve for the estimate analytically, This is the familiar OLS formula. The rest of the ingredients to equation 11.2 are d E x x , tt f x, x y x  x. tttttt Equation 11.2 gives a formula for OLS standard errors, Tttt  E x x 1 E x y . TttTtt 1 E x x 1 t t E x x 1. t t var  As we estimate  2 by the sample variance of the residuals, we can estimate the quantity in brackets in 11.13 by its sample counterpart. This formula reduces to some interesting special cases. Serially Uncorrelated, Homoskedastic Errors These are the usual OLS assumptions, and it is good the usual formulas emerge. Formally, the OLS assumptions are E t xt,xt 1 ...t 1, t 2 ... 0, 11.14 E 2 x ,x ... ... constant 2. 11.15 t t t 1 t 1  T  E  x x  t t t j t j 11.13 j",
        "11.4. Using GMM for Regressions 209 To use these assumptions, I use the fact that E ab E E a b b . The first assumption means that only the j 0 term enters the sum E x x  E 2x x . t t t j t j t t t j The second assumption means that E 2x x E 2 E x x 2E x x . tttttttt Hence equation 11.13 reduces to our old friend, 1 1 var  2E xx 1 2X X . Ttt The last notation is typical of econometrics texts, in which X x1 x2 . . . xt represents the data matrix. Heteroskedastic Errors If we delete the conditional homoskedasticity assumption 11.15 , we cannot pull the  out of the expectation, so the standard errors are var  1 E x x 1E 2x x E x x 1. T These are known as heteroskedasticity consistent standard errors or White standard errors after White 1980 . Hansen Hodrick Errors Hansen and Hodrick 1982 run forecasting regressions of say six month returns, using monthly data. We can write this situation in regression notation as y  x  , t 1,2,...,T. t k k t t k Fama and French 1988b also use regressions of overlapping long horizon returns on variables such as dividend price ratio and term premium. Such regressions are an important part of the evidence for predictability in asset returns. tt ttt tt",
        "210 11. GMM: General Formulas and Applications Under the null that one period returns are unforecastable, we will still see correlation in the t due to overlapping data. Unforecastable returns imply E tt j 0 for j k but not for j k. Therefore, we can only rule out terms in S lower than k. Since we might as well correct for potential heteroskedasticity while we are at it, the standard errors are 1 k 1 var  E x x 1 E  x x  E x x 1. j k 1 11.5 Prespecified Weighting Matrices and Moment Conditions kTtt ttt jt j tt Prespecified rather than optimal weighting matrices can emphasize economically interesting results, they can avoid the trap of blowing up stan dard errors rather than improving pricing errors, they can lead to estimates that are more robust to small model misspecifications. This is analogous to the fact that OLS is often preferable to GLS in a regression context. The GMM formulas for a fixed weighting matrix W are var b 1 d Wd 1d WSWd d Wd 1, T var g 1 I d d Wd 1d W S I Wd d Wd 1d . T T In the basic approach outlined in Chapter 10, our final estimates were based on the efficient S 1 weighting matrix. This objective maxi mizes the asymptotic statistical information in the sample about a model, given the choice of moments gT . However, you may want to use a pre specified weighting matrix W S 1 instead, or at least as a diagnostic accompanying more formal statistical tests. A prespecified weighting matrix lets you, rather than the S matrix, specify which moments or linear combi nation of moments GMM will value in the minimization min b g b Wg b . TT A higher value of Wii forces GMM to pay more attention to getting the ith moment right in the parameter estimation. For example, you might feel that some assets suffer from measurement error, are small and illiquid, and hence should be deemphasized, or you may want to keep GMM from look ing at portfolios with strong long and short position. I give some additional motivations below.",
        "11.5. Prespecified Weighting Matrices and Moment Conditions 211 You can also go one step further and impose which linear combinations aT ofmomentconditionswillbesettozeroinestimationratherthanusethe choice resulting from a minimization, aT d S 1 or aT d W . The fixed W estimate still trades off the accuracy of individual moments according to the sensitivity of each moment with respect to the parameter. For example, ifg g1 g2 ,W I,but g b 110 ,sothatthesecondmomentis 10 times more sensitive to the parameter value than the first moment, then GMM with fixed weighting matrix sets 1 g1 10 g2 0. TT The second moment condition will be 10 times closer to zero than the first. If you really want GMM to pay equal attention to the two moments, then you can fix the aT matrix directly, for example aT 1 1 or aT 1 1 . Using a prespecified weighting matrix or using a prespecified set of momentsisnotthesamethingasignoringcorrelationoftheerrorsut inthe distribution theory. The S matrix will still show up in all the standard errors and test statistics. How to Use Prespecified Weighting Matrices Once you have decided to use a prespecified weighting matrix W or a prespecified set of moments aT g b 0, the general distribution theory T outlined in Section 11.1 quickly gives standard errors of the estimates and moments, and therefore a  2 statistic that can be used to test whether all the moments are jointly zero. Section 11.1 gives the formulas for the case that aT isprespecified.IfweuseweightingmatrixW,thefirst orderconditions tomin b g b Wg b are TTTT TT g b T Wg b d Wg b 0, TT so we map into the general case with aT d W. Plugging this value var b 1 d Wd 1d WSWd d Wd 1. 11.16 T You can check that this formula reduces to 1 T d S 1d 1 with W S 1. Plugging a d W into equation 11.5 , we find the variance covariance matrix of the moments gT , var g 1 I d d Wd 1d W S I Wd d Wd 1d . 11.17 T b into 11.4 , the variance covariance matrix of the estimated coefficients is T",
        "212 11. GMM: General Formulas and Applications As in the general formula, the terms to the left and right of S account for the fact that some linear combinations of moments are set to zero in each sample. Equation 11.17 can be the basis of 2 tests for the overidentifying restrictions. If we interpret 1 to be a generalized inverse, then g var g 1g 2 moments parameters . TTT As in the general case, you have to pseudo invert the singular var g , for T example by inverting only the nonzero eigenvalues. The major danger in using prespecified weighting matrices or moments aT is that the choice of moments, units, and of course the prespecified aT or W must be made carefully. For example, if you multiply the second moment by 10 times its original value, the optimal S 1 weighting matrix will undo this transformation and weight them in their original proportions. The identity weighting matrix will not undo such transformations, so the units should be picked right initially. Motivations for Prespecified Weighting Matrices Robustness, as with OLS vs. GLS When errors are autocorrelated or heteroskedastic, every econometrics text book shows you how to improve on OLS by making appropriate GLS corrections. If you correctly model the error covariance matrix and if the regression is perfectly specified, the GLS procedure can improve efficiency, i.e., give estimates with lower asymptotic standard errors. However, GLS is less robust. If you model the error covariance matrix incorrectly, the GLS estimates can be much worse than OLS. Also, the GLS transformations can zero in on slightly misspecified areas of the model. This may be good to show that the glass is half empty, but keeps you from seeing that it is half full, or tasting what s inside. GLS is best, but OLS is pretty darn good. You often have enough data that wringing every last ounce of statistical pre cision low standard errors from the data is less important than producing estimates that do not depend on questionable statistical assumptions, and that transparently focus on the interesting features of the data. In these cases, it is often a good idea to use OLS estimates. The OLS standard error formulas are wrong, though, so you must correct the standard errors of the OLS estimates for these features of the error covariance matrices, using the formulas we developed in Section 11.4. GMM works the same way. First stage or otherwise fixed weighting matrix estimates may give up something in asymptotic efficiency, but they are still consistent, and they can be more robust to statistical and economic problems. You still want to use the S matrix in computing standard errors,",
        "11.5. Prespecified Weighting Matrices and Moment Conditions 213 though, as you want to correct OLS standard errors, and the GMM formulas show you how to do this. Even if in the end you want to produce efficient estimates and tests, it is a good idea to calculate first stage estimates, standard errors and model fit tests. Ideally, the parameter estimates should not change by much, and the second stage standard errors should be tighter. If the efficient parameter estimates do change a great deal, it is a good idea to diagnose why this is so. It must come down to the efficient parameter estimates strongly weighting moments or linear combinations of moments that were not important in the first stage, and that the former linear combination of moments disagrees strongly with the latter about which parameters fit well. Then, you can decide whether the difference in results is truly due to efficiency gain, or whether it signals a model misspecification. Near Singular S The spectral density matrix is often nearly singular, since asset returns are highly correlated with each other, and since we often include many assets relative to the number of data points. As a result, second stage GMM and, as we will see below, maximum likelihood or any other efficient technique tries to minimize differences and differences of differences of moments in order to extract statistically orthogonal components with lowest variance. One may feel that this feature leads GMM to place a lot of weight on poorly estimated, economically uninteresting, or otherwise nonrobust aspects of the data. In particular, portfolios of the form 100R1 99R2 assume that investors can in fact purchase such heavily leveraged portfolios. Short sale costs often rule out such portfolios or significantly alter their returns, so one may not want to emphasize pricing them correctly in the estimation and evaluation. For example, suppose that S is given by 1 S , 1 so We can factor S 1 into a square root by the Choleski decomposition. This produces a triangular matrix C such that C C S 1. You can check that 1 11  S . 1 2  1",
        "214 the matrix 11. GMM: General Formulas and Applications 1  C 1 2 1 2 11.18 01 works. Then, the GMM criterion mingT S 1gT is equivalent to min g C Cg . TT CgT gives the linear combination of moments that efficient GMM is trying to minimize. Looking at 11.18 , as  1, the 2, 2 element stays at 1, but the 1, 1 and 1, 2 elements get very large and of opposite signs. For example, if  0.95, then 3.20 3.04 C . 01 In this example, GMM pays a little attention to the second moment, but places three times as much weight on the difference between the first and second moments. Larger matrices produce even more extreme weights. At a minimum, it is a good idea to look at S 1 and its Choleski decomposition to see what moments GMM is prizing. The same point has a classic interpretation, and is a well known danger with classic regression based tests. Efficient GMM wants to focus on well measured moments. In asset pricing applications, the errors are typically close to uncorrelated over time, so GMM is looking for portfolios with small values of var m Re . Roughly speaking, those will be assets with small t 1 t 1 return variance. Thus, GMM will pay most attention to correctly pricing the sample minimum varianceportfolio,andGMM sevaluationofthemodelbytheJT test will focus on its ability to price this portfolio. Now, consider what happens in a sample, as illustrated in Figure 11.1. The sample mean variance frontier is typically a good deal wider than the true, or ex ante mean variance frontier. In particular, the sample minimum variance portfolio may have little to do with the true minimum variance portfolio. Like any portfolio on the sample frontier, its composition largely reflects luck that is why we have asset pricing models in the first place rather than just price assets with portfolios on the sample frontier. The sample minimum variance return is also likely to be composed of strong long short positions. In sum, you may want to force GMM not to pay quite so much attention to correctly pricing the sample minimum variance portfolio, and you may",
        "11.5. Prespecified Weighting Matrices and Moment Conditions 215 Figure 11.1. True or ex ante and sample or ex post mean variance frontier. The sample often shows a spurious minimum variance portfolio. want to give less importance to a statistical measure of model evaluation that focuses on the model s ability to price that portfolio. Economically Interesting Moments The optimal weighting matrix makes GMM pay close attention to linear combinations of moments with small sampling error in both estimation and evaluation. You may want to force the estimation and evaluation to pay attention to economically interesting moments instead. The initial portfolios are usually formed on an economically interesting characteristic such as size, beta, book market, or industry. You typically want in the end to see how well the model prices these initial portfolios, not how well the model prices potentially strange portfolios of those portfolios. If a model fails, you may want to characterize that failure as the model does not price small stocks, not the model does not price a portfolio of 900 small firm returns 600 large firm returns 299 medium firm returns. Level Playing Field The S matrix changes as the model and as its parameters change. See the definition, 10.5 or 11.3 . As the S matrix changes, which assets the GMM estimate tries hard to price well changes as well. For example, the S matrix from one model may put a lot of weight on the T bill return, while that of another model may put a lot of weight on a stock excess return. Comparing the results of such estimations is like comparing apples and oranges. By fixing the weighting matrix, you can force GMM to pay attention to the various assets in the same proportion while you vary the model. The fact that S matrices change with the model leads to another sub tle trap. One model may improve a JT gT S 1gT statistic because it",
        "216 11. GMM: General Formulas and Applications blows up the estimates of S, rather than by making any progress on lower ing the pricing errors gT . No one would formally use a comparison of JT tests across models to compare them, of course. But it has proved nearly irresistible for authors to claim success for a new model over previous ones bynotingimprovedJT statistics,despitedifferentweightingmatrices,differ ent moments, and sometimes much larger pricing errors. For example, if youtakeamodelmt andcreateanewmodelbysimplyaddingnoise,unre latedtoassetreturns insample ,m m ,thenthemomentcondition ttt g E m Re E m  Re isunchanged.However,thespectralden T T t t T t t t sity matrix S E m  2ReRe can rise dramatically. This can reduce tttt the JT , leading to a false sense of improvement. Conversely, if the sample contains a nearly risk free portfolio of the test assets, or a portfolio with apparently small variance of mt 1Rte 1, then the JT test essentially evaluates the model by how well it can price this one portfolio. This can lead to a statistical rejection of a much improved model even a very small gT will produce a large gT S 1gT if there is a small eigenvalue of S. Some Prespecified Weighting Matrices Two examples of economically interesting weighting matrices are the second moment matrix of returns, advocated by Hansen and Jagannathan 1997 , and the simple identity matrix, which is used implicitly in much empirical asset pricing. Second Moment Matrix Hansen and Jagannathan 1997 advocate the use of the second moment matrix of payoffs W E xx 1 in place of S. They motivate this weighting matrix as an interesting distance measure between a model for m, say y, and the space of true m s. Precisely, the minimum distance second moment between a candidate discount factor y and the space of true discount factors is the same as the minimum value of the GMM criterion with W E xx 1 as weighting matrix. To see why this is true, refer to Figure 11.2. The distance between y and the nearest valid m is the same as the distance between proj y X and x . As usual, consider the case that X is generated from a vector of payoffs x If you use a common weighting matrix W for all models, and evaluate the models by gT WgT , then you can avoid this trap. The question are the pricing errors small? is as interesting as the question if we drew artificial data over and over again from a null statistical model, how often would we estimate a ratio of pricing errors to their estimated variance gT S 1gT this big or larger?",
        "11.5. Prespecified Weighting Matrices and Moment Conditions 217 Figure 11.2. Distance between y and nearest m distance between proj y X and x . with price p. From the OLS formula, proj y X E yx E xx 1x. x is the portfolio of x that prices x by construction, x p E xx 1x. Then, the distance between y and the nearest valid m is y nearest m proj y X x E yx E xx 1x p E xx 1x E yx p E xx 1x E yx p E xx 1 E yx p gT E xx 1gT . You might want to choose parameters of the model to minimize this economic measure of model fit, or this economically motivated linear combination of pricing errors, rather than the statistical measure of fit S 1. You might also use the minimized value of this criterion to compare two mod els. In that way, you are sure the better model is better because it improves on the pricing errors rather than just blowing up the weighting matrix. Identity Matrix Using the identity matrix weights the initial choice of assets or portfolios equally in estimation and evaluation. This choice has a particular advantage with large systems in which S is nearly singular, as it avoids most of the problems associated with inverting a near singular S matrix. Many empirical asset pricing studies use OLS cross sectional regressions, which are the same thing as a first stage GMM estimate with an identity weighting matrix.",
        "218 11. GMM: General Formulas and Applications Comparing the Second Moment and Identity Matrices The second moment matrix gives an objective that is invariant to the initial choice of assets or portfolios. If we form a portfolio Ax of the initial pay offs x, with nonsingular A i.e., a transformation that does not throw away information , then E yAx Ap E Axx A 1 E yAx Ap E yx p E xx 1 E yx p . The optimal weighting matrix S shares this property. It is not true of the identity or other fixed matrices. In those cases, the results will depend on the initial choice of portfolios. Kandel and Stambaugh 1995 have suggested that the results of sev eral important asset pricing model tests are highly sensitive to the choice of portfolio, i.e., that authors inadvertently selected a set of portfolios on which the CAPM does unusually badly in a particular sample. Insisting that weighting matrices have this kind of invariance to portfolio selection might be a good device to guard against this problem. On the other hand, if you want to focus on the model s predictions for economically interesting portfolios, then it would not make much sense for the weighting matrix to undo the specification of economically interesting portfolios! For example, many studies want to focus on the ability of a model to describe expected returns that seem to depend on a characteristic such as size, book market, industry, momentum, etc. Also, the second moment matrix is often even more nearly singular than the spectral density matrix, since E xx cov x E x E x . Therefore, it often emphasizes portfo lios with even more extreme short and long positions, and is no help on overcoming the near singularity of the S matrix. 11.6 Estimating on One Group of Moments, Testing on Another You may want to force the system to use one set of moments for estimation and another for testing. The real business cycle literature in macroeco nomics does this extensively, typically using first moments for estimation calibration and second moments i.e., first moments of squares for evaluation. A statistically minded macroeconomist might like to know whether the departures of model from data second moments are large compared to sampling variation, and would like to include sampling uncertainty about the parameter estimates in this evaluation. You might want to choose parameters using one set of asset returns stocks, domestic assets, size portfolios, first nine size deciles, well measured",
        "11.7. Estimating the Spectral Density Matrix 219 assets and then see how the model does out of sample on another set of assets bonds, foreign assets, book market portfolios, small firm portfolio, questionably measured assets, mutual funds . However, you want the distri bution theory for evaluation on the second set of moments to incorporate sampling uncertainty about the parameters from their estimation on the first set of moments, and correlation between the estimation moments and the evaluation moments. You can do all this very simply by using an appropriate weighting matrix or a prespecified moment matrix aT . For example, if the first N moments will be used to estimate N parameters, and the remaining M moments will be usedtotestthemodel outofsample, useaT IN 0N M .Iftherearemore moments N than parameters in the estimation block, you can construct a weighting matrix W which is an identity matrix in the N N estimation block and zero elsewhere. Then aT gT bW will simply contain the first N columns of g b followed by zeros. The test moments will not be used T in estimation. You could even use the inverse of the upper N N block of S not the upper block of the inverse of S! to make the estimation a bit more efficient. 11.7 Estimating the Spectral Density Matrix The optimal weighting matrix S depends on population moments, and depends on the parameters b. Work back through the definitions, Hints on estimating the spectral density or long run covariance matrix. 1 Use a sensible first stage estimate. 2 Remove means. 3 Downweight higher order correlations. 4 Consider parametric structures for autocor relation and heteroskedasticity. 5 Use the null to limit the number of correlations or to impose other structure on S. 6 Size problems; consider a factor or other parametric cross sectional structure for S. 7 Iteration and simultaneous b, S estimation. t t j j E u u , S ut mt b xt pt 1 . How do we estimate this matrix? The big picture is simple: following the usual philosophy, estimate population moments by their sample counter parts. Thus, use the first stage b estimates and the data to construct sample",
        "220 11. GMM: General Formulas and Applications versions of the definition of S. This procedure can produce a consistent estimate of the true spectral density matrix, which is all the asymptotic distribution theory requires. The details are important, however, and this section gives some hints. Also, you may want a different, and less restrictive, estimate of S for use in standard errors than you do when you are estimating S for use in a weighting matrix. 1 Use a sensible first stage W , or transform the data. In the asymptotic theory, you can use consistent first stage b estimates formed by any nontrivial weighting matrix. In practice, of course, you should use a sensible weighting matrix so that the first stage estimates are not ridiculously inefficient. W I is often a good choice. Sometimes, some moments will have different units than other moments. For example, the dividend price ratio is a number like 0.04. Therefore,themomentformedbyRt 1 d pt willbeabout0.04aslarge as the moment formed by Rt 1 1. If you use W I, GMM will pay muchlessattentiontotheRt 1 d pt moment.Itiswise,then,toeither useaninitialweightingmatrixthatoverweightstheRt 1 d pt moment, or to transform the data so the two moments are about the same mean and variance. For example, you could use Rt 1 1 d pt . It is also useful to start with moments that are not horrendously correlated with each other, or to remove such correlation with a clever W . For example, you might consider Ra and Rb Ra rather than Ra and Rb. You can accomplish this directly, or by starting with 1 110 2 1 W . 0 1 1 1 1 1 2 Remove means. Under the null, E ut 0, so it does not matter to the asymptotic dis tribution theory whether you estimate the covariance matrix by removing means, using 1 T 1 T T t 1 ut u ut u , u ut, T t 1 or whether you estimate the second moment matrix by not removing means. However, Hansen and Singleton 1982 advocate removing the means in sample, and this is generally a good idea. It is already an obstacle to second stage estimation that estimated S matrices and even simple variance covariance matrices are often nearly singular, providing an unreliable weighting matrix when inverted. Since",
        "222 11. GMM: General Formulas and Applications sample mean by looking at the variance in a single sample of shorter sums, var 1 k uj . The S matrix is sometimes called the long run covariance k j 1 matrix for this reason. In fact, one could estimate S directly as a variance of kth sums and obtain almost the same estimator, that would also be positive definite in any sample, k j 1 1 T v v t , v t 1 1 T S This estimator has been used when measurement of S is directly inter esting Cochrane 1988 , Lo and MacKinlay 1988 . A variety of other weighting schemes have been advocated. What value of k, or how wide a window if of another shape, should you use? Here again, you have to use some judgment. Too short values of k,togetherwithaut thatissignificantlyautocorrelated,andyoudonot correct for correlation that might be there in the errors. Too long a value of k, together with a series that does not have much autocorrelation, and the performance of the estimate and test deteriorates. If k T 2, for example, you are really using only two data points to estimate the variance of the mean. The optimum value then depends on how much persistence or low frequency movement there is in a particular application, versus accuracy of the estimate. There is an extensive statistical literature about optimal window width, or size of k. Alas, this literature mostly characterizes the rate at which k should increase with sample size. You must promise to increase k as sample size increases, but not as quickly as the sample size increases limT k , limT k T 0 in order to obtain consistent estimates. In practice, promises about what you would do with more data are pretty meaningless, and usually broken once more data arrives. 4 Consider parametric structures for autocorrelation and heteroskedasticity. Nonparametric corrections such as 11.19 often do not perform very well in typical samples. The problem is that nonparametric tech niques are really very highly parametric; you have to estimate many correlations in the data. Therefore, the nonparametric estimate varies a good deal from sample to sample. This variation may make S 1 weighted estimates much less efficient, and sometimes worse than estimates formed on a fixed weighting matrix. Also, the asymptotic distribution theory ignores sampling variation in covariance matrix estimates. The u t j , T k t k 1 vt v vt v . k T k t k 1",
        "11.7. Estimating the Spectral Density Matrix 223 asymptotic distribution can therefore be a poor approximation to the finite sample distribution of statistics like the JT . You can get more accurate standard errors with a Monte Carlo or bootstrap rather than relying on asymptotic theory. Alternatively, you can impose a parametric structure on the S matrix which can address both problems. Just because the formulas are expressed in terms of a sum of covariances does not mean you have to estimate them that way; GMM is not inherently tied to nonparametric covariance matrix estimates. For example, if you model a scalar u as an AR 1 with parameter , then you can estimate two numbers  and  2 rather than a whole list of u autocorrelations, and calculate j If this structure is not a bad approximation, imposing it can result in more reliable estimates and test statistics since you have to estimate many fewer coefficients. You could transform the data in such a way that there is less correlation to correct for in the first place. This is a very useful formula, by the way. You are probably used to calculating the standard error of the mean as  x  x . T This formula assumes that the x are uncorrelated over time. If an AR 1 is not a bad model for their correlation, you can quickly adjust for correlation by using instead. This sort of parametric correction is very familiar from OLS regres sion analysis. The textbooks commonly advocate the AR 1 model for serial correlation as well as parametric models for heteroskedasticity cor rections. There is no reason not to follow a similar approach for GMM statistics. 5 Use the null to limit correlations? In the typical asset pricing setup, the null hypothesis specifies that Et ut 1 Et mt 1Rt 1 1 0, as well as E ut 1 0. This implies that all the autocorrelation terms of S drop out; E u u 0 for j 0. t t j The lagged u could be an instrument z; the discounted return should 2 j 21  S E uu    . t t j u u1  j  x T 1   x 1",
        "11.7. Estimating the Spectral Density Matrix 225 On the other hand, adding extra correlations can help with the power of test statistics the probability of rejection given that an alternative is true since they converge to the correct spectral density matrix. This trade off requires some thought. For measurement rather than pure testing, using a spectral density matrix that can accommodate alternatives may be the right choice. For example, in the return forecasting regres sions, one is really focused on measuring return forecastability rather than just formally testing the hypothesis that it is zero. If you are testing an asset pricing model that predicts u should not be autocorrelated, and there is a lot of correlation if this issue makes a big difference then this is an indication that something is wrong with the model: that including u as one of your instruments z would result in a rejection or at least substantially change the results. If the u are close to uncorrelated, then it really does not matter if you add a few extra terms or not. 6 Size problems; consider a factor or other parametric cross sectional structure. If you try to estimate a covariance matrix that is larger than the num ber of data points say 2000 NYSE stocks and 800 monthly observations , the estimate of S, like any other covariance matrix, is singular by con struction. This fact leads to obvious problems when you try to invert S! More generally, when the number of moments is more than around 1 10 the number of data points, S estimates tend to become unstable and near singular. Used as a weighting matrix, such an S matrix tells you to pay lots of attention to strange and probably spurious linear combina tions of the moments, as I emphasized in Section 11.5. For this reason, most second stage GMM estimations are limited to a few assets and a few instruments. A good, but as yet untried alternative might be to impose a factor structure or other well behaved structure on the covariance matrix. The near universal practice of grouping assets into portfolios before analysis already implies an assumption that the true S of the underlying assets has a factor structure. Grouping in portfolios means that the individual assets have no information not contained in the portfolio, so that a weighting matrix S 1 would treat all assets in the portfolio identically. It might be better to estimate an S imposing a factor structure on all the primitive assets. Another response to the difficulty of estimating S is to stop at first stage estimates, and only use S for standard errors. One might also use a highly structured estimate of S as weighting matrix, while using a less constrained estimate for the standard errors. This problem is of course not unique to GMM. Any estimation tech nique requires us to calculate a covariance matrix. Many traditional",
        "226 11. GMM: General Formulas and Applications estimatessimplyassumethatut errorsarecross sectionallyindependent. This false assumption leads to understatements of the standard errors far worse than the small sample performance of any GMM estimate. Our econometric techniques all are designed for large time series and small cross sections. Our data has a large cross section and short time series. A large unsolved problem in finance is the development of appropriate large N small T tools for evaluating asset pricing models. 7 Alternatives to the two stage procedure: iteration and one step. Hansen and Singleton 1982 describe the above two step procedure, and it has become popular for that reason. Two alternative procedures may perform better in practice, i.e., may result in asymptotically equiv alent estimates with better small sample properties. They can also be simpler to implement, and require less manual adjustment or care in specifying the setup moments, weighting matrices which is often just as important. a Iterate. The second stage estimate b density as the first stage. It might seem appropriate that the estimate of b and of the spectral density should be consistent, i.e., to find a fixed point of b min b g b S b 1g b . One way to search for TT such a fixed point is to iterate: find b2 from b ming b S 1 b g b , 11.21 2 b T 1T where b1 is a first stage estimate, held fixed in the minimization over b . Then use b to find S b , find 222 b min g b S b 1g b , 3 b T 2 T and so on. There is no fixed point theorem that such iterations will converge, but they often do, especially with a little massaging. IonceusedS bj bj 1 2 inthebeginningpartofaniterationto keep it from oscillating between two values of b . Ferson and Foerster 1994 find that iteration gives better small sample performance than two stage GMM in Monte Carlo experiments. This procedure is also likely to produce estimates that do not depend on the initial weighting matrix. b Pick b and S simultaneously. It is not true that S must be held fixed as one searches for b. Instead, one can use a new S b for each value of b. Explicitly, one can estimate b by min g b S 1 b g b . 11.22 b TT 2 will not imply the same spectral",
        "Problems 227 The estimates produced by this simultaneous search will not be numerically the same in a finite sample as the two step or iterated estimates. The first order conditions to 11.21 are g b S 1 b1 g b 0, b T T while the first order conditions in 11.22 add a term involving the derivatives of S b with respect to b. However, the latter terms vanish asymptotically, so the asymptotic distribution theory is not affected. Hansen, Heaton, and Yaron 1996 conduct some Monte Carlo experiments and find that this estimate may have small sample advantages in certain problems. However, one step minimization may find regions of the parameter space that blow up the spectral density matrix S b rather than lower the pricing errors gT . Often, one choice will be much more convenient than another. For linear models, you can find the minimizing value of b from the first order conditions 11.23 analytically. This fact eliminates the need to search so an iterated estimate is much faster than a one step estimator. For nonlinear perform this search many times, it may be much quicker to minimize once form, so the search may run into greater numerical difficulties. Problems Chapter 11 1. Use the delta method version of the GMM formulas to derive the sampling variance of an autocorrelation coefficient. models,eachstepinvolvesanumericalsearchoverg b Sg b .Ratherthan TT over g b S b g b . On the other hand, the latter is not a locally quadratic TT 11.23 2. a Write a formula for the standard error of OLS regression coeffi cients that corrects for autocorrelation but not heteroskedasticity. b Show in this case that conventional standard errors are OK if the x s are uncorrelated over time, even if the errors  are correlated over time. If the GMM errors come from an asset pricing model, ut mt Rt 1, can you ignore lags in the spectral density matrix? What if you know that returns are predictable? What if the error is formed from an instrument managed portfolio ut z t 1 ? 3.",
        "13.2. The Case of Excess Returns 257 The GMM estimate is a cross sectional regression of mean excess returns on the second moments of returns with factors. From here on in, the distribution theory is unchanged from the last section. Mean Returns on Covariances We can obtain a cross sectional regression of mean excess returns on covariances, which are just a heartbeat away from betas, by choosing the normalization a 1 b E f rather than a 1. Then, the model is m 1 b f E f withmeanE m 1.Thepricingerrorsare g b ET mRe ET Re ET Ref b, T whereIdenotef f E f .Wehave b which now denotes the covariance matrix of returns and factors. The first g b d T E Ref , order condition to min gT WgT is now d W ET R e db 0. Then, the GMM estimates of b are First stage : b d d 1d E Re , 1T Second stage : b d S 1d 1d S 1E Re . 2T The GMM estimate is a cross sectional regression of expected excess returns on the covariance between returns and factors. Naturally, the model says that expected excess returns should be proportional to the covariance between returns and factors, and we estimate that relation by a linear regression. The standard errors and variance of the pricing errors are the same as in 13.2 and 13.3 , with d now representing the covariance matrix. The formulas are almost exactly identical to those of the cross sectional regressions in Section 12.2. The p E mx formulation of the model for excess returns is equivalent to E Re cov Re,f b; thus covariances enter in place of betas. There is one fly in the ointment: The mean of the factor E f is esti mated, and the distribution theory should recognize sampling variation induced by this fact, as we did for the fact that betas are generated regres sors in the cross sectional regressions of Section 12.2. The moments are in fact g ETRe Re f Ef b , T ET f Ef",
        "14 Maximum Likelihood Maximum likelihood is, like GMM, a general organizing principle that is a useful place to start when thinking about how to choose parameters and evaluate a model. It comes with an asymptotic distribution theory, which, like GMM, is a good place to start when you are unsure about how to treat various problems such as the fact that betas must be estimated in a cross sectional regression. As we will see, maximum likelihood is a special case of GMM. Given a statistical description of the data, it prescribes which moments are statisti cally most informative. Given those moments, ML and GMM are the same. Thus, ML can be used to defend why one picks a certain set of moments, or for advice on which moments to pick if one is unsure. In this sense, max imum likelihood paired with carefully chosen statistical models justifies the regression tests above, as it justifies standard regressions. On the other hand, ML does not easily allow you to use other non efficient moments, if you suspect that ML s choices are not robust to misspecifications of the economic or statistical model. For example, ML will tell you how to do GLS, but it will not tell you how to adjust OLS standard errors for nonstandard error terms. Hamilton 1994, pp. 142 148 and the appendix in Campbell, Lo, and MacKinlay 1997 give nice summaries of maximum like lihood theory. Campbell, Lo, and MacKinlay s Chapters 5 and 6 treat many more variations of regression based tests and maximum likelihood. 267",
        "270 14. Maximum Likelihood If we estimate a model restricting the parameters, the maximum value of the likelihood function will necessarily be lower. However, if the restriction is true, it should not be that much lower. This intuition is captured in the likelihood ratio test 2L L 2 . 14.5 unrestricted restricted number of restrictions The form and idea of this test are much like the  2 difference test for GMM objectives that we met in Section 11.1. 14.2 ML is GMM on the Scores ML is a special case of GMM. ML uses the information in the auxiliary statis tical model to derive statistically most informative moment conditions. To see this fact, start with the first order conditions for maximizing a likelihood function L xt ;  T t 1 lnf xt xt 1,xt 2 ...;  0. 14.6 This is a GMM estimate. It is the sample counterpart to a population moment condition lnf xt xt 1,xt 2 ...;  g  E 0. 14.7 The term ln f xt xt 1, xt 2 . . . ;   is known as the score. It is a random variable, formed as a combination of current and past data xt,xt 1,... . Thus, maximum likelihood is a special case of GMM, a special choice of which moments to examine. For example, suppose that x follows an AR 1 with known variance, xt xt 1 t, and suppose the error terms are i.i.d. normal random variables. Then, 2 lnf xt xt 1,xt 2,...; const. t const. xt xt 1 22 2 and the score is lnf xt xt 1,xt 2 ...;  22 xt xt 1 xt 1 . 2",
        "14.2. ML is GMM on the Scores 271 The first order condition for maximizing likelihood is 1 T xt xt 1 xt 1 0. This expression is a moment condition, and you will recognize it as the OLS estimator of , which we have already regarded as a case of GMM. The example shows another property of scores: The scores should be unforecastable. In the example, Et 1 xt xt 1 xt 1 2 Et 1 txt 1 2 0. 14.8 T t 1 Intuitively, if we used a combination of the x variables E h x t , x t 1 , . . . 0 that was predictable, we could form another moment an instrument that described the predictability of the h variable and use that moment to get more information about the parameters. To prove this property more gen erally, start with the fact that f xt xt 1, xt 2, . . . ;  is a conditional density and therefore must integrate to one, 1 0 0 f xt xt 1,xt 2,...; dxt, f xt xt 1,xt 2,...;  dxt , lnf xt xt 1,xt 2,...;  f xt xt 1,xt 2,...; dxt, lnf xt xt 1,xt 2,...; 0 Et 1  . Furthermore, as you might expect, the GMM distribution theory formulas give the same result as the ML distribution, i.e., the information matrix is the asymptotic variance covariance matrix. To show this fact, apply the GMM distribution theory 11.2 to 14.6 . The derivative matrix is g  1 T 2lnf xt xt 1,xt 2,...; d T I.    T t 1 This is the second derivative expression of the information matrix. The S matrix is lnf xt xt 1,xt 2,...; lnf xt xt 1,xt 2,...; E I.",
        "280 15. Time Series, Cross Section, GMM DF Tests no Newey West weights. It is singular in the data sample and many Monte Carlo replications. Interestingly, this singularity has minor effects on stan dard errors, but causes disasters when you use the spectral density matrix to weight a second stage GMM. I also find that second stage efficient GMM is only very slightly more efficient than first stage GMM, but is somewhat less robust; it is more sensi tive to the poor spectral density matrix and its asymptotic standard errors can be misleading. As OLS is often better than GLS, despite the theoretical effi ciency advantage of GLS, first stage GMM may be better than second stage GMM in many applications. This section should give comfort that the apparently new GMM discount factor formulation is almost exactly the same as traditional methods in the traditional setup. There is a widespread impression that GMM has difficulty in small samples. The literature on the small sample properties of GMM for example, Ferson and Foerster 1994 , Fuhrer, Moore, and Schuh 1995 naturally tries hard setups, with highly nonlinear models, highly persistent and heteroskedastic errors, important condition ing information, potentially weak instruments, and so forth. Nobody would write a paper trying GMM in a simple situation such as this one, correctly foreseeing that the answer would not be very interesting. Unfortunately, many readers take from this literature a mistaken impression that GMM always has difficulty in finite samples, even in very standard setups. This is not the case. Jagannathan and Wang 2000 also compare the GMM discount factor approach to classic regression tests. They show analytically that the parame ter estimates, standard errors, and  2 statistics are asymptotically identical to those of an expected return beta cross sectional regression when the factor is not a return. 15.1 Three Approaches to the CAPM in Size Portfolios The time series approach sends the expected return beta line through the market return, ignoring other assets. The OLS cross sectional regression minimizes the sum of squared pricing errors, so allows some market pricing error to fit other assets better. The GLS cross sectional regression weights pricing errors by the residual covariance matrix, so reduces to the time series regression when the factor is a return and is included in the test assets. The GMM discount factor estimates, standard errors, and 2 statistics are very close to time series and cross sectional regression estimates in this classic setup.",
        "15.2. Monte Carlo and Bootstrap 289 Start with the Monte Carlo evaluation of the time series test in Table 15.3. The i.i.d. and 0 lag distributions produce nearly exact rejection probabilities in the long sample and slightly too many 7.5 rejections in the short sample. Moving down, GMM distributions here correct for things that are not there. This has a small but noticeable effect on the sensible 3 lag test, which rejects slightly too often under this null. Naturally, this is worse for the short sample, but looking across the rows, the time series and discount factor tests are nearly identical in every case. The variation across technique is almost zero, given the spectral density estimate. The 24 lag unweighted spectral density is the usual disaster, rejecting far too often. It is singular in many samples. In the long sample, the 1 tail of this distribution occurs at a 2 value of 440 rather than the 23.2 of the 2 distribution! 10 The long sample block bootstrap in the right half of the tables shows even in this simple setup how i.i.d. normal assumptions can be misleading. The traditional i.i.d.  2 test has almost half the correct size it rejects a 5 test 2.8 of the time, and a 1 test 0.6 of the time. Removing the assumption that returns and factors are independent, going from i.i.d. to 0 lags, brings about half of the size distortion back, while adding one of the sensible autocorrelation corrections does the rest. In each row, the time series and GMM DF methods produce almost exactly the same results again. The 24 lag spectral density matrices are a disaster as usual. Table 15.4 shows the rejection probabilities under the alternative. The most striking feature of the table is that the GMM discount factor test gives almost exactly the same rejection probability as the time series test, for each choice of spectral density estimation technique. When there is a difference, the GMM discount factor test rejects slightly more often. The 24 lag tests reject most often, but this is not surprising given that they reject almost as often under the null. Parameter Estimates and Standard Errors Table 15.5 presents the sampling variation of the  and b estimates. The rows and columns marked   ,  b , and in italic font, give the variation of the estimated  or b across the 5000 artificial samples. The remaining rows and columns give the average across samples of the standard errors. The presence of pricing errors has little effect on the estimated b or  and their standard errors, so I only present results under the null that the CAPM is true. The parameters are not directly comparable the b param eter includes the variance as well as the mean of the factor, and ET R em is the natural GMM estimate of the mean market return as it is the time series estimate of the factor risk premium. Still, it is interesting to know and to compare how well the two methods do at estimating their central parameter.",
        "290 Table 15.5. 15. Time Series, Cross Section, GMM DF Tests Monte Carlo and block bootstrap evaluation of the sampling variability of parameter estimates b and  Monte Carlo Block Bootstrap Time series GMM DF Time series 1st 2nd stage GMM DF 1st 2nd stage T 876:   ,  b 0.19 0.64 i.i.d. 0.18 0 lags 0.18 0.65 3 lags NW 0.18 0.65 24 lags 0.18 0.62 T 240:   ,  b 0.35 1.25 i.i.d. 0.35 0 lags 0.35 1.23 3 lags NW 0.35 1.22 24 lags 0.29 1.04 0.61 0.62 130 1.24 1.26 191 0.18 0.60 0.18 0.59 0.19 0.27 0.19 0.63 0.67 0.60 0.67 0.67 0.62 0.66 1724 0.24 stage  b E s.e. stage  b 0.20 0.69 E s.e. 0.35 1.14 0.35 1.11 0.36 0.69 0.31 1.24 1.45 1.15 1.31 1.48 1.14 1.15 893 0.75 0.37 1.40 The Monte Carlo redraws 5000 artificial data sets of length T 876 from a random normal assuming that the CAPM is true. The block bootstrap redraws the data in groups of 3 with replacement. The row and columns marked   and  b and using italic font give the variation across samples of the estimated  and b. The remaining entries of time series 1st stage and E s.e. columns in roman font give the average value of the computed standard error of the parameter estimate, where the average is taken over the 5000 samples. The central message of this table is that the GMM DF estimates behave almost exactly as the time series beta model estimate, and the asymptotic standard error formulas almost exactly capture the sampling variation of the estimates. The second stage GMM DF estimate is a little bit more efficient at the cost of slightly misleading standard errors. Start with the long sample and the first column. All of the standard error formulas give essentially identical and correct results for the time series estimate. Estimating the sample mean is not rocket science. The first stage GMM DF estimator in the second column behaves the same way, except the usually troublesome 24 lag unweighted estimate. The second stage GMM DF estimate in the third and fourth columns uses the inverse spectral density matrix to weight, and so the estimator depends on the choice of spectral density estimate. The sensible spectral density estimates not 24 lags produce second stage estimates that vary less than the first stage estimates, 0.61 0.62 rather than 0.64. Second stage GMM is more efficient, meaning that it produces estimates with smaller sampling variation. However, the table shows that the efficiency gain is quite small,",
        "15.2. Monte Carlo and Bootstrap 291 so not much is lost if one prefers first stage OLS estimates. The sensible spectral density estimates produce second stage standard errors that again almost exactly capture the sampling variation of the estimated parameters. The 24 lag unweighted estimate produces hugely variable estimates and artificially small standard errors. Using bad or even singular spectral density estimates seems to have a secondary effect on standard error calculations, but using its inverse as a weighting matrix can have a dramatic effect on estimation. With the block bootstrap in the right hand side of Table 15.5, the time series estimate is slightly more volatile as a result of the slight autocorrelation in the market return. The i.i.d. and zero lag formulas do not capture this effect, but the GMM standard errors that allow autocorrelation do pick it up. However, this is a very minor effect as there is very little autocorrelation in the market return. The effect is more pronounced in the first stage GMM DF estimate, since the smaller firm portfolios depart more from the normal i.i.d. assumption. The true variation is 0.69, but standard errors that ignore autocorrelation only produce 0.63. The standard errors that correct for autocorrelation are nearly exact. In the second stage GMM DF, the sensible spectral density estimates again produce slightly more efficient estimates than the first stage, with variation of 0.67 rather than 0.69. This comes at a cost, though, that the asymptotic standard errors are a bit less reliable. In the shorter sample, we see that standard errors for the mean market return in the time series column are all quite accurate, except the usual 24 lag case. In the GMM DF case, we see that the actual sampling variability of the b estimate is no longer smaller for the second stage. The second stage estimate is not more efficient in this small sample. Furthermore, while the first stage standard errors are still decently accurate, the second stage standard errors substantially understate the true sampling variability of the parameter estimate. They represent a hoped for efficiency that is not present in the small sample. Even in this simple setup, first stage GMM is clearly a better choice for estimating the central parameter, and hence for examining individual pricing errors and their pattern across assets.",
        "16 Which Method? The point of GMM discount factor methods is not a gain in efficiency or simplicity in a traditional setup linear factor model, i.i.d. normally distributed returns, etc. It is hard to beat the efficiency or simplicity of regression methods in those setups. The promise of the GMM discount factor approach is its ability to transparently handle nonlinear or otherwise complex models, especially including conditioning information, and that it allows you to circumvent inevitable model misspecifications or simplifica tions and data problems by keeping the econometrics focused on interesting issues. The alternative is usually some form of maximum likelihood. This is much harder in most circumstances, since you have to write down a complete statistical model for the joint distribution of your data. Just evaluating, let alone maximizing, the likelihood function is often challenging. Whole series of papers are written on the econometric issues of particular cases, for exam ple how to maximize the likelihood functions of univariate continuous time models for the short interest rate. Empirical asset pricing faces an enduring tension between these two philosophies. The choice essentially involves trade offs between statistical efficiency, the effects of misspecification of both the economic and statistical models, and the clarity and economic interpretability of the results. There are situations in which it is better to trade some small efficiency gains for the robustness of simpler procedures or more easily interpretable moments; OLS can be better than GLS. The central reason is specification errors: the fact that our statistical and economic models are at best quantitative para bles. There are other situations in which you may really need to squeeze every last drop out of the data, intuitive moments are statistically very inefficient, and more intensive maximum likelihood approaches are more appropriate. Unfortunately, the environments are complex, and differ from case to case. We do not have universal theorems from statistical theory or generally applicable Monte Carlo evidence. Specification errors by their nature resist 293",
        "Which Method? 297 then consider portfolios of the original securities formed from a nonsingular matrix A. They follow E ARe A A. You can make all these portfolios have the same  by choosing A so that A constant, and then they will have a spread in alphas. You will see a plot in which all the portfolios have the same beta but the average returns are spread up and down. Conversely, you can pick A to make the expected return beta plot look as good as you want. GLS has an important feature in this situation: the GLS cross sectional regression is independent of such repackaging of portfolios. If you trans form a set of returns Re to ARe, then the OLS cross sectional regression is transformed from to   A A 1 A AE Re . This does depend on the repackaging A. However, the residual covariance matrixofARe isA A ,sotheGLSregression    1  E R e   1  1  1 E R e is not affected so long as A is full rank and therefore does not throw away information  1 1 1E Re .   A A A 1A 1  A A A 1AE Re The spectral density matrix and second stage estimate share this prop erty in GMM estimates. These are not the only weighting matrix choices that are invariant to portfolios. For example, Hansen and Jagannathan s 1997 suggestion of the return second moment matrix has the same property. This is a fact, but it does not show that OLS chooses a particularly good or bad set of portfolios. Perhaps you do not think that GLS choice of portfolios is particularly informative. In this case, you use OLS precisely to focus attention on a particular set of economically interesting portfolios. The choice depends subtly on what you want your test to accomplish. If you want to prove the model wrong, then GLS helps you to focus on the most informative portfolios for proving the model wrong. That is exactly what an efficient test is supposed to do. However, many models are wrong, but still",
        "298 16. Which Method? pretty darn good. It is a shame to throw out the information that the model does a good job of pricing an interesting set of portfolios. The sensible compromise would seem to be to report the OLS estimate on interesting portfolios, and also to report the GLS test statistic that shows the model to be rejected. That is, in fact, the typical collection of facts. Additional Examples of Trading Off Efficiency for Robustness Here are some additional examples of situations in which it has turned out to be wise to trade off some apparent efficiency for robustness to model misspecifications. Low frequency time series models. In estimating time series models such as the AR 1 yt yt 1 t, maximum likelihood minimizes one step ahead forecast error variance, E 2 . But any time series model is only an t approximation, and the researcher s objective may not be one step ahead forecasting. For example, in making sense of the yield on long term bonds, we are interested in the long run behavior of the short rate of interest. In estimating the magnitude of long horizon univariate mean reversion in stock returns, we want to know only the sum of autocorrelations or mov ing average coefficients. Writing pt a 1 t, we want to know a 1 . We will study this application in Section 19.1. The approximate model that generates the smallest one step ahead forecast error variance may be quite different from the model that best matches the long run behavior of the series. Cochrane 1988 contains a more detailed analysis of this point in the context of long horizon GDP forecasting. Lucas money demand estimate. Lucas 1988 is a gem of an example. Lucas was interested in estimating the income elasticity of money demand. Money and income trend upwards over time and over business cycles, but also have some high frequency movement that looks like noise. If you run a regression in log levels, mt a byt t, you get a sensible coefficient of about b 1, but you find that the error term is strongly serially correlated. Following standard advice, most researchers run GLS, which amounts pretty much to first differencing the data, mt mt 1 b yt yt 1 t. This error term passes its Durbin Watson statistic, but the b estimate is much lower, which does not make much economic sense, and, worse, is unstable, depending a lot on time period and data definitions. Lucas realized that the regression in differences threw out most of the information in the data, which was in the trend, and focused on the high frequency noise. Therefore,",
        "300 16. Which Method? shapes, and the information in the term structure for future movements in yields and the volatility of yields. They are very useful for derivative pricing. But it is never the case in actual yield data that yields of all maturities are exact functions of K yields. Actual data on N yields always require N shocks. Again, a ML approach reports a log likelihood function for any set of parameters. Addressing Model Misspecification The ML philosophy offers an answer to model misspecification: specify the right model, and then do ML. If regression errors are correlated, model and estimate the covariance matrix and do GLS. If you are worried about proxy errors in the pricing factor, short sales costs or other transactions costs so that model predictions for extreme long short positions should not be relied on, if you are worried about time aggregation or mismeasurement of consumption data, nonnormal or non i.i.d. returns, time varying betas and factor risk premia, additional pricing factors and so on do not chat about them, write them down, and then do ML. Following this lead, researchers have added measurement errors to real business cycle models Sargent 1989 is a classic example and affine yield models in order to break the stochastic singularity I discuss this case a bit more in Section 19.6 . The trouble is, of course, that the assumed structure of the measurement errors now drives what moments ML pays attention to. And seriously modeling and estimating the measurement errors takes us further away from the economically interesting parts of the model. Measurement error augmented models will often wind up specify ing sensible moments, but by assuming ad hoc processes for measurement error, such as i.i.d. errors. Why not just specify the sensible moments in the first place? More generally, authors tend not to follow this advice, in part because it is ultimately infeasible. Economics necessarily studies quantitative parables rather than completely specified models. It would be nice if we could write down completely specified models, if we could quantitatively describe all the possible economic and statistical model and specification errors, but we cannot. The GMM framework, used judiciously, allows us to evaluate misspec ified models. It allows us to direct that the statistical effort focus on the interesting predictions while ignoring the fact that the world does not match the uninteresting simplifications. For example, ML only gives you a choice of OLS, whose standard errors are wrong, or GLS, which you may not trust in small samples or which may focus on uninteresting parts of the data. GMM allows you to keep an OLS estimate, but to correct the stan dard errors for non i.i.d. distributions. More generally, GMM allows you to specify an economically interesting set of moments, or a set of moments",
        "Which Method? 301 that you feel will be robust to misspecifications of the economic or statisti cal model, without having to spell out exactly what is the source of model misspecification that makes those moments optimal or even interest ing and robust. It allows you to accept the lower efficiency of the estimates under some sets of statistical assumptions, in return for such robustness. At the same time, the GMM framework allows you to flexibly incor porate statistical model misspecifications in the distribution theory. For example, knowing that returns are not i.i.d. normal, you may want to use the time series regression technique anyway. This estimate is not incon sistent, but the standard errors that ML formulas pump out under this assumption are inconsistent. GMM gives a flexible way to derive at least an asymptotic set of corrections for statistical model misspecifications of the time series regression coefficient. Similarly, a pooled time series cross sectional OLS regression is not inconsistent, but standard errors that ignore cross correlation of error terms are far too small. The calibration of real business cycle models is often really noth ing more than a GMM parameter estimate, using economically sensible moments such as average output growth, consumption output ratios, etc. to avoid the stochastic singularity that would doom a ML approach. Kydland and Prescott s 1982 idea that empirical microeconomics would provide accurate parameter estimates for macroeconomic and financial models has pretty much vanished. Calibration exercises usually do not com pute standard errors, nor do they report any distribution theory associated with the evaluation stage when one compares the model s predicted sec ond moments with those in the data. Following Burnside, Eichenbaum, and Rebelo 1993 , however, it is easy enough to calculate such a dis tribution theory to evaluate whether the difference between predicted second moments and actual moments is large compared to sampling vari ation, including the variation induced by parameter estimation in the same sample by listing the first and second moments together in the gT vector. Used judiciously is an important qualification. Many GMM estima tions and tests suffer from lack of thought in the choice of moments, test assets, and instruments. For example, early GMM papers tended to pick assets and especially instruments pretty much at random. Industry portfo lios have almost no variation in average returns to explain. Authors often included many lags of returns and consumption growth as instruments to test a consumption based model. However, the seventh lag of returns really does not predict much about future returns given lags 1 6, and the first order serial correlation in seasonally adjusted, ex post revised consump tion growth may be economically uninteresting. More recent work tends to emphasize a few well chosen assets and instruments that capture important and economically interesting features of the data.",
        "302 16. Which Method? Auxiliary Model ML requires an auxiliary statistical model. For example, in the classic ML formalization of regression tests, we had to stop to assume that returns and factors are jointly i.i.d. normal. As the auxiliary statistical model becomes more and more complex and hence realistic, more and more effort is devoted to estimating the auxiliary statistical model. ML has no way of knowing that some parameters a, b; , , risk aversion  are more important than others , and parameters describing time varying conditional moments of returns. A very convenient feature of GMM is that it does not require such an auxiliary statistical model. For example, in studying GMM we went straight from p E mx to moment conditions, estimates, and distribution theory. This is an important saving of the researcher s and the reader s time, effort, and attention. Finite Sample Distributions Many authors say they prefer regression tests and the GRS statistic in partic ular because it has a finite sample distribution theory, and they distrust the finite sample performance of the GMM asymptotic distribution theory. This argument does not have much force. The finite sample distribu tion only holds if returns really are normal and i.i.d., and if the factor is perfectly measured. Since these assumptions do not hold, it is not obvious that a finite sample distribution that ignores non i.i.d. returns will be a better approximation than an asymptotic distribution that corrects for them. All approaches give essentially the same answers in the classic setup of i.i.d. returns. The issue is how the various techniques perform in more complex setups, especially with conditioning information, and here there are no analytic finite sample distributions. In addition, once you have picked the estimation method how you will generate a number from the data; or which moments you will use finding its finite sample distribution, given an auxiliary statistical model, is simple. Just run a Monte Carlo or bootstrap. Thus, picking an estimation method because it delivers analytic formulas for a finite sample distribution under false assumptions should be a thing of the past. Analytic formulas for finite sample distributions are useful for comparing estimation methods and arguing about statistical properties of estimators, but they are not necessary for the empiricists main task. Finite Sample Quality of Asymptotic Distributions, and Nonparametric Estimates Several investigations Ferson and Foerster 1994 , Hansen, Heaton, and Yaron 1996 have found cases in which the GMM asymptotic distribution",
        "Which Method? 303 theory is a poor approximation to a finite sample distribution theory. This is especially true when one asks nonparametric corrections for autocorrela tion or heteroskedasticity to provide large corrections and when the number of moments is large compared to the sample size, or if the moments one uses for GMM turn out to be very inefficient Fuhrer, Moore, and Schuh 1995 , which can happen if you put in a lot of instruments with low forecast power. The ML distribution is the same as GMM, conditional on the choice of moments, but typical implementations of ML also use the parametric time series model to simplify estimates of the terms in the distribution theory as well as to derive the likelihood function. If this is the case if the nonparametric estimates of the GMM distri bution theory perform poorly in a finite sample, while the parametric ML distribution works well there is no reason not to use a parametric time series model to estimate the terms in the GMM distribution as well. For example, rather than calculate j E utut j from a large sum of auto correlations, you can model ut ut 1 t , estimate , and then calculate The Case for ML In the classic setup, the efficiency gain of ML over GMM on the pricing errors is tiny. However, several studies have found cases in which the statistically motivated choice of moments suggested by ML has important efficiency advantages. For example, Jacquier, Polson, and Rossi 1994 study the estimation of a time series model with stochastic volatility. This is a model of the form 2 u more detail. j 2 u 1  1  .Section11.7discussedthisideain j dSt St dt Vt dZ1t, dVt V Vt dt  Vt dZ2t, 16.1 and S is observed but V is not. The obvious and easily interpretable moments include the autocorrelation of squared returns, or the autocorrelation of the absolute value of returns. However, Jacquier, Polson, and Rossi find that the resulting estimates are far less efficient than those resulting from the ML scores. Of course, this study presumes that the model 16.1 really is exactly true. Whether the uninterpretable scores or the interpretable moments really perform better to give an approximate model of the form 16.1 , given some other data generating mechanism, is open to discussion. Even in the canonical OLS versus GLS case, a wildly heteroskedastic error covariance matrix can mean that OLS spends all its effort fitting unim portant data points. A judicious application of GMM OLS in this case",
        "Which Method? 307 Summary The bottom line is simple: It is ok to do a first stage or simple GMM estimate rather than an explicit maximum likelihood estimate and test. Many people and, unfortunately, many journal referees seem to think that nothing less than a full maximum likelihood estimate and test is acceptable. This section is long in order to counter that impression; to argue that at least in many cases of practical importance, a simple first stage GMM approach, focusing on economically interpretable moments, can be adequately efficient, robust to model misspecifications, and ultimately more persuasive."
    ],
    "Topic 8": [
        "19 Term Structure of Interest Rates Term structure models are particularly simple, since bond prices are just the expected value of the discount factor. In equations, the price at time j t of a zero coupon bond that comes due at time t j is Pt Et mt,t j . Thus, once you specify a time series process for a one period discount factor m t , t 1 , you can in principle find the price of any bond by chaining together j the discount factors and finding Pt Et mt, t 1mt 1, t 2 . . . mt j 1, t j . As with option pricing models, this chaining together can be hard to do, and much of the analytical machinery in term structure models centers on this technical question. As with option pricing models, there are two equivalent ways to do the chaining together: Solve the discount factor forward and take an integral, or find a partial differential equation for prices and solve it backwards from the maturity date. 19.1 Definitions and Notation A quick introduction to bonds, yields, holding period returns, forward rates, and swaps. p N log price of N period zero coupon bond at time t . t y N 1p N logyield. N hpr N p N 1 p N log holding period return. t 1 t 1 t hpr dP N,t 1 P N,t dt instantaneous return. P P N f N N 1 p N p N 1 forwardrate. ttt f N,t 1 P N,t instantaneousforwardrate. P N 349",
        "350 19. Term Structure of Interest Rates Bonds The simplest fixed income instrument is a zero coupon bond. A zero coupon bond is a promise to pay one dollar a nominal bond or one unit of the consumption good a real bond on a specified date. I use a superscript in parentheses to denote maturity: P 3 is the price of a three year zero coupon t bond. I will suppress the t subscript when it is not necessary. Idenotelogsbylowercasesymbols,p N lnP N .Thelogpricehasa nice interpretation. If the price of a one year zero coupon bond is 0.95, i.e., 95 per dollar face value, the log price is ln 0.95 0.051. This means that the bond sells at a 5 discount. Logs also give the continuously compounded rate. If we write erN 1 P N , then the continuously compounded rate is rN lnP N . Coupon bonds are common in practice. For example, a 100 face value 10 year coupon bond may pay 5 every year for 10 years and 100 at 10 years. Coupon bonds are often issued with semiannual or more frequent pay ments, 2.50 every six months for example. We price coupon bonds by considering them as a portfolio of zeros. Yield The yield of a bond is the fictional, constant, known, annual, interest rate that justifies the quoted price of a bond, assuming that the bond does not default. It is not the rate of return of the bond. From this definition, the yield of a zero coupon bond is the number Y N that satisfies tt Hence P N 1 . Y N N Y N 1 1 N , y N 1 p N . P N N The latter expression nicely connects yields and prices. If the price of a 4 year bond is 0.20 or a 20 discount, that is 5 discount per year, or a yield of 5 . The yield of any stream of cash flows is the number Y that satisfies N CFj P . j 1 Yj In general, you have to search for the value Y that solves this equation, given the cash flows and the price. So long as all cash flows are positive, this is fairly easy to do.",
        "19.1. Definitions and Notation 351 As you can see, the yield is just a convenient way to quote the price. In using yields we make no assumptions. We do not assume that actual interest rates are known or constant; we do not assume the actual bond is default free. Bonds that may default trade at lower prices or higher yields than bonds that are less likely to default. This only means a higher return if the bond happens not to default. Holding Period Returns If you buy an N period bond and then sell it it has now become an N 1 period bond you achieve a return of or, of course, back P N 1 t 1 HPR N t 1 paid P N 19.1 t hpr N p N 1 p N . t 1 t 1 t We date this return from t to t 1 as t 1 because that is when you find out its value. If this is confusing, take the time to write returns as HPRt t 1 and then you will never get lost. In continuous time, we can easily find the instantaneous holding period return of bonds with fixed maturity date P T , t P T,t P T,t P T,t dP T,t P and, taking the limit, hpr hpr , . However, it is nicer to look for a bond pricing function P N,t that fixes the maturity rather than the date. As in 19.1 , we then have to account for the fact that you sell bonds that have shorter maturity than you buy: hpr P N , t P N , t P N,t P N ,t P N,t P N,t P N,t , and, taking the limit dP N,t 1 P N,t hpr dt. 19.2 P N,t P P N",
        "352 19. Term Structure of Interest Rates Forward Rate The forward rate is defined as the rate at which you can contract today to borrow or lend money starting at period N , to be paid back at period N 1. You can synthesize a forward contract from a spectrum of zero coupon bonds, so the forward rate can be derived from the prices of zero coupon bonds. Here is how. Suppose you buy one N period zero coupon bond and simultaneously sell x N 1 period zero coupon bonds. Let us track your cash flow at every date: Buy N period zero Today0: P N Time N : 1 Time N 1: Sell x N 1 period zeros xP N 1 x Net cash flow xP N 1 P N 1 x Now, choose x so that today s cash flow is zero: P N x . P N 1 You pay or get nothing today, you get 1.00 at N , and you pay P N P N 1 at N 1. You have synthesized a contract signed today for a loan from N to N 1 a forward rate! Thus, P N F N N 1 Forwardrateatt forN N 1 t , and of course 19.3 People sometimes identify forward rates by the initial date, f N , and some times by the ending date, f N 1 . I use the arrow notation when I want to be t really clear about dating a return. Forward rates have the lovely property that you can always express a bond price as its discounted present value using forward rates, p N p N p N 1 p N 1 p N 2 p 2 p 1 p 1 ttttt ttt f N 1 N f N 2 N 1 f 1 2 y 1 tttt t P N 1 t f N N 1 p N p N 1 . ttt t",
        "19.1. Definitions and Notation y 1 f 0 1 of course , so tt N 1 1 353 N 1 N j j 1 p f; tt j 0 N j j 1 P F. tt j 0 Intuitively, the price today must be equal to the present value of the payoff at rates you can lock in today. In continuous time, we can define the instantaneous forward rate 1 P N,t p Nt f N , t . 19.4 P N N Then, forward rates have the same property that you can express today s price as a discounted value using the forward rate, x 0 Equations 19.3 and 19.4 express forward rates as derivatives of the price versus maturity curve. Since yield is related to price, we can relate forward rates to the yield curve directly. Differentiating the definition of yield y N,t p N,t N, f N , t . y N,t 1 p N , t P N,t e N f x,t dx. 1 p N,t NN2 N NNN Thus, y N,t f N,t y N,t N N . In the discrete case, 19.3 implies f N N 1 Ny N N 1 y N 1 y N 1 N y N 1 y N . tttttt Forward rates are above the yield curve if the yield curve is rising, and vice versa. p N , t 1 1 y N , t N x 0 f x , t dx",
        "354 19. Term Structure of Interest Rates Swaps and Options Swaps are an increasingly popular fixed income instrument. The simplest example is a fixed for floating swap. Party A may have issued a 10 year fixed coupon bond. Party B may have issued a 10 year variable rate bond a bond that promises to pay the current one year rate. For example, if the current rate is 5 , the variable rate issuer would pay 5 for every 100 of face value. A long term variable rate bond is the same thing as rolling over one period debt. They may be unhappy with these choices. For example, the fixed rate payer may not want to be exposed to interest rate risk that the present value of his promised payments rises if interest rates decline. The variable rate issuer may want to take on this interest rate risk, betting that rates will rise or to hedge other commitments. If they are unhappy with these choices, they can swap the payments. The fixed rate issuer pays off the variable rate coupons, and the variable rate issuer pays off the fixed rate coupons. Obvi ously, only the difference between fixed and variable rate actually changes hands. Swapping the payments is much safer than swapping the bonds. If one party defaults, the other can drop out of the contract, losing the difference in price resulting from intermediate interest rate changes, but not losing the principal. For this reason, and because they match the patterns of cashflows that companies usually want to hedge, swaps have become very popular tools for managing interest rate risk. Foreign exchange swaps are also popular: Party A may swap dollar payments for party B s yen payments. Obviously, you do not need to have issued the underlying bonds to enter into a swap contract you simply pay or receive the difference between the variable rate and the fixed rate each period. The value of a pure floating rate bond is always exactly one. The value of a fixed rate bond varies. Swaps are set up so no money changes hands initially, and the fixed rate is calibrated so that the present value of the fixed payments is exactly one. Thus, the swap rate is the same thing as the yield on a comparable coupon bond. Many fixed income securities contain options, and explicit options on fixed income securities are also popular. The simplest example is a call option. The issuer may have the right to buy the bonds back at a speci fied price. Typically, he will do this if interest rates fall a great deal, making a bond without this option more valuable. Home mortgages contain an interesting prepayment option: if interest rates decline, the homeowner can pay off the loan at face value, and refinance. Options on swaps also exist; you can buy the right to enter into a swap contract at a future date. Pricing all of these securities is one of the tasks of term structure modeling.",
        "19.2. Yield Curve and Expectations Hypothesis 355 19.2 Yield Curve and Expectations Hypothesis The yield curve is a plot of yields of zero coupon bonds as a function of their maturity. Usually, long term bond yields are higher than short term bond yields a rising yield curve. Sometimes short yields are higher than long yields an inverted yield curve. The yield curve sometimes has humps or other shapes as well. The expectations hypothesis is the classic theory for understanding the shape of the yield curve. More generally, we want to think about the evolution of yields the expected value and conditional variance of next period s yields. This is obvi ously the central ingredient for portfolio theory, hedging, derivative pricing, and economic explanation. We can state the expectations hypothesis in three mathematically equivalent forms: The expectations hypothesis is three equivalent statements about the pattern of yields across maturity: 1. The N period yield is the average of expected future one period yields. 2. The forward rate equals the expected future spot rate. 3. The expected holding period returns are equal on bonds of all maturities. The expectations hypothesis is not quite the same thing as risk neutrality, since it ignores 1 2 2 terms that arise when you move from logs to levels. 1. The N period yield is the average of expected future one period yields y N 1E y 1 y 1 y 1 y 1 riskpremium . 19.5 t Ntt t 1 t 2 t N 1 2. The forward rate equals the expected future spot rate f N N 1 E y 1 risk premium . 19.6 t tt N 3. Theexpectedholdingperiodreturnsareequalonbondsofallmaturities E hpr N y 1 risk premium . 19.7 t t 1 t The risk premia in 19.5 19.7 are related, but not identical. You can see how the expectations hypothesis explains the shape of the yield curve. If the yield curve is upward sloping long term bond yields are higher than short term bond yields the expectations hypothesis says this is because short term rates are expected to rise in the future. You can view the expectations hypothesis as a response to a classic misconception. If long term yields are 10 but short term yields are 5 , an unsophisticated investor might think that long term bonds are a better investment. The expectations hypothesis shows how this may not be true.",
        "356 19. Term Structure of Interest Rates If short rates are expected to rise in the future, this means that you will roll over the short term bonds at a really high rate, say 20 , giving the same long term return as the high yielding long term bond. Contrariwise, when the short term interest rates rise in the future, long term bond prices decline. Thus, the long term bonds will only give a 5 rate of return for the first year. You can see from the third statement that the expectations hypothe sis is roughly the same as risk neutrality. If we had said that the expected level of returns was equal across maturities, that would be the same as risk neutrality. The expectations hypothesis specifies that the expected log return is equal across maturities. This is typically a close approxima tion to risk neutrality, but not the same thing. If returns are lognormal, then E R eE r 1 2 2 r . If mean returns are about 10 or 0.1 and the standard deviation of returns is about 0.1, then 1  2 is about 0.005, which is 2 very small but not zero. We could easily specify risk neutrality in the third expression of the expectations hypothesis, but then it would not imply the other two; 1  2 terms would crop up. 2 The intuition of the third form is clear: risk neutral investors will adjust positions until the expected one period returns are equal on all securities. Any two ways of getting money from t to t 1 must offer the same expected return. The second form adapts the same idea to the choice of locking in a forward contract versus waiting and borrowing and lending at the spot rate. Risk neutral investors will load up on one or the other contract until the expected returns are the same. Any two ways of getting money from t N to t N 1 must give the same expected return. The first form reflects a choice between two ways of getting money from t to N . You can buy an N period bond, or roll over N one period bonds. Risk neutral investors will choose one over the other strategy until the expected N period return is the same. The three forms are mathematically equivalent. If every way of getting money from t to t 1 gives the same expected return, then so must every way of getting money from t 1 to t 2, and, chaining these together, every way of getting money from t to t 2. For example, let us show that forward rate expected future spot rate implies the yield curve. Start by writing f N 1 N E y 1 . t t t N 1 Add these up over N , f 0 1 f 1 2 f N 2 N 1 f N 1 N tttt E y 1 y 1 y 1 y 1 . ttt 1t 2 t N 1 The right hand side is already what we are looking for. Write the left hand side in terms of the definition of forward rates, remembering P 0 1",
        "19.3. Term Structure Models A Discrete Time Introduction 357 sop 0 0, f 0 1 f 1 2 f N 2 N 1 f N 1 N tttt p 0 p 1 p 1 p 2 p N 1 p N tttttt p N Ny N . tt You can show all three forms 19.5 19.7 are equivalent by following similar arguments. It is common to add a constant risk premium and still refer to the result ing model as the expectations hypothesis, and I include a risk premium in parentheses to remind you of this idea. One end of each of the three state ments does imply more risk than the other. A forward rate is known while the future spot rate is not. Long term bond returns are more volatile than short term bond returns. Rolling over short term real bonds is a riskier long term investment than buying a long term real bond. If real rates are constant, and the bonds are nominal, then the converse can hold: short term real rates can adapt to inflation, so rolling over short nominal bonds can be a safer long term real investment than long term nominal bonds. These risks will generate expected return premia if they covary with the discount factor, and our theory should reflect this fact. If you allow an arbitrary, time varying risk premium, the model is a tau tology, of course. Thus, the entire content of the expectations hypothesis augmented with risk premia is in the restrictions on the risk premium. We will see that the constant risk premium model does not do that well empiri cally. One of the main points of term structure models is to quantify the size and movement over time in the risk premium. 19.3 Term Structure Models A Discrete Time Introduction Term structure models specify the evolution of the short rate and poten tially other state variables, and the prices of bonds of various maturities at any given time as a function of the short rate and other state variables. I examine a very simple example based on an AR 1 for the short rate and the expectations hypothesis, which gives a geometric pattern for the yield curve. A good way to generate term structure models is to write down a process for the discount fac tor, and then price bonds as the conditional mean of the discount factor. This procedure guarantees the absence of arbitrage. I give a very simple example of an AR 1 model for the log discount factor, which also results in geometric yield curves.",
        "358 19. Term Structure of Interest Rates A natural place to start in modeling the term structure is to model yields statistically. You might run regressions of changes in yields on the lev els of lagged yields, and derive a model of the mean and volatility of yield changes. You would likely start with a factor analysis of yield changes and express the covariance matrix of yields in terms of a few large factors that describe their common movement. The trouble with this approach is that you can quite easily reach a statistical representation of yields that implies an arbitrage opportunity, and you would not want to use such a statistical char acterization for economic understanding of yields, for portfolio formation, or for derivative pricing. For example, a statistical analysis usually suggests that a first factor should be a level factor, in which all yields move up and down together. It turns out that this assumption violates arbitrage: the long maturity yield must converge to a constant.1 How do you model yields without arbitrage? An obvious solution is to use the discount factor existence theorem: Write a statistical model for a positive discount factor, and find bond prices as the expectation of this discount factor. Such a model will be, by construction, arbitrage free. Conversely, any arbitrage free distribution of yields can be captured by some positive discount factor, so you do not lose any generality with this approach. A Term Structure Model Based on the Expectations Hypothesis We can use the expectations hypothesis to give the easiest example of a term structure model. This one does not start from a discount factor and so may not be arbitrage free. It does quickly illustrate what we mean by a term structure model. Suppose the one period yield follows an AR 1 , y 1   y 1   . t 1 t t 1 Now, we can use the expectations hypothesis 19.5 to calculate yields on bonds of all maturities as a function of today s one period yield, y 2 1E y 1 y 1 t 2tt t 1 1 More precisely, the long term forward rate, if it exists, must never fall. Problem 7 guides you through a simple calculation. Dybvig, Ingersoll, and Ross 1996 derive the more general statement. 1 y 1   y 1  2tt 1  2  y 1  . t",
        "19.3. Term Structure Models A Discrete Time Introduction 359 Continuing in this way, You can see some issues that will recur throughout the term structure models. First, the model 19.8 can describe different yield curve shapes at different times. If the short rate is below its mean, then there is a smoothly upward sloping yield curve. Long term bond yields are higher, as short rates are expected to increase in the future. If the short rate is above its mean, we get a smoothly inverted yield curve. This particular model cannot produce humps or other interesting shapes that we sometimes see in the term struc ture. Second, this model predicts no average slope of the term structure: E y N E y 1 . In fact, the average term structure seems to slope up tt slightly and more complex models will reproduce this feature. Third, all bond yields move together in the model. If we were to stack the yields up in a VAR representation, it would be y 1  y 1   , t 1 t t 1 1 1 N t N1 t y N  y 1  . 19.8 2 2 1  y  y  , t 1 t 2 t 1 N 1 1 N . y  y  . N t 1 t N 1  t 1 You can write the right hand variable in terms of y 1 if you want any t one yield carries the same information as any other. The error terms are all the same. We can add more factors to the short rate process, to improve on this prediction, but most tractable term structure models maintain less factors than there are bonds, so some perfect factor structure is a common prediction of term structure models. Fourth, this model has a problem in that the short rate, following an AR 1 , can be negative. Since people can always hold cash, nominal short rates are never negative, so we want to start with a short rate process that does not have this feature. Fifth, this model shows no conditional heteroskedasticity the conditional variance of yield changes is always the same. The term structure data show times of high and low volatility, and times of high yields and high yield spreads seem to track these changes in volatility. Modeling conditional volatility is crucially important for valuing term structure options. With this simple model in hand, you can see some obvious directions for generalization. First, we will want more complex driving processes than an AR 1 . For example, a hump shape in the conditionally expected short",
        "360 19. Term Structure of Interest Rates rate will result in a hump shaped yield curve. If there are multiple state variables driving the short rate, then we will have multiple factors driving the yield curve which will also result in more interesting shapes. We also want processes that keep the short rate positive in all states of nature. Second, we will want to add some market prices of risk some risk premia. This will allow us to get average yield curves to not be flat, and time varying risk premia seem to be part of the yield data. We will also want to check that the market prices are reasonable, and in particular that there are no arbitrage opportunities. The yield curve literature proceeds in exactly this way: specify a short rate process and the risk premia, and find the prices of long term bonds. The trick is to specify sufficiently complex assumptions to be interesting, but preserve our ability to solve the models. The Simplest Discrete Time Model The simplest nontrivial model I can think of is to let the log of the discount factor follow an AR 1 with normally distributed shocks. I write the AR 1 for the log rather than the level in order to make sure the discount factor is positive, precluding arbitrage. Log discount factors are typically slightly negative, so I denote the unconditional mean E ln m  lnmt 1   lnmt  t 1. In turn, you can think of this discount factor model as arising from a consumption based power utility model with normal errors, mt 1 e  Ct 1 Ct , ct 1 ct  ct ct 1 t 1. The term structure literature has only started to explore whether the empiri cally successful discount factor processes can be connected empirically back to macroeconomic events in this way. From this discount factor, we can find bond prices and yields. This is easy because the conditional mean and variance of an AR 1 are easy to find. I am following the strategy of solving the discount factor forward rather than solving the price backward. We need y 1 p 1 lnE elnmt 1 , ttt y 2 1p 2 1 lnE elnmt 1 lnmt 2 , t2t2t",
        "19.3. Term Structure Models A Discrete Time Introduction 361 and so on. Iterating the AR 1 forward, so lnmt 2  2 lnmt  t 1 t 2, lnmt 3  3 lnmt  2t 1 t 2 t 3, lnmt 1  lnmt 2   2 lnmt  1  t 1 t 2. Similarly, lnmt 1  lnmt 2  lnmt 3   2 3 lnmt  1  2 t 1 1  t 2 t 3. 2 Using the rule for a lognormal E e x e E x 1 x2 , we have finally y 1   lnm  12, tt2  2 t2t4 y 2  y 3  1 1  2 2, 1 1  2 1  2 2 t3t6 Notice all yields move as linear functions of a single state variable, lnmt .Therefore,wecansubstituteoutthediscountfactorandexpress the yields on bonds of any maturity as functions of the yields on bonds of one maturity. Which one we choose is arbitrary, but it is conventional to use the shortest interest rate as the state variable. With E y 1  12, we lnm   2 3 lnm  2. can write our term structure model as y 1 E y 1  y 1 E y 1 t t 1 t 2 19.9 2 k 1 . , 1  1 1  2 y 2  y 1 E y 1 2, t2t4 1  2 y 3  y 1 E y 1 t3t y N  t 1  N y 1 E y 1 N 1  t  2 N j  1 1  2 1  2 2 6 2,  2Nj 1 k 1",
        "362 19. Term Structure of Interest Rates This is the form in which term structure models are usually written an evolution equation for the short rate process together, in general, with other factors or other yields used to identify those factors , and then longer rates written as functions of the short rate, or the other factors. This is still not a very realistic term structure model. In the data, the aver age yield curve the plot of E y N versus N is slightly upward sloping. t The average yield curve from this model is slightly downward sloping as the 2 terms pile up. The effect is not large; with  0.9 and  0.02, I  find E y 2 E y 1 0.02 and E y 3 E y 1 0.06 . Still, it does tt tt not slope up. More importantly, this model only produces smoothly upward sloping or downward sloping term structures. For example, with  0.9, the first three terms multiplying the one period rate in 19.9 are 0.86, 0.81, 0.78. Two , three , and four period bonds move exactly with one period bonds using these coefficients. The solution, of course, is to specify more complex discount rate processes that give rise to more interesting term structures. 19.4 Continuous Time Term Structure Models The basic steps: 1. Write a time series model for the discount factor, typically in the form 2. Solve the discount factor model forward and take expectations, to find d r dt  dz, dr r dt r dz. bond prices t P N E tt t N . 3. Alternatively, from the basic pricing equation 0 E d P we can find a differential equation that the price must follow, P 1 2P P P  2 rP . rr2 r2r N rr You can solve this back from P 0 1. N I contrast the discount factor approach to the market price of risk and arbitrage pricing approaches.",
        "19.4. Continuous Time Term Structure Models 363 Term structure models are usually more convenient in continuous time. As always, I specify a discount factor process and then find bond prices. A wide and popular class of term structure models are based on a discount factor process of the form d Vasicek: CIR: r dt  dz, dr  r r dt r dz. rdt  dz, dr  dt  dz . rr 19.10 This specification is analogous to a discrete time model of the form mt 1 xt t 1, xt 1 xt t 1. This is a convenient representation rather than a generalized autoregres sive process, since the state variable x carries the mean discount factor information. The r variable starts out as a state variable for the drift of the discount factor. However, you can see quickly that it will become the short rate pro cess since E d r f dt. The dots remind you that these terms tt can be functions of state variables whose evolution must also be modeled. Wecanalsowrite t,rt,rt toremindourselvesthatthesequantitiesvary over time. Term structure models differ in the specification of the functional forms for r , r ,  . We will study three famous examples, the Vasicek model, the Cox Ingersoll Ross model, and the general affine specification. The first two are d 19.11 d r dt  r dz, 19.12 dr  r r dt r rdz. The Vasicek model is quite similar to the AR 1 we studied in the last section. The CIR model adds the square root terms in the volatility. This specification captures the fact that higher interest rates seem to be more volatile. In the other direction, it keeps the level of the interest rate from falling below zero. Weneedr 2r toguaranteethatthesquarerootprocessdoesnotget stuck at zero. Having specified a discount factor process, it is a simple matter to find bond prices. Once again, P N E t N . tt t",
        "364 19. Term Structure of Interest Rates We can solve the discount factor forward and take the expectation. We can also use the instantaneous pricing condition 0 E d P to find a partial differential equation for prices, and solve that backward. Both methods naturally adapt to pricing term structure derivatives call options on bonds, interest rate floors or caps, swaptions that give you the righttoenteraswap,andsoforth.WesimplyputanypayoffxC thatdepends on interest rates or interest rate state variables inside the expectation P N E sxC s ds. tt s t t Alternatively, the price of such options will also be a function of the state variables that drive the term structure, so we can solve the bond pricing differential equation backwards using the option payoff rather than one as the boundary condition. Expectation Approach As with the Black Scholes option pricing model, we can solve the discount factor forward, and then take the expectation. We can write the solution2 to 19.10 as and thus, T12 T 0 T e rs  ds s 0 2 s s 0  sdzs P T Ee T rs 12ds Tdzs. 0 0 s 0 2 s s 0 s 19.13 For example, in a riskless economy  0, we obtain the continuous time present value formula, P T e T rsds. 0 With a constant interest rate r , P T e rT. 0 In more interesting situations, solving the equation forward and tak ing the expectation analytically is not so easy. Conceptually and numerically, it is easy, of course. Just simulate the system 19.10 forward a few thousand times, and take the average. 2 If this is mysterious, write first d 1d 2 dln r  dt  dz 2 2 2 and then integrate both sides from zero to T . s 0 12",
        "19.4. Continuous Time Term Structure Models 365 Differential Equation Approach Recall the basic pricing equation for a security with price S and no dividends is dS dS d Et r dt Et SS . 19.14 The left hand side is the expected excess return. As we guessed an option price C S,t and used 19.14 to derive a differential equation for the call option price, so we will guess a bond price P N , t and use this equation to derive a differential equation for the bond price. If we specified bonds by their maturity date T, P t,T , we could apply 19.14 directly. However, it is nicer to look for a bond pricing func tion P N , t that fixes the maturity rather than the date. Equation 19.2 gives the holding period return for this case, which adds an extra term to correct for the fact that you sell younger bonds than you buy, P P N given maturity P N , t , is dP N,t 1 P N,t return dt. Thus, the fundamental pricing equation, applied to the price of bonds of dP 1 P N,t dP d Et r dt Et . 19.15 PP N P Now, we are ready to find a differential equation for the bond price, just as we did for the option price to derive the Black Scholes formula. Guess that all the time dependence comes through the state variable r , so P N , r . Using Ito s lemma, P 1 2P P dP  2 dt dz. rr 2 r2r rr Plugging in to 19.15 and canceling dt, we obtain the fundamental differential equation for bonds, P 1 2P P rr2 r2r N rr  2 P rP   . 19.16 All you have to do is specify the functions r , r ,  and solve the differential equation.",
        "366 19. Term Structure of Interest Rates Market Price of Risk and Risk Neutral Dynamic Approaches The bond pricing differential equation 19.16 is conventionally derived without discount factors. One conventional approach is to write the short rate process dr r dt r dz, and then specify that any asset whose payoffs have shocks r dz must offer a Sharpe ratio of  . We would then write P 1 2P P P  2 rP . rr2 r2r N rr With   , this is just 19.16 of course. If the discount factor and shock are imperfectly correlated, then   . Different authors use the words market price of risk in different ways. Cox, Ingersoll, and Ross 1985, p. 398 warn against modeling the right hand side as P r directly; this specification could lead to a positive expected return when r 0 and hence an infinite Sharpe ratio or arbitrage opportunity. By generating expected returns as the covariance of payoff shocks and discount factor shocks, we naturally avoid this mistake and other subtle ways of introducing arbitrage opportunities without realizing that you have done so. A second conventional approach is to use an alternative process for the interest rate and discount factor, If we use this alternative process, we obtain d 19.17 r dt, dr r r  dt r dz. P 1 P P   2 rP 0, rr r 2 r2r N which is of course the same thing. This is the risk neutral probability approach, since the drift term in 19.17 is not the true drift that you would estimate in the data, and since the discount factor is nonstochas tic. Since 19.17 gives the same prices, we can find and represent the bond price via the integral P N E e T rsds , tt When we derive the model from a discount factor, the single discount factor carries two pieces of information. The drift or conditional mean of where E represents expectation with respect to the risk neutral pro cess defined in 19.17 rather than the true probabilities defined by the process 19.10 . s 0",
        "19.4. Continuous Time Term Structure Models 367 the discount factor gives the short rate of interest, while the covariance of the discount factor shocks with asset payoff shocks generates expected returns or market prices of risk. I find it useful to write the discount factor model to keep the term structure connected with the rest of asset pricing, and to remind myself where market prices of risk come from, and reasonable values for their magnitude. Of course, this beauty is in the eye of the beholder, as the result is the same no matter which method you follow. The fact that there are fewer factors than bonds means that once you have as many bond prices as you have factors, you can derive all the oth ers by no arbitrage arguments and make this look like option pricing. Some derivations of term structure models follow this approach, setting up arbitrage portfolios. Solving the Bond Price Differential Equation Now we have to solve the partial differential equation 19.16 subject to the boundary condition P N 0, r 1. Solving this equation is straightforward conceptually and numerically. Express 19.16 as P P 1 2P2    rP. rr r N r 2 r2 WecanstartatN 0onagridofr,andP 0,r 1.ForfixedN,wecan work to one step larger N by evaluating the derivatives on the right hand side. The first step is P N,r P 0,r At the second step, P r N, 2P r2 0, so P 2 N , r P N , r 1 2r N r2 r r N2. Now the derivatives of r and r with respect to r will start to enter, and we let the computer take it from here. In practice, it would be better to solve in this way for the log price. Analytic solutions only exist in special cases, which we study next. P N N 1 r N. P N,r N N",
        "368 19. Term Structure of Interest Rates 19.5 Three Linear Term Structure Models As we have seen, term structure models are easy in principle and numer ically: specify a discount factor process and find its conditional expectation or solve the bond pricing partial differential equation back from maturity. In practice, the computations are hard. I present next three famous spe cial cases of term structure models specifications for the discount factor process that allow analytical or quickly calculable solutions. Analytical or close to analytical solutions are still important, because we have not yet found good techniques for reverse engineering the term structure. We know how to start with a discount factor process and find bond prices. We do not know how to start with the characteristics of bond prices that we want to model and construct an appropriate discount factor process. Thus, in evaluating term structure models, we will have to do lots of the forward calculations from assumed discount factor model to bond prices and it is important that we should be able to do them quickly. Vasicek Model via PDE The Vasicek 1977 model is a special case that allows a fairly easy analytic solution. The method is the same as the more complex analytic solution in the CIR and affine classes, but the algebra is easier, so this is a good place to start. The Vasicek discount factor process is Using this process in the basic bond differential equation 19.16 , we obtain I solve the Vasicek, the Cox, Ingersoll, and Ross, and the Affine model. Each model gives a linear function for log bond prices and yields, for example, ln P N , r A N B N r . d r dt  dz, dr  r r dt r dz. P 1 2P P P  r r 2 rP  . 19.18 r 2dr2r N rr I will solve this equation with the usual unsatisfying nonconstructive technique guess the functional form of the answer and show it is right.",
        "19.5. Three Linear Term Structure Models 369 I guess that log yields and hence log prices are a linear function of the short rate, P N,r eA N B N r. 19.19 I take the partial derivatives required in 19.18 and see if I can find A N and B N to make 19.18 work. The result is a set of ordinary differential equations for A N and B N , and these are of a particularly simple form that can be solved by integration. I solve them, subject to the boundary condition imposed by P 0, r 1. The result is B N 1 1 e N ,  12  2 A N r r r N B N r B N 2. 19.20 19.21 22 4 The exponential form of 19.19 means that log prices and log yields are linear functions of the interest rate, p N , r A N B N r , A N B N y N , r r . NN Solving the PDE: Details The boundary condition P 0, r 1 will be satisfied if A 0 B 0 r 0. Since this must hold for every r , we will need A 0 0, B 0 0. Given the guess 19.19 , the derivatives that appear in 19.18 are 1 P B N , P r 1 2P B N 2, P r2 1 P A N B N r. P N Substituting these derivatives in 19.18 , 2 B N  r r 1B N 22 A N B N r r B N  . rr",
        "370 19. Term Structure of Interest Rates This equation has to hold for every r, so the terms multiplying r and the 19.22 We can solve this pair of ordinary differential equations by simple integration. The second one is constant terms must separately be zero: A N 1 B N 2  2  r   B N , rr B N 1 B N . and hence A N 12B N 2r2 r r B N , 2 dB 1 B, dN dB 1 B dN , 1 ln 1 B N ,  B N 1 1 e N .  19.23 We solve the first equation in 19.22 by simply integrating it, and Note B 0 0 so we did not need a constant in the integration. choosing the constant to set A 0 0. Here we go: 2 A N r B N 2dN r r B N dN C, 2 2  r N 2N r N 1 2e e dN r 1 e dN C, A N N r N C. We pick the constant of integration to give A 0 0. You can do this explic itly, or figure out directly that the result is achieved by subtracting one from thee N terms, A N 22 r2 2e N e 2N r e N  22  2   2 2 e N 1 e 2N 1 A N r N 22  2 r e N 1 r N .",
        "19.5. Three Linear Term Structure Models 371 Now, we just have to make it pretty. I am aiming for the form given in 19.21 . Note Then We are done. Vasicek Model by Expectation What if we solve the discount rate forward and take an expectation instead? The Vasicek model is simple enough that we can follow this approach as well, and get the same analytic solution. The same methods work for the other models, but the algebra gets steadily worse. B N 2 1 1 2e N e 2N , 2 2 1 e N e 2N 1 B N 2 B N 2 2B N e 2N 1.  ,  2   r2r N 2B N B N B N r 22 2  N B N , A N A N r B N 2 r r r N B N . 2  2 4 22 The model is The bond price is d r dt  dz, dr  r r dt r dz. 19.24 19.25 19.26 00 P N E N . 0 Iuse0andN ratherthant andt N tosavealittlebitonnotation. To find the expectation in 19.26 , we have to solve the system 19.24 19.25 forward. The steps are simple, though the algebra is a bit daunting. First, we solve r forward. Then, we solve forward. ln t turns out to be conditionally normal, so the expectation in 19.26 is the expectation of a lognormal. Collecting terms that depend on r0 as the B N term, and the constant term as the A N term, we find the same solution as 19.20 19.21 .",
        "372 19. Term Structure of Interest Rates The interest rate is just an AR 1 . By analogy with a discrete time AR 1 , you can guess that its solution is dr t r tdt etdrt, dr t r tdt et r r dt etrdzt, dr t r t dt ete tr t dt etr dzt, dr t etr dzt. Then, This equation is easy to solve, r t r 0 r t s 0 t es dzs, t s 0 e  t s r dzs e t r0 1 e t r . To derive this solution, define r by rt 19.27 et rt r r0 r r r t et rt r . es dzs, rt r e t r0 r r t s 0 t1t ln ln r 2 ds  dz. t0s2 s s 0 e  t s dzs. Now, we solve the discount factor process forward. It is not pretty, but And we have 19.27 . it is straightforward: d 1d 2 12 dln r dt dz, ln t ln 0 s 0 e  s u r dzu u 0 t 2 2 t2 t s 0 Plugging in the interest rate solution 19.27 , ts s 0 1t e s r r r 2 ds  dz. 02 s s 0",
        "374 19. Term Structure of Interest Rates Plugging in the mean from 19.28 and the variance from 19.29 , 19.30 19.31 N 12 1 e N lnP r  N r r 02 0 12 r  N r r  1 e N 2 2 2 r 1 e 2N 43 All that remains is to make it pretty. To compare it with our previous result, we want to express it in the form ln P N , r0 A N B N r0. The coefficient on r0 19.30 is To simplify the constant term, recall that 19.32 implies B N the same expression we derived from the partial differential equation. 1 e N  , 19.32 1 e 2N  B N 2 2B N . Thus, the constant term the terms that do not multiply r0 in 19.30 is 1 1 e N 1 2 A N r 2N r r N 12 2 r  r r N B N r  B N 2 . 22  2 r r  1 e N r1 e 2N 2  43 112 r 2N r B N r N 22  2 r r  B N r B N 2 2B N  42 Again, this is the same expression we derived from the partial differential equation. This integration is usually expressed under the risk neutral measure. If we write the risk neutral process d dr  r r r dt rdz. 2 2  42 r dt,",
        "19.5. Three Linear Term Structure Models Then the bond price is The result is the same, of course. Cox Ingersoll Ross Model For the Cox Ingersoll Ross 1985 model dr  r r dt r rdz, our differential equation 19.16 becomes P 1 2P  r r P N,r eA N B N r. Substituting the derivatives of 19.34 into 19.33 , 375 be zero, 2 r dt  r dz, P N Ee N rsds. 0 s 0 d P r 2 r2r N rr P Guess again that log prices are a linear function of the short rate, 2r rP  r. 19.33 19.34 B N  r r 1B N 22r A N B N r r B N  r. rr Again, the coefficients on the constant and on the terms in r must separately B N 1 12B N 2   B N , 2r r A N B N r . 19.35 The ordinary differential equations 19.35 are quite similar to the Vasicek case, 19.22 . However, now the variance terms multiply an r , so the B N differential equation has the extra B N 2 term. We can still solve both dif ferential equations, though the algebra is a little bit more complicated. The result is B N A N 2 eN 1   r eN 1 2 , r 2 r 2ln 2  eN 1 2 N ,",
        "376 where 19. Term Structure of Interest Rates    2 22, r r    r . The CIR model can also be solved by expectation. In fact, this is how Cox, Ingersoll, and Ross 1985 actually solve it their marginal value of wealth JW isthesamethingasthediscountfactor.However,wheretheinterestrate in the Vasicek model was a simple conditional normal, the interest rate now has a noncentral 2 distribution, so taking the integral is a little messier. Multifactor Affine Models The Vasicek and CIR models are special cases of the affine class of term structure models Duffie and Kan 1996 , Dai and Singleton 2000 . These models allow multiple factors, meaning all bond yields are not just a function of the short rate. Affine models maintain the convenient form that log bond prices are linear functions of the state variables. This means that we can take K bond yields themselves as the state variables, and the yields will reveal anything of interest in the hidden state variables. The short rate and its volatility will be forecast by lagged short rates but also by lagged long rates or interest rate spreads. My presentation and notation are similar to Dai and Singleton s, but as usual I add the discount factor explicitly. Here is the affine model setup: dy  y y dt dw, r 0  y, 19.36 19.37 19.38 19.39 Equation 19.36 describes the evolution of the state variables. In the end, yields will be linear functions of the state variables, so we can take the state variables to be yields; thus I use the letter y. y denotes a K dimensional vector of state variables.  is now a K K matrix, y is a K dimensional vector, is a K K matrix. Equation 19.37 describes the mean of the discount factor or short rate as a linear function of the state variables. Equation 19.38 is the discount factor. b is a K dimensional vector that describes how the discount factor responds to the K shocks. The more responds to a shock, the higher the market price of risk of that shock. Equation 19.39 describes the shocks dw. The functional form nests the CIR square root type models d r dt b dw, dw   ydz, iiiiij E dz dz 0.",
        "19.5. Three Linear Term Structure Models 377 ifi 0andtheVasicektypeGaussianprocessifi 0.Youcannotpicki and  arbitrarily, as you have to make sure that   y 0 for all values of iii y that the process can attain. Duffie and Kan 1996 and Dai and Singleton characterize this admissibility criterion. We find bond prices in the affine setup following exactly the same steps as for the Vasicek and CIR models. Again, we guess that prices are linear functions of the state variables y: P N,y eA N B N y. We apply Ito s lemma to this guess, and substitute in the basic bond pricing equation 19.15 . We obtain ordinary differential equations that A N and B N must satisfy, i i I use the notation x i to denote the ith element of a vector x. As with the CIR and Vasicek models, these are ordinary differential equations that can be solved by integration starting with A 0 0, B 0 0. While they do not always have analytical solutions, they are quick to solve numerically much quicker than solving a partial differential equation. Derivation To derive 19.41 and 19.40 , we start with the basic bond pricing equation 19.15 , which I repeat here, 19.42 B N 1 2 B N B N b i B N i , 19.40 N i2i A N 1 2 B N b i B N i B N y 0. 19.41 N i2i The derivatives are dP 1 P dP d Et r dt Et . PP N P We need dP P . Using Ito s lemma, dP 1 P 1 1 2P dy dy dy. P P y 2P y y 1 P P y 1 2P P y y 1 P P N B N , B N B N , A N B N y. N N",
        "378 19. Term Structure of Interest Rates Thus, the first term in 19.42 is E dP B N  y y dt 1E dw B N B N dw . tP 2t Et dwi dwj 0, which allows us to simplify the last term. If w1 w2 0, then, bbbbw w bb w w w 1 1 1 2 1 b2w2 b2w2 b2w2. 12bbbbw1122 ii 21222 Applying the same algebra to our case,3 Etdw B N B N dw B N 2idwi2 i Iusethenotation x i todenotetheithelementoftheK dimensionalvector x. In sum, we have dP 1 2 E tP2iii i B N  y y dt B N   y dt. 19.43 i The right hand side term in 19.42 is matrices and the fact that the last term is a scalar to write E dw dP d tP dP d P B N dw dw b . ii Et dw dw is a diagonal matrix with elements   y . Thus, 19.44 i B N B N dw Tr E dw B N B N dw B N 2   y dt. i B N b   y. i ii i E 3 More elegantly, but less directly, we can use the fact that Tr AB Tr BA for square Tr E B N dwdw B N Tr B N E dwdw B N B N 2Edw2 . ii i ii",
        "19.6. Bibliography and Comments 379 Now, substituting 19.43 and 19.44 in 19.42 , along with the easier P N central term, we get 2i A N B N 1 2   y ii B N y   y B N  y y i 0 1 2 A N B N i B N b   y. i ii i N N Once again, the terms on the constant and each yi must separately be zero. The constant term: B N y i The terms multiplying y: 1 2 B N B N  y B N b ii, i B N y 0. 0 2i N i i B N y Taking the transpose and solving, B N b  y. ii A N 1 2 B N b i B N N i2i y  y 2ii N i ii ii B N 1 2  B N B N b i B N N i2i i 19.6 Bibliography and Comments i . The choice of discrete versus continuous time is really one of convenience. Campbell, Lo, and MacKinlay 1997 give a discrete time treatment, show ing that bond prices are linear functions of the state variables even in a discrete time two parameter square root model. Backus, Foresi, and Telmer 1998 is a good survey of bond pricing models in discrete time. Models also do not have to be affine. Constantinides 1992 is a nice discrete time model; its discount factor is driven by the squared value of AR 1 state variables. It gives closed form solutions for bond prices. The bond prices are not linear functions of the state variables, but it is the exis tence of closed forms rather than linearity of the bond price function that makes affine models so attractive. Constantinides model allows for both signs of the term premium, as we seem to see in the data.",
        "380 19. Term Structure of Interest Rates So far most of the term structure literature has emphasized the risk neutral probabilities, rarely making any reference to the separation between drifts and market prices of risk. This is not a serious shortcoming for option pricing uses, for which modeling the volatilities is much more important than modeling the drifts. It is also not a serious shortcoming when the model is used to draw smooth yield curves across maturities. However, it makes the models unsuitable for bond portfolio analysis and other uses. Many mod els imply high and time varying market prices of risk or conditional Sharpe ratios. Recently, Backus, Foresi, Mozumdar, and Wu 1997 , Duffee 1999 , Duarte 2000 , and Dai and Singleton 2002 have started the important task of specifying term structure models that fit the empirical facts about expected returns in term structure models. In particular, they try to fit the Fama Bliss 1987 and Campbell and Shiller 1991 regressions that relate expected returns to the slope of the term structure see Chapter 2 , while maintaining the tractability of affine models. Cochrane and Piazzesi 2003 extend the Fama Bliss regressions, finding significantly greater predictabil ity in bond returns, and show how to reverse engineer an affine model to incorporate any pattern of predictability. Term structure models used in finance amount to regressions of inter est rates on lagged interest rates. Macroeconomists also run regressions of interest rates on a wide variety of variables, including lagged inter est rates, but also lagged inflation, output, unemployment, exchange rates, and so forth. They often interpret these equations as the Federal Reserve s policy making rule for setting short rates as a function of macro economic conditions. This interpretation is particularly clear in the Taylor rule literature Taylor 1999 and monetary VAR literature; see Christiano, Eichenbaum, and Evans 1999 and Cochrane 1994b for surveys. Some one, it seems, is missing important right hand variables. Cochrane and Piazzesi 2002 use long maturity yields to isolate monetary policy shocks in high frequency data and find that this change has important effects on the estimated responses. The criticism of finance models is stinging when we use the short rate as the only state variable. Multifactor models are more subtle. If any variable forecasts future interest rates, then it becomes a state variable, and it should be revealed by bond yields. Thus, bond yields should completely drive out any other macroeconomic state variables as interest rate forecasters. They do not, which is an interesting observation. In addition, there is an extensive literature that studies yields from a purely statistical point of view, Gallant and Tauchen 1997 for example, and a literature that studies high frequency behavior in the federal funds market, for example Hamilton 1996 . Obviously, these three literatures need to become integrated. Balduzzi, Bertola, and Foresi 1996 consider a model based on the federal",
        "19.6. Bibliography and Comments 381 funds target, and Piazzesi forthcoming integrated a careful specification of high frequency moves in the federal funds rate into a term structure model. Cochrane and Piazzesi 2002 use interest rates in a monetary VAR. The models studied here are all based on diffusions with rather slow moving state variables. These models generate one day ahead densities that are almost exactly normal. In fact, as Das 2002 and Johannes 2000 point out, one day ahead densities have much fatter tails than normal dis tributions predict. This behavior could be modeled by fast moving state variables. However, it is more natural to think of this behavior as generated by a jump process, and Johannes nicely fits a combined jump diffusion for yields. This specification can change pricing and hedging characteristics of term structure models significantly. All of the term structure models in this chapter describe many bond yields as a function of a few state variables. This is a reasonable approxi mation to the data. Almost all of the variance of yields can be described in terms of a few factors, typically a level , slope , and hump factor. Knez, Litterman, and Scheinkman 1994 and Litterman and Scheinkman 1991 make the point with a formal maximum likelihood factor analysis, but you can see the point with a simple eigenvalue decomposition of log yields. See Table 19.1. Not only is the variance of yields well described by a factor model, but the information in current yields about future yields the expected changes in yields and the conditional volatility of yields is well captured by one level and a few spreads as well. It is a good approximation, but it is an approximation. Actual bond prices do not exactly follow any smooth yield curve, and the covariance Table 19.1 Maturity 12345 6.36 0.45 0.61 0.75 0.10 0.47 0.08 0.10 0.07 0.07 0.45 0.21 0.62 0.49 0.36 0.45 0.44 0.44 0.12 0.36 0.50 0.41 0.11 0.46 0.39 0.55 0.55 0.68 0.60 0.21 Level Slope Curvature Eigenvalue decomposition of the covariance matrix of zero coupon bond yields, 1952 1997. The first column gives the square root of the eigenvalues. The columns marked 1 5 give the eigenvectors corresponding to 1 5 year zero coupon bond yields. I decomposed the covariance matrix as Q Q ; 2 gives the diagonal entries in and the rest of the table gives the entries of Q . With this decomposition, we can say that bond yields are generated by y Q 1 2, E  I, thus Q give loadings on the shocks .",
        "382 19. Term Structure of Interest Rates matrix of actual bond yields does not have an exact K factor structure the remaining eigenvalues are not zero. Hence you cannot estimate a term structure model directly by maximum likelihood; you either have to estimate the models by GMM, forcing the estimate to ignore the stochastic singularity, or you have to add distasteful measurement errors. As always, the importance of an approximation depends on how you use the model. If you take the model literally, a bond whose price deviates by one basis point is an arbitrage opportunity. In fact, it is at best a good Sharpe ratio, but a K factor model will not tell you how good it will not quantify the risk involved in using the model for trading purposes. Hedging strate gies calculated from K factor models may be sensitive to small deviations as well. One solution has been to pick different parameters at each point in time Ho and Lee 1986 . This approach is useful for derivative pricing, but is obviously not a satisfactory solution. Models in which the whole yield curve is a state variable, Kennedy 1994 , Santa Clara and Sornette 2001 , are another interesting response to the problem, and potentially provide a realistic description of the data. The market price of interest rate risk reflects the market price of real interest rate changes and the market price of inflation or whatever real factors are correlated with inflation and explain investors fear of it. The relative contributions of inflation and real rates in interest rate changes are very important for the nature of the risks that bondholders face. For example, if real rates are constant and nominal rates change on inflation news, then short term bonds are the safest real long term investment. If inflation is constant and nominal rates change on real rate news, then long term bonds are the safest long term investment. The data seem to suggest a change in regime between the 1970s and 1990s: in the 1970s, most inter est rate changes were due to inflation, while the opposite seems true now. Despite all these provocative thoughts, though, little empirical work has been done that usefully separates interest rate risk premia into real and inflation premium components. Buraschi and Jiltsov 1999 is one recent effort in this direction, but a lot more remains to be done. Problems Chapter 19 1. Complete the proof that each of the three statements of the expecta tions hypothesis implies the other. Is this also true if we add a constant risk premium? Are the risk premia in each of the three statements of the yield curve of the same sign?",
        "Problems 383 2. Under the expectations hypothesis, if long term yields are higher than short term yields, does this mean that future long term rates should go up, down, or stay the same? Hint: a plot of the expected log bond prices over time will really help here. 3. Start by assuming risk neutrality, E HPR N Y 1 for all maturities N . t 1 t Try to derive the other representations of the expectations hypothesis. Now you see why we specify that the expected log returns are equal. 4. Look at 19.13 and show that adding orthogonal dw to the discount factor has no effect on bond pricing formulas. 5. Look at 19.13 and show that P e rT if interest rates are constant, i.e., if d r dt  dz. 6. Show that if interest rates follow a Gaussian AR 1 process dr  r r dt dz and the market price of interest rate risk is zero, d 7. Show that a flat yield curve that shifts up and down is impossible. Start with 19.2 . If yields follow y N,t y t ; dy t  y dt  y dz find holding period returns on N year zeros. Show that the Sharpe ratio increases to infinity as N grows. r dt, then the expectation hypothesis with constant risk premia holds."
    ],
    "Topic 10": [
        "20 Expected Returns in the Time Series and Cross Section The first revolution in finance started the modern field. Peaking in the early 1970s, this revolution established the CAPM, random walk, efficient markets, portfolio based view of the world. The pillars of this view are: 1. The CAPM is a good measure of risk and thus a good explanation why some stocks, portfolios, strategies, or funds assets, generically earn higher average returns than others. 2. Returns are unpredictable. In particular, a Stock returns are close to unpredictable. Prices are close to random walks; expected returns do not vary greatly through time. Techni cal analysis that tries to divine future returns from past price and volume data is nearly useless. Any apparent predictability is either a statistical artifact which will quickly vanish out of sample, or can not be exploited after transactions costs. The near unpredictability of stock returns is simply stated, but its implications are many and subtle. Malkiel 1990 is a classic and easily readable introduction. It also remains widely ignored, and therefore is the source of lots of wasted trading activity. b Bond returns are nearly unpredictable. This is the expectations model of the term structure. If long term bond yields are higher than short term yields if the yield curve is upward sloping this does not mean that expected long term bond returns are any higher than those on short term bonds. Rather, it means that short term interest rates are expected to rise in the future, so you expect to earn about the same amount on short term or long term bonds at any horizon. c Foreign exchange bets are not predictable. If a country has higher interest rates than are available in the United States for bonds of a 389",
        "390 d 20. Expected Returns in the Time Series and Cross Section similar risk class, its exchange rate is expected to depreciate. After you convert your investment back to dollars, you expect to make the same amount of money holding foreign or domestic bonds. Stock market volatility does not change much through time. Not only are returns close to unpredictable, they are nearly identically distributed as well. 3. Professional managers do not reliably outperform simple indices and passive portfolios once one corrects for risk beta . While some do better than the market in any given year, some do worse, and the outcomes look very much like good and bad luck. Managers who do well in one year are not more likely to do better than average the next year. The average actively managed fund does about 1 worse than the market index. The more actively a fund trades, the lower returns to investors. Together, these views reflected a guiding principle that asset markets are, to a good approximation, informationally efficient Fama 1970, 1991 . This statement means that market prices already contain most information about fundamental value. Informational efficiency in turn derives from competition. The business of discovering information about the value of traded assets is extremely competitive, so there are no easy quick profits to be made, as there are not in every other well established and competitive industry. The only way to earn large returns is by taking on additional risk. These statements are not doctrinaire beliefs. Rather, they summarize the findings of a quarter century of extensive and careful empirical work. However, every single one of them has now been extensively revised by a new generation of empirical research. Now, it seems that: 1. There are assets, portfolios, funds, and strategies whose average returns cannot be explained by their market betas. Multifactor models dominate the empirical description, performance attribution, and explanation of average returns. 2. Returns are predictable. In particular, a Variablesincludingthedividend priceratioandtermpremiumcan in fact predict substantial amounts of stock return variation. This phenomenon occurs over business cycle and longer horizons. Daily, weekly, and monthly stock returns are still close to unpredictable, and technical systems for predicting such movements are still close to useless after transactions costs. b Bondreturnsarepredictable.Thoughtheexpectationsmodelworks well in the long run, a steeply upward sloping yield curve means that expected returns on long term bonds are higher than on short term bonds for the next year. c Foreign exchange returns are predictable. If you buy bonds in a country whose interest rates are unusually higher than those in the",
        "20.1. Time Series Predictability 391 United States, you expect a greater return, even after converting back to dollars. d Stockmarketvolatilitydoesinfactchangethroughtime.Conditional second moments vary through time as well as first moments. Means and variances do not seem to move in lockstep, so conditional Sharpe ratios vary through time. 3. Somefundsseemtooutperformsimpleindices,evenaftercontrollingfor risk through market betas. Fund returns are also slightly predictable: past winning funds seem to do better in the future, and past losing funds seem to do worse than average in the future. For a while, this seemed to indicate that there is some persistent skill in active management. However, we now see that multifactor performance attribution models explain most fund persistence: funds earn persistent returns by following fairly mechanical styles, not by persistent skill at stock selection Carhart 1997 . Again, these views summarize a large body of empirical work. The strength and interpretation of many results are hotly debated. This new view of the facts need not overturn the view that markets are reasonably competitive and therefore reasonably efficient. It does substan tially enlarge our view of what activities provide rewards for holding risks, and it challenges our economic understanding of those risk premia. As of the early 1970s, asset pricing theory anticipated the possibility and even proba bility that expected returns should vary over time and that covariances past market betas would be important for understanding cross sectional varia tion in expected returns. What took another 15 to 20 years was to see how important these long anticipated theoretical possibilities are in the data. 20.1 Time Series Predictability I start by looking at patterns in expected returns over time in large market indices, and then look at patterns in expected returns across stocks. Long Horizon Stock Return Regressions Dividend price ratios forecast excess returns on stocks. Regression coef ficients and R2 rise with the forecast horizon. This is a result of the fact that the forecasting variable is persistent.",
        "20.1. Time Series Predictability 395 The numerator in the long horizon regression coefficient is E rt 1 rt 2 rt k xt , 20.3 where the symbols represent deviations from their means. With stationary r and x, E rt j xt E rt 1xt j , so this is the same moment as E rt 1 xt xt 1 xt 2 , 20.4 the numerator of a regression coefficient of one year returns on many lags of price dividend ratios. Of course, if you run a multiple regression of returns on lags of p d, you quickly find that most lags past the first do not help the forecast power. That statement would be exact in the AR 1 example. This observation shows once again that one year and multiyear fore castability are two sides of the same coin. It also suggests that on a purely statistical basis, there will not be a huge difference between one year return forecasts and multiyear return forecasts correcting the latter for the serial correlation of the error term due to overlap . Hodrick 1992 comes to this conclusion in a careful Monte Carlo experiment, comparing moments of the form 20.3 , 20.4 , and E rt 1xt . Also, Jegadeesh 1991 used the equivalence between 20.3 and 20.4 to test for long horizon predictability using one month returns and a moving average of instruments. The direct or implied multiyear regressions are thus mostly useful for illustrating the dramatic economic implications of forecastability, rather than as clever statis tical tools that enhance power and allow us to distinguish previously foggy hypotheses. The slow movement of the price dividend ratio means that on a purely statistical basis, return forecastability is an open question. What we really know see Figure 20.1 is that low prices relative to dividends and earnings in the 1950s preceded the boom market of the early 1960s; that the high price dividend ratios of the mid 1960s preceded the poor returns of the 1970s; that the low price ratios of the mid 1970s preceded the current boom. We really have three postwar data points: a once per generation change in expected returns. In addition, the last half of the 1990s has seen a his torically unprecedented rise in stock prices and price dividend ratios or any other ratio . This rise has cut the postwar return forecasting regression coefficient in half. On the other hand, another crash or even just a decade of poor returns will restore the regression. Data back to the 1600s show the same pattern, but we are often uncomfortable making inferences from centuries old data.",
        "20.1. Time Series Predictability 411 and French 1988a ran regressions of long horizon returns on past long horizon returns, rt t k a bkrt k t t k, 20.28 basically updating classic autocorrelation tests from the 1960s to long horizon data. They found negative and significant b coefficients: a string of good past returns forecasts bad future returns. Poterba and Summers 1988 considered a related variance ratio statistic. If stock returns are i.i.d., then the variance of long horizon returns should grow with the horizon, var rt t k var rt 1 rt 2 rt k kvar rt 1 . They computed the variance ratio statistic 1 var rt t k k var rt 1 20.29 vk . They found variance ratios below one. Stocks, it would seem, really are safer for long run investors who can afford to wait out the ups and downs of the market, common Wall Street advice, long maligned by academics. These two statistics are closely related, and reveal the same basic fact: stock returns have a string of small negative autocorrelations. To see this relation, write the variance ratio statistic kj 1 rt j and write the regression coefficient in 20.28 , vk 1 var k var rt 1 k j k k j k j 1 2 k k j k j j k j k k j k j. 1 k k b cov r, r k var rt t k t j t j 1 j 1 j, 20.30 k j 1 kvar rt 1 k k j j 1 1 k var rt t k k vk k Both statistics are based on tent shaped sums of autocorrelations, as illus trated by Figure 20.3. If there are many small negative autocorrelations which bring returns back slowly after a shock, these autocorrelations might be individually insignificant. Their sum might be economically and sta tistically significant, however, and these two statistics will reveal that fact",
        "412 20. Expected Returns in the Time Series and Cross Section Figure 20.3. Long horizon regression and variance ratio weights on autocorrelations. by focusing on the sum of autocorrelations. The long horizon regression weights emphasize the middle of the autocorrelation function, so a k year horizon long horizon regression is comparable to a somewhat longer variance ratio. Impulse Response Function and Mean Reversion We think of many negative higher order autocorrelations as bringing prices back after a shock, so it is natural to characterize mean reversion via the impulse response function of prices to a shock directly. If, after a shock, prices are expected to trend upward, we have momentum. If, after a shock, prices are expected to come back a bit, we have mean reversion. To think about this characterization more precisely, start by writing returns as a moving average of their own shocks. From a regression of returns on past returns a L rt t youcanfindthej intherepresentation j 0 20.31 rt jt j  L t a L 1t. Mostsimply,justsimulate 20.31 forward. Thej arethemovingaverage representation or impulse response function they tell you what happens toallfutureexpectedreturnsfollowingashock.Letpt representthecumu lative returns, or the log value of a dollar invested, pt pt 1 rt . Then, the partial sum k j tells you the effect on invested wealth pt k of a uni j 1 variate shock t. The sum j 1 j  1 measures the long run effect",
        "20.1. Time Series Predictability 413 of a shock. I normalize the scale of the moving average representation with 0 1. A natural measure of mean reversion, then, is whether the long run effect of a shock is greater or less than its instantaneous effect whether  1 is greater than, equal to, or less than one. This measure is also closely related to autocorrelations, long horizon regressions, and variance ratios, by  1 2 limvk 1 2 j 2. 20.32 If returns are i.i.d., the variance ratio is one at all horizons; all autocorrela tions are zero, and all  past the first are zero so  1 1, 2 1. A long j string of small negative autocorrelations means a variance ratio less than k j 1 j 0 j in 20.30 . For the second equality, you can recognize in both expressions the spectral density of r at frequency zero. The 2 term enters because j the variance ratio is scaled by the variance of the series, and the long run response is scaled by the impact response, or equivalently by the variance of the shock to the series. It is possible in extreme cases for the variance ratio to be below one, but  1 1 and vice versa. Numbers Table 20.5 presents long horizon return regressions and an estimate of the variance of long horizon returns. The long horizon regressions do show some interesting mean reversion, especially in the 2 4 year range. However, that turns around at year 7 and disappears by year 10. The variance ratios j 1 so the long run effect on price is lower than the impact effect that is mean reversion. one, and means j 0 The right hand equality of 20.32 follows by just taking the k Table 20.5. Mean reversion using logs, 1926 1996 Horizon k years 1 2 3 5 7 10  rk k k Sharpe k 19.8 20.6 0.08 0.15 0.31 0.30 19.7 18.2 0.22 0.04 0.30 0.31 16.5 16.3 0.24 0.08 0.36 0.39 r denotes the difference between the log value weighted NYSE return and the log treasury bill return.  rk  rt t k is the variance of long horizon returns. k is the long horizon regression coefficient in rt t k  krt k t t k. The Sharpe ratio is E rt t k  rt t k .",
        "414 20. Expected Returns in the Time Series and Cross Section do show some long horizon stabilization. At year 10, the variance ratio is 16.3 19.8 2 0.68. The last row of Table 20.5 calculates Sharpe ratios, to evaluate whether stocks really are safer in the long run. The mean log return grows linearly with horizon whether returns are autocorrelated or not E r1 r2 2E r . If the variance also grows linearly with the horizon, as it does for nonauto correlated returns, then the Sharpe ratio grows with the square root of the horizon. If the variance grows more slowly than the horizon, then the Sharpe ratio grows faster than the square root of the horizon. This is the fundamental question for whether stocks are unconditionally safer for the long run. Table 20.5 includes the long horizon Sharpe ratios, and you can see that they do increase. You would not be to blame if you thought that the evidence of Table 20.5 was rather weak, especially compared with the dramatic dividend price regressions. It is, and it is for this reason that most current evidence for predictability focuses on other variables such as the d p ratio. In addition, Table 20.6 shows that the change from log returns to levels of returns, while having a small effect on long horizon regressions, destroys any evidence for higher Sharpe ratios at long horizons. Table 20.7 shows the same results in the postwar period. Some of the negative long horizon regression coefficients are negative and significant, but there are just as large positive coefficients, and no clear pattern. The variance ratios are flat or even rising with horizons, and the Sharpe ratios are flat or even declining with horizon. In sum, the direct evidence for mean reversion in index returns seems quite weak. I consider next whether indirect evidence, values of these statis tics implied by other estimation techniques, still indicate mean reversion. The mean reversion of individual stock returns as examined by Fama and French 1988a is somewhat stronger, and results in the stronger cross sectional reversal effect described in Section 20.2. Table 20.6. Mean reversion using gross returns, 1926 1996  rk k k Sharpe k 20.6 22.3 0.02 0.21 0.41 0.41 22.5 24.9 28.9 39.5 0.22 0.03 0.22 0.63 0.41 0.40 0.40 0.38 Horizon k years 1 2 3 5 7 10 r denotes the difference between the gross not log long horizon value weighted NYSE return and the gross treasury bill return.",
        "20.1. Time Series Predictability 421 In our simple setup, we can write the return d p VAR 20.17 20.18 as Then, write returns as 1 b 1 bL 1 bL rt 1 1 b dpt dt 1 dpt 1 b dt dpt , and hence, 1 bL rt 1 d t 1 dp t 1 dp t bdt . 20.40 Here, you can see that rt must follow an ARMA 1,1 with one root equal tobandtheotherroottobedetermined.Defineyt 1 bL rt,andthus yt 1 L t.Thentheautocovariancesofyfrom 20.40 are E y2 1 b2 2  1 2 2  2  b   , , t 1 d dp ddp rt 1 1 b dt pt dt 1 dpt 1 , dt 1 pt 1 b dt pt dpt 1. rt 1 dpt dt 1 dpt 1 , E yt 1 yt b  2 d   2 dp 1  b  d , dp , whileyt 1 L t implies t 1 t  Hence, we can find  from the condition 1 2 2q. 1 2 2, t 1  E y2 E y y 2. 1 b2 2 d 1 2 2 dp 2  b  d,dp  b2 d 2 dp 1 b  d,dp The solution the root less than one is  q q2 1. For more general processes, such as computations from an estimated VAR, it is better to approach the problem via the spectral density. This approach allows you to construct the univariate representation directly with out relying on cleverness. If you write yt rt xt , the VAR is yt A L t. Then spectral density of returns Sr z is given by the top left element of Sy z A z E  A z 1 with z e i. Like the autocorrelation, the spectral density is the same object whether it comes from the univari ate or multivariate representation. You can find the autocorrelations by",
        "20.1. Time Series Predictability 427 Table 20.8. Average continuously compounded log one year holding period returns on zero coupon bonds of varying maturity Maturity Avg. Return Std. N E hpr N error t 1 t 1 1 5.83 0.42 2 6.15 0.54 3 6.40 0.69 4 6.40 0.85 5 6.36 0.98 Annual data from CRSP 1953 1997. 2.83 3.65 4.66 5.71 6.58 Std. dev.  hpr N the increasing standard deviation of bond returns as maturity rises. The small increase in returns for long term bonds, equivalent to a slight average upward slope in the yield curve, is usually excused as a small liquidity pre mium. In fact, the curious pattern in Table 20.8 is that bonds do not share the high Sharpe ratios of stocks. Whatever factors account for the volatility of bond returns, they seem to have very small risk prices. Table 20.8 is again a tip of an iceberg of an illustrious career for the expectations hypothesis. Especially in times of great inflation and exchange rate instability, the expectations hypothesis does a very good first order job. However, one can ask a more subtle question. Perhaps there are times when long term bonds can be forecast to do better, and other times when short term bonds are expected to do better. If the times even out, the uncon ditional averages in Table 20.8 will show no pattern. Equivalently, we might want to check whether a forward rate that is unusually high forecasts an unusual increase in spot rates. Table 20.9 gets at these issues, updating Fama and Bliss 1987 classic regression tests. Campbell and Shiller 1991 and Campbell 1995 make the same point with regressions of yield changes on yield spreads. The left hand panel presents a regression of the change in yields on the forward spot spread. The expectations hypothesis predicts a coefficient of 1.0, since the forward rate should equal the expected future spot rate. At a one year horizon we see instead coefficients near zero and a negative adjusted R2. Forward rates one year out seem to have no predictive power whatsoever for changes in the spot rate one year from now. On the other hand, by four years out, we see coefficients within one standard error of 1.0. Thus, the expectations hypothesis seems to do poorly at short 1 year horizons, but much better at longer horizons and on average Table 20.8 . If the yield expression of the expectations hypothesis does not work at one year horizons, then the expected return expression of the expectations",
        "428 y 1 t N 20. Expected Returns in the Time Series and Cross Section Table 20.9. Forecasts based on forward spot spread y 1 t Change in yields Holding period returns hpr N 1 y 1 t 1 t a b f N N 1 y 1 t N tt tt a b f N N 1 y 1 t 1 N a  a b  b R2 a  a b 0.1 0.3 1.10 0.5 0.5 1.46 0.4 0.8 1.30 0.5 1.0 1.31  b R2 0.36 0.16 0.44 0.19 0.54 0.10 0.63 0.07 1 0.1 0.3 2 0.01 0.4 3 0.04 0.5 4 0.3 0.5 0.10 0.36 0.37 0.33 0.41 0.33 0.77 0.31 0.02 0.005 0.013 0.11 OLS regressions 1953 1997 annual data. Yields and returns in annual percentages. The left hand panel runs the change in the one year yield on the forward spot spread. The right hand panel runs the one period excess return on the forward spot spread. hypothesis must not hold either one must be able to forecast one year bond returns. To check this fact, the right hand panel of Table 20.9 runs regressions of the one year excess return on long term bonds on the forward spot spread. Here, the expectations hypothesis predicts a coefficient of zero: no signal including the forward spot spread should be able to tell you that this is a particularly good time for long bonds versus short bonds. As you can see, the coefficients in the right hand panel of Table 20.9 are all about 1.0. A high forward rate does not indicate that interest rates will be higher one year from now; it seems entirely to indicate that you will earn that much more holding long term bonds. The coefficients in yield and return regressions are linked. For example in the first row 1.10 0.10 1.0, and this holds as an identity. Fama and Bliss call them complementary regressions. Figures 20.6 and 20.7 provide a pictorial version of the results in Table 20.9. Suppose that the yield curve is upward sloping as in the top panel. What does this mean? A naive investor might think this pattern indi cates that long term bonds give a higher return than short term bonds. The expectations hypothesis denies this conclusion. If the expectations hypoth esis were true, the forward rates plotted against maturity in the top panel would translate one for one to the forecast of future spot rates in the bottom panel, as plotted in the line marked Expectations model. Rises in future short rates should lower bond prices, cutting off the one period advantage of long term bonds. The rising short rates would directly raise the multiyear advantage of short term bonds. We can calculate the actual forecast of future spot rates from the esti mates in the left hand panel of Table 20.9, and these are given by the line marked Estimates in Figure 20.7. The essence of the phenomenon is slug gish adjustment of the short rates. The short rates do eventually rise to meet",
        "430 20. Expected Returns in the Time Series and Cross Section the forward rate forecasts, but not as quickly as the forward rates predict that they should. As dividend growth should be forecastable so that returns are not fore castable, short term yields should be forecastable so that returns are not forecastable. In fact, yield changes are almost unforecastable at a one year horizon, so, mechanically, bond returns are. We see this directly in the first row of the left hand panel of Table 20.9 for the one period yield. It is an implication of the right hand panel as well. If p N p N 1 p 1 0 1 p N p N 1 p 1  , t 1tt tttt 1 p N 0 1 p N  , t 1 t t 1 y N 0 1y N  N. t 1 t t 1 hpr N 1 y 1 0 1 f N N 1 y 1  t 1 t t t t 1 , 20.42 then, writing out the definition of holding period return and forward rate, A coefficient of 1.0 in 20.42 is equivalent to yields or bond prices that follow random walks: yield changes that are completely unpredictable. Of course yields are stationary and not totally unpredictable. How ever, they move slowly. Thus, yield changes are very unpredictable at short horizons but much more predictable at long horizons. That is why the coef ficients in the right hand panel of Table 20.9 build with horizon. If we did holding period return regressions at longer horizons, they would gradually approach the expectations hypothesis result. The roughly 1.0 coefficients in the right hand panel of Table 20.9 mean that a one percentage point increase in forward rate translates into a one percentage point increase in expected return. It seems that old fallacy of confusing bond yields with their expected returns also contains a grain of truth, at least for the first year. However, the one for one variation of expected returns with forward rates does not imply a one for one variation of expected returns with yield spreads. Forward rates are related to the slope of the yield curve, f N N 1 y 1 p N p N 1 y 1 ttttt Ny N N 1 y N 1 y 1 ttt N y N 1 y N y N 1 y 1 . tttt Thus, the forward spot spread varies more than the yield spread, so regres sion coefficients of holding period yields on yield spreads give coefficients 20.43",
        "20.1. Time Series Predictability 431 greater than one. Expected returns move more than one for one with yield spreads. Campbell 1995 reports coefficients of excess returns on yield spreads that rise from one at a two month horizon to 5 at a five year horizon. The facts are analogous to the dividend price regression. There, div idends should be forecastable so that returns are not forecastable. But dividends were essentially unforecastable and the dividend yield was per sistent. These facts implied that a one percentage point change in dividend yield implied a 3 5 percentage point change in expected excess returns. Of course, there is risk: the R2 are all about 0.1 0.2, about the same values as the R2 from the dividend price regression at a one year horizon, so this strategy will often go wrong. Still, 0.1 0.2 is not zero, so the strategy does pay off more often than not, in violation of the expectations hypothesis. Furthermore, the forward spot spread is a slow moving variable, typically reversing sign once per business cycle. Thus, the R2 build with horizon as with the D P regression, peaking in the 30 range Fama and French 1989 . Also, Cochrane and Piazzesi 2003 extend these regressions to more maturities on the right hand side, and find R 2 as high as 44 . The fact that the regressions in Table 20.9 run the change in yield and the excess return on the forward spot spread is very important. The over all level of interest rates moves up and down a great deal but slowly over time. Thus, if you run y N a bf N 1  , you will get a coefficient t j t t N b almost exactly equal to 1.0 and a stupendous R2, seemingly a stunning validation of the expectations hypothesis. If you run a regression of tomor row s temperature on today s temperature, the regression coefficient will be near 1.0 with a huge R2 as well, since the temperature varies a lot over the year. But today s temperature is not a useful temperature forecast. To measure a temperature forecast we want to know if the forecast can predict the change in temperature. Is forecast today s temperature a good mea sure of tomorrow s temperature today s temperature ? Table 20.9 runs this regression. The decomposition in 20.43 warns us of one of several econometric traps in this kind of regression. Notice that two of the three right hand variables are the same. Thus any measurement error in p N 1 and p 1 will tt induce a spurious common movement in left and right hand variables. In addition, since the variables are a triple difference, the difference may elim inate a common signal and isolate measurement error or noise. There are pure measurement errors in the bond data, and we seldom observe pure discount bonds of the exactly desired maturity. In addition, various liquidity and microstructure effects can influence the yields of particular bonds in ways that are not exploitable for typical investors. As an example of what this sort of measurement error can do, suppose all bond yields are 5 , but there is one error in the two period bond price",
        "432 20. Expected Returns in the Time Series and Cross Section Table 20.10. Numerical example of the effect of measurement error in yields on yield regressions t0123 p 1 5 t 5 5 5 p 2 t p 3 t y i ,i 2 t y 2 t f 1 2 t f 1 2 y 1 tt hpr 2 1 y 1 tt 10 15 15 15 5 5 5 7.5 5 10 0 5 0 0 10 10 15 15 5 5 5 5 5 5 0 0 5 0 at time 1: rather than being 10 it is 15. Table 20.10 tracks the effects of this error. It implies a blip of the one year forward rate in year one, and then a blip in the return from holding this bond from year one to year two. The price and forward rate error automatically turns into a subsequent return when the error is corrected. If the price is real, of course, this is just the kind of event we want the regression to tell us about the forward rate did not correspond to a change in future spot rate, so there was a large return; it was a price that was out of line and if you could trade on it, you should. But the regression will also pounce on measurement error in prices and indicate spuriously forecastable returns. Foreign Exchange Suppose interest rates are higher in Germany than in the United States. Does this mean that one can earn more money by investing in German bonds? There are several reasons that the answer might be no. First, of course, is default risk. While not a big problem for German government bonds, Russia and other governments have defaulted on bonds in the past The expectations model works well on average. However, a foreign inter est rate one percentage point higher than its usual differential with the U.S. rate equivalently, a one percentage point higher forward spot spread seems to indicate even more than one percentage point expected excess return; a further appreciation of the foreign currency.",
        "20.1. Time Series Predictability 433 and may do so again. Second, and more important, is the risk of devaluation. If German interest rates are 10 , U.S. interest rates are 5 , but the Euro falls 5 relative to the dollar during the year, you make no more money holding the German bonds despite their attractive interest rate. Since lots of investors are making this calculation, it is natural to conclude that an interest rate differential across countries on bonds of similar credit risk should reveal an expectation of currency devaluation. The logic is exactly the same as the expectations hypothesis in the term structure. Initially attractive yield or interest rate differentials should be met by an offsetting event so that you make no more money on average in one country or another, or in one maturity versus another. As with bonds, the expectations hypothesis is slightly different from pure risk neutrality since the expectation of the log is not the log of the expectation. Again, the size of the phenomena we study usually swamps this distinction. As with the expectations hypothesis in the term structure, the expected depreciation view ruled for many years, and still constitutes an important first order understanding of interest rate differentials and exchange rates. For example, interest rates in east Asian currencies were very high on the eve of the currency collapses of 1997, and many banks were making tidy sums borrowing at 5 in dollars to lend at 20 in local currencies. This situation should lead one to suspect that traders expect a 15 devaluation, or a small chance of a larger devaluation. That is, in this case, exactly what happened. Many observers and policy analysts who ought to know better often attribute high nominal interest rates in troubled countries to tight monetary policy that is strangling the economy to defend the currency. In fact, one s first order guess should be that such high nominal rates reflect a large probability of devaluation loose monetary and fiscal policy and that they correspond to much lower real rates. Still, does a 5 interest rate differential correspond to an exactly 5 expected depreciation, or does some of it still represent a high expected return from holding debt in that country s currency? Furthermore, while expected depreciation is clearly a large part of the story for high interest rates in countries that have constant high inflation or that may suffer spectacular depreciation of a pegged exchange rate, how does the story work for, say, the United States versus Germany, where inflation rates diverge little, yet exchange rates fluctuate a surprisingly large amount? Table 20.11 presents the facts, as summarized by Hodrick forthcoming and Engel 1996 . The first row of Table 20.11 presents the average appre ciation of the dollar against the indicated currency over the sample period. The dollar fell against DM, yen, and Swiss Franc, but appreciated against the pound. The second row gives the average interest rate differential the amount by which the foreign interest rate exceeds the U.S. interest rate. According to the expectations hypothesis, these two numbers should",
        "434 20. Expected Returns in the Time Series and Cross Section Mean appreciation Mean interest differential b , 1975 1989 R 2 b, 1976 1996 Table 20.11. DM 1.8 3.6 3.9 2.1 3.1 2.0 .026 .033 0.7 1.8 SF 5.0 3.0 3.7 5.9 2.1 2.6 .034 .033 2.4 1.3 The first row gives the average appreciation of the dollar against the indicated currency, in percent per year. The second row gives the average interest differ ential foreign interest rate less domestic interest rate, measured as the forward premium the 30 day forward rate less the spot exchange rate. The third through fifth rows give the coefficients and R2 in a regression of exchange rate changes on the interest differential forward premium, s st a b ft st  a b rf rd  t 1 t 1 t t t 1 , where s log spot exchange rate, f forward rate, rf foreign interest rate, r d domestic interest rate. Source: Hodrick forthcoming and Engel 1996 . be equal interest rates should be higher in countries whose currencies depreciate against the dollar. The second row shows roughly the right pattern. Countries with steady long term inflation have steadily higher interest rates, and steady depreci ation. The numbers in the first and second rows are not exactly the same, but exchange rates are notoriously volatile so these averages are not well measured. Hodrick shows that the difference between the first and second rows is not statistically different from zero. This fact is exactly analogous to the fact of Table 20.8 that the expectations hypothesis works well on aver age for U.S. bonds and is the tip of an iceberg of empirical successes for the expectations hypothesis as applied to currencies. As in the case of bonds, however, we can also ask whether times of temporarily higher or lower interest rate differentials correspond to times of above and below average depreciation as they should. The third and fifth rows of Table 20.11 address this question, updating Hansen and Hodrick s 1980 and Fama s 1984 regression tests. The number here should be 1.0 in each case an extra percentage point interest differential should correspond to one extra percentage point expected depreciation. As you can see, we have exactly the opposite pattern: a higher than usual interest rate abroad seems to lead, if anything, to further appreciation. It seems that the old fallacy of confusing interest rate differentials across countries with expected returns, forgetting about depreciation, also contains a grain of truth. This is the forward discount puzzle, and takes its place alongside the forecastability of stock and bond returns. Of course it has produced a",
        "20.2. The Cross Section: CAPM and Multifactor Models 435 similar avalanche of academic work dissecting whether it is really there and if so, why. Hodrick 1987 , Engel 1996 , and Lewis 1995 provide surveys. The R 2 shown in Table 20.11 are quite low. However, like D P, the inter est differential is a slow moving forecasting variable, so the return forecast R2 build with horizon. Bekaert and Hodrick 1992 report that the R2 rise to the 30 40 range at six month horizons and then decline again. Still, taking advantage of this predictability, like the bond strategies described above, is quite risky. The puzzle does not say that one earns more by holding bonds from countries with higher interest rates than others. Average inflation, depreci ation, and interest rate differentials line up as they should. If you just buy bonds with high interest rates, you end up with debt from Turkey and Brazil, whose currencies inflate and depreciate steadily. The puzzle does say that one earns more by holding bonds from countries whose interest rates are higher than usual relative to U.S. interest rates. However, the fact that the usual rate of depreciation and usual interest differential varies through time, if they are well defined concepts at all, may diminish if not eliminate the out of sample performance of trading rules based on these regressions. The foreign exchange regressions offer a particularly clear cut case in which Peso problems can skew forecasting regressions. Lewis 1995 cred its Milton Friedman for coining the term to explain why Mexican interest rates were persistently higher than U.S. interest rates in the early 1970s even though the currency had been pegged for more than a decade. A small prob ability of a huge devaluation each period can correspond to a substantial interest differential. You will see long stretches of data in which the expec tations hypothesis seems not to be satisfied, because the collapse does not occur in sample. The Peso subsequently collapsed, giving substantial weight to this view. Since then, Peso problems have become a generic term for the effects of small probabilities of large events on empirical work. Rietz 1988 offered a Peso problem explanation for the equity premium that investors are afraid of another great depression which has not happened in sample. Selling out of the money put options and earthquake insurance in Los Angeles are similar strategies whose average returns in a sample will be severely affected by rare events that may not be seen in surprisingly long samples. 20.2 The Cross Section: CAPM and Multifactor Models Having studied how average returns change over time, now we study how average returns change across different stocks or portfolios.",
        "446 20. Expected Returns in the Time Series and Cross Section Table 20.12. Average monthly returns from reversal and momentum strategies Strategy Reversal Momentum Reversal Momentum Period Months 6307 9312 60 13 0.74 Portfolio Formation Average Return, 10 1 Monthly 6307 9312 12 2 1.31 3101 6302 60 13 1.61 3101 6302 12 2 0.38 Each month, allocate all NYSE firms on CRSP to 10 portfolios based on their performance during the portfolio formation months interval. For example, 60 13 forms portfolios based on returns from 5 years ago to 1 year, 1 month ago. Then buy the best performing decile portfolio and short the worst performing decile portfolio. Source: Fama and French 1996, Table VI . hence dwindle down to a low price, market value, or market book ratio subsequently do well. Table 20.12, taken from Fama and French 1996 , reveals that this is in fact the case. As usual, this table is the tip of an iceberg of research on these effects, starting with DeBont and Thaler 1985 and Jegadeesh and Titman 1993 . Reversal Here is the reversal strategy. Each month, allocate all stocks to 10 port folios based on performance in year 5 to year 1. Then, buy the best performing portfolio and short the worst performing portfolio. The first row of Table 20.12 shows that this strategy earns a hefty 0.74 monthly return.2 Past long term losers come back and past winners do badly. This is a cross sectional counterpart to the mean reversion that we stud ied in Section 1.4. Fama and French 1988a already found substantial mean reversion negative long horizon return autocorrelations in disag gregated stock portfolios, so one would expect this phenomenon. Spreads in average returns should correspond to spreads in betas. Fama and French verify that these portfolio returns are explained by their three factor model. Past losers have a high HML beta; they move together with value stocks, and so inherit the value stock premium. 2 Fama and French do not provide direct measures of standard deviations for these portfo lios. One can infer, however, from the betas, R 2 values, and standard deviation of market and factor portfolios that the standard deviations are roughly 1 2 times that of the market return, so that Sharpe ratios of these strategies are comparable to that of the market return.",
        "448 20. Expected Returns in the Time Series and Cross Section above the mean. Since the strategy buys the winners and shorts the losers, an R2 of 0.0025 implies that one should earn a 2 monthly return by the momentum strategy more even than the 1.3 shown in Table 20.12. Lewellen 2000 offers a related explanation for momentum coming from small cross correlations of returns. We have known at least since Fama 1965 that monthly and higher frequency stock returns have slight, statistically significant predictability with R2 in the 0.01 range. However, such small though statistically signif icant high frequency predictability, especially in small stock returns, has also since the 1960s always failed to yield exploitable profits after one accounts for transactions costs, thin trading, high short sale costs, and other microstructure issues. Hence, one naturally worries whether momentum is really exploitable after transactions costs. Momentum does require frequent trading. The portfolios in Table 20.12 are reformed every month. Annual winners and losers will not change that often, but the winning and losing portfolios must still be turned over at least once per year. Carhart 1997 calculates transactions costs and con cludes that momentum is not exploitable after those costs are taken into account. Moskowitz and Grinblatt 1999 note that most of the appar ent gains come from short positions in small, illiquid stocks, positions that also have high transactions costs. They also find that a large part of momentum profits come from short positions taken November, antic ipating tax loss selling in December. This sounds a lot more like a small microstructure glitch rather than a central parable for risk and return in asset markets. Table 20.12 already shows that the momentum effect essentially dis appears in the earlier data sample, while reversal is even stronger in that sample. Ahn, Boudoukh, Richardson, and Whitelaw 2002 show that appar ent momentum in international index returns is missing from the futures markets, also suggesting a microstructure explanation. Of course, it is possible that a small positive autocorrelation is there and related to some risk. However, it is hard to generate real positive autocorre lation in realized returns. As we saw in Section 20.2, a slow and persistent variation in expected returns most naturally generates negative autocorre lation in realized returns. News that expected returns are higher means future dividends are discounted at a higher rate, so today s price and return declines. The only way to overturn this prediction is to suppose that expected return shocks are positively correlated with shocks to current or expected future dividend growth. A convincing story for such correlation has not yet been constructed. On the other hand, the required positive correlation is very small and not very persistent.",
        "20.3. Summary and Interpretation 449 20.3 Summary and Interpretation While the list of new facts appears long, similar patterns show up in every case. Prices reveal slow moving market expectations of subsequent excess returns, because potential offsetting events seem sluggish or absent. The patterns suggest that there are substantial expected return premia for taking on risks of recession and financial stress unrelated to the market return. Magnifying Glasses The effects are not completely new. We knew since the 1960s that high frequency returns are slightly predictable, with R2 of 0.01 to 0.1 in daily to monthly returns. These effects were dismissed because there did not seem to be much that one could do about them. A 51 49 bet is not very attractive, especially if there is any transactions cost. Also, the increased Sharpe ratio one can obtain by exploiting predictability is directly related to the forecast R2, so tiny R2, even if exploitable, did not seem like an important phenomenon. Many of the new facts amount to clever magnifying glasses, ways of making small facts economically interesting. For forecasting market returns, we now realize that R2 rise with horizon when the forecasting variables are slow moving. Hence small R 2 at high frequency can mean really substantial R2, in the 30 50 range, at longer horizons. Equivalently, we realize that small expected return variation can add up to striking price variation if the expected return variation is persistent. For momentum and reversal effects, the ability to sort stocks and funds into momentum based portfolios means that small predictability times portfolios with huge past returns gives important subsequent returns. Dogs that Did Not Bark In each case, an apparent difference in yield should give rise to an offsetting movement, but seems not to do so. Something should be predictable so that returns are not predictable, and it is not. The d p forecasts of the market return were driven by the fact that dividends should be predictable, so that returns are not. Instead, divi dend growth seems nearly unpredictable. As we saw, this fact and the speed of the d p mean reversion imply the observed magnitude of return predictability. The term structure forecasts of bond returns were driven by the fact that bond yields should be predictable, so that returns are not. Instead, yields seem nearly unpredictable at the one year horizon. This fact means that the forward rate moves one for one with expected returns, and that a one",
        "450 20. Expected Returns in the Time Series and Cross Section percentage point increase in yield spread signals as much as a 5 percentage point increase in expected return. Exchange rates should be forecastable so that foreign exchange returns are not. Instead, a one percentage point increase in interest rate abroad seems to signal a greater than one percentage point increase in expected return. Prices Reveal Expected Returns If expected returns rise, prices are driven down, since future dividends or other cash flows are discounted at a higher rate. A low price, then, can reveal a market expectation of a high expected or required return. Most of our results come from this effect. Low price dividend, price earnings, price book values signal times when the market as a whole will have high average returns. Low market value price times shares relative to book value signals securities or portfolios that earn high average returns. The small firm effect derives from low prices other measures of size such as number of employees or book value alone have no predictive power for returns Berk 1997 . The 5 year reversal effect derives from the fact that five years of poor returns lead to a low price. A high long term bond yield means that the price of long term bonds is low, and this seems to signal a time of good long term bonds returns. A high foreign interest rate means a low price on foreign bonds, and this seems to indicate good returns on the foreign bonds. The most natural interpretation of all these effects is that the expected or required return the risk premium on individual securities as well as the market as a whole varies slowly over time. Thus we can track market expectations of returns by watching price dividend, price earnings, or book market ratios. Macroeconomic Risks The price based patterns in time series and cross sectional expected returns suggest a premium for holding risks related to recession and economy wide financial distress. All of the forecasting variables are connected to macro economic activity Fama and French 1989 . The dividend price ratio is highly correlated with the default spread and rises in bad times. The term spread forecasts bond and stock returns, and is also one of the best recession forecasters. It rises steeply at the bottoms of recessions, and is inverted at the top of a boom. Thus, return forecasts are high at the bottom of busi ness cycles and low at the top of booms. Value and small cap stocks are typically distressed. Formal quantitative and empirically successful eco nomic models of the recession and distress premia are still in their infancy I think Campbell and Cochrane 1999 is a good start , but the story is",
        "452 20. Expected Returns in the Time Series and Cross Section Doubts Momentum is, so far, unlike all the other results. The underlying phe nomenon is a small predictability of high frequency returns. However, the price based phenomena make this predictability important by noting that, with a slow moving forecasting variable, the R 2 build over horizon. Momen tum is based on a fast moving forecast variable the last year s return. Therefore the R 2 decline with horizon. Instead, momentum makes the tiny autocorrelation of high frequency returns significant by forming portfolios of extreme winners and losers, so a small continuation of huge past returns gives a large current return. All the other results are easily digestible as a slow, business cycle related time varying expected return. This specification gives negative autocorrelation unless we add a distasteful positive corre lation of expected return and dividend shocks and so does not explain momentum. Momentum returns have also not yet been linked to business cycles or financial distress in even the informal way that I suggested for the price based strategies. Thus, it still lacks much of a plausible economic interpretation. To me, this adds weight to the view that it is not there, it is not exploitable, or it represents a small illiquidity tax loss selling of small illiquid stocks that will be quickly remedied once a few traders understand it. In the entire history of finance there has always been an anomaly du jour, and momentum is it right now. We will have to wait to see how it is resolved. Many of the anomalous risk premia seem to be declining over time. The small firm effect completely disappeared in 1980; you can date this as the publication of the first small firm effect papers or the founding of small firm mutual funds that made diversified portfolios of small stocks available to average investors. To emphasize this point, Figure 20.14 plots size portfolio average returns versus beta in the period since 1979. You can see that not only has the small firm premium disappeared, the size related variation in beta and expected return has disappeared. The value premium has been cut roughly in half in the 1990s, and 1990 is roughly the date of widespread popularization of the value effect, though  T leaves a lot of room for error here. As you saw in Table 20.4, the last five years of high market returns have cut the estimated return predictability from the dividend price ratio in half. These facts suggest an uncomfortable implication: that at least some of the premium the new strategies yielded in the past was due to the fact that they were simply overlooked, they were artifacts of data dredging, or they survived only until funds were created that allow many investors to hold diversified portfolios that exploit them. Since they are hard to measure, one is tempted to put less emphasis on these average returns. However, they are crucial to our interpretation of the facts. The CAPM is perfectly consistent with the fact that there are additional sources of common variation. For example, it was long understood that"
    ],
    "Topic 4": [
        "392 20. Expected Returns in the Time Series and Cross Section Table 20.1. OLS regressions of percent excess returns value weighted NYSE treasury bill rate and real dividend growth on the percent VW dividend price ratio Rt t k a b Dt Pt Dt k Dt a b Dt Pt years b  b R2 b  b R2 Horizon k 1 5.3 2.0 2 10 3.1 3 15 4.0 5 33 5.8 0.15 2.0 1.1 0.06 0.23 2.5 2.1 0.06 0.37 2.4 2.1 0.06 0.60 4.7 2.4 0.12 R t t k indicates the k year return. Standard errors in parentheses use GMM to correct for heteroskedasticity and serial correlation. Sample 1947 1996. The left hand regression in Table 20.1 gives a simple example of mar ket return predictability, updating Fama and French 1988b . Low prices relative to dividends forecast higher subsequent returns. The one year hori zon 0.15 R2 is not particularly remarkable. However, at longer and longer horizons larger and larger fractions of return variation are forecastable. At a five year horizon 60 of the variation in stock returns is forecastable ahead of time from the price dividend ratio. One can object to dividends as the divisor for prices. However, ratios formed with just about any sensible divisor work about as well, including earnings, book value, and moving averages of past prices. Many other variables forecast excess returns, including the term spread between long and short term bonds, the default spread, the T bill rate Fama and French 1989 , and the earnings dividend ratio Lamont 1998 . Macro variables forecast stock returns as well, including the invest ment capital ratio Cochrane 1991d and the consumption wealth ratio Lettau and Ludvigson 2001b . Most of these variables are correlated with each other and correlated with or forecast business cycles. This fact suggests a natural explanation, emphasized by Fama and French 1989 : Expected returns vary over busi ness cycles; it takes a higher risk premium to get people to hold stocks at the bottom of a recession. When expected returns go up, prices go down. We see the low prices, followed by the higher returns expected and required by the market. Regressions do not have to have causes on the right and effects on the left. You run regressions with the variable orthogonal to the error on the right, and that is the case here since the error is a forecasting error. This is like a regression of actual weather on a weather forecast. Table 20.2, adapted from Lettau and Ludvigson 2001b , compares sev eral of these variables. At a one year horizon, both the consumption wealth",
        "20.1. Time Series Predictability Table 20.2. Horizon years cay 1 6.7 1 1 1 5.4 6 12.4 6 6 6 5.9 393 Long horizon return forecasts d p 0.14 0.07 0.95 0.89 d e 0.08 0.05 0.68 0.65 rrel R2 0.18 0.04 4.5 0.10 3.8 0.23 0.16 0.39 5.10 0.03 1.36 0.42 The return variable is log excess returns on the S P composite index. cay is Lettau and Ludvigson s consumption to wealth ratio. d p is the log dividend yield and d e is the log earnings yield. rrel is a detrended short term interest rate. Sample 1952:4 1998:3. Source: Lettau and Ludvigson 2001b, Table 6 . ratio and the detrended T bill rate forecast returns, with R 2 of 0.18 and 0.10, respectively. At the one year horizon, these variables are more important than the dividend price and dividend earnings ratios, and their presence cuts the dividend ratio coefficients in half. However, the d p and d e ratios are slower moving than the T bill rate and consumption wealth ratio. They track decade to decade movements as well as business cycle movements. This means that their importance builds with horizon. By six years, the bulk of the return forecastability again comes from the dividend ratios, and it is their turn to cut down the cay and T bill regression coefficients. The cay and d e variables have not been that affected by the late 1990s, while this time period has substantially cut down our estimate of dividend yield forecastability. I emphasize that excess returns are forecastable. We have to understand this as time variation in the reward for risk, not time varying interest rates. One naturally slips in to nonrisk explanations for price variation; for exam ple that the current stock market boom is due to life cycle savings of the baby boomers. A factor like this does not reference risks; it predicts that interest rates should move just as much as stock returns. Persistent d p; Long Horizons Are Not A Separate Phenomenon The results at different horizons are not separate facts, but reflections of a single underlying phenomenon. If daily returns are very slightly pre dictable by a slow moving variable, that predictability adds up over long horizons. For example, you can predict that the temperature in Chicago will rise about 1 3 degree per day in the springtime. This forecast explains very little of the day to day variation in temperature, but tracks almost all",
        "394 20. Expected Returns in the Time Series and Cross Section Figure 20.1. Dividend price ratio of value weighted NYSE. of the rise in temperature from January to July. Thus, the R2 rises with horizon. Thus, a central fact driving the predictability of returns is that the dividend price ratio is very persistent. Figure 20.1 plots the d p ratio and you can see directly that it is extremely slow moving. Below, I estimate an AR 1 coefficient around 0.9 in annual data. To see more precisely how the results at various horizons are linked, and how they result from the persistence of the d p ratio, suppose that we forecast returns with a forecasting variable x, according to rt 1 bxt t 1, 20.1 xt 1 xt t 1. 20.2 Obviously, you de mean the variables or put constants in the regressions. Small values of b and R2 in 20.1 and a large coefficient  in 20.2 imply mathematically that the long horizon regression has a large regression coefficient and large R2. To see this, write rt 1 rt 2 b 1  xt bt 1 t 1 t 2, rt 1 rt 2 rt 3 b 1  2 xt bt 1 bt 2 t 1 t 2 t 3. You can see that with  near one, the coefficients increase with horizon, almost linearly at first and then at a declining rate. The R 2 are a little messier to work out, but also rise with horizon.",
        "396 20. Expected Returns in the Time Series and Cross Section Volatility Price dividend ratios can only move at all if they forecast future returns, if they forecast future dividend growth, or if there is a bubble if the price dividend ratio is nonstationary and is expected to grow explosively. In the data, most variation in price dividend ratios results from varying expected returns. Excess volatility relative to constant discount rate present value models is thus exactly the same phenomenon as forecastable long horizon returns. I also derive the very useful price dividend and return linearizations. Ignoring constants means , j 1 j 1 dt j rt j , pt dt Et r E r E E j d jr , tt 1ttt 1 t j t j j 0 j 1 rt 1 dt 1  dt 1 pt 1 dt pt . The volatility test literature starting with Shiller 1981 and LeRoy and Porter 1981 see Cochrane 1991c for a review started out trying to make a completely different point. Predictability seems like a sideshow. The stunning fact about the stock market is its extraordinary volatility. On a typical day, the value of the U.S. capital stock changes by a full percentage point, and days of 2 or 3 percentage point changes are not uncommon. In a typical year it changes by 16 percentage points, and 30 percentage point changes are not uncommon. Worse, most of that volatility seems not to be accompanied by any important news about future returns and discount rates. Thirty percent of the capital stock of the United States vanished in a year and nobody noticed? Surely, this observation shows directly that markets are not efficient that prices do not correspond to the value of capital without worrying about predictability? It turns out, however, that excess volatility is exactly the same thing as return predictability. Any story you tell about prices that are too high or too low necessarily implies that subsequent returns will be too low or too high as prices rebound to their correct levels. When prices are high relative to dividends or earnings, cashflow, book value, or some other divisor , one of three things must be true: 1 Investors expect dividends to rise in the future. 2 Investors expect returns to be low in the future. Future cashflows are discounted at a lower than usual rate, leading to higher prices. 3 Investors expect prices to rise forever,",
        "20.1. Time Series Predictability 397 giving an adequate return even if there is no growth in dividends. This statement is not a theory, it is an identity: If the price dividend ratio is high, either dividends must rise, prices must decline, or the price dividend ratio must grow explosively. The open question is, which option holds for our stock market? Are prices high now because investors expect future earnings, dividends, etc. to rise, because they expect low returns in the future, or because they expect prices to go on rising forever? Historically, we find that virtually all variation in price dividend ratios has reflected varying expected excess returns. Exact Present Value Identity To document this statement, we need to relate current prices to future dividends and returns. Start with the identity and hence 1 R 1 R R 1 Pt 1 Dt 1 t 1 t 1 t 1 Pt Pt R 1 1 Pt 1 Dt 1. Dt t 1 Dt 1 Dt 20.5 We can iterate this identity forward and take conditional expectations to obtain the identity P j t E Dt where Dt Dt Dt 1. Wecoulditerate 20.5 forwardto but prices are not stationary, so we cannot find the variance of prices from a time series average. Much of the early volatility test controversy centered on stationarity problems. Equation 20.6 also requires a limiting con dition that the price dividend ratio cannot explode faster than returns, D . I come back to this condition below. t j lim E j j R 1 P t k 1 t k t j 1 R 1 D , t k t k 20.6 k 1 j P R 1D, t t k t j j 1 k 1 t j Equation 20.6 shows that high prices must, mechanically, come from high future dividend growth or low future returns. Approximate Identity The nonlinearity of 20.6 makes it hard to handle, and means that we cannot use simple time series tools. You can linearize 20.6 directly with a",
        "398 20. Expected Returns in the Time Series and Cross Section Taylor expansion. Cochrane 1991a takes this approach. Campbell and Shiller 1988a approximate the one period return identity before iterating, which is algebraically simpler. Start again from the obvious, 1 R 1R R 1 Pt 1 Dt 1. t 1 t 1 t 1 Pt MultiplyingbothsidesbyPt Dt andmassagingtheresult, Pt R 1 1 Pt 1 Dt 1. Dt t 1 Dt 1 Dt Taking logs, and with lowercase letters denoting logs of uppercase letters, pt dt rt 1 dt 1 ln 1 ept 1 dt 1 . Taking a Taylor expansion of the last term about a point P D e p d , where pt dt rt 1 dt 1 ln 1 P D P D pt 1 dt 1 p d 1 P D rt 1 dt 1 k  pt 1 dt 1 k ln 1 P  p d . D 20.7 Since the average dividend yield is about 4 and average price dividend ratio is about 25,  is a number very near one. I will use  0.96 for calculations,  P D 1 1 D P 0.96. 1 P D 1 D P Without the constant k , the equation can also apply to deviations from means or any other point. Now, iterating forward is easy, and results in the approximate identity j 1 dt j rt j . 20.8 limj j pt j dt j 0.Ireturntothisconditionbelow. pt dt const. j 1 Again,weneedaconditionthatpt dt doesnotexplodefasterthan t,",
        "20.1. Time Series Predictability 399 Since 20.8 holds ex post, we can take conditional expectations and relate price dividend ratios to ex ante dividend growth and return forecasts, j 1 dt j rt j . 20.9 by high dividend growth d , or low returns r . Which is it? pt dt const. Et Now it is really easy to see that a high price dividend ratio must be followed Decomposing The Variance of Price Dividend Ratios To address this issue, equation 20.8 implies j 1 j 1 d tt tt t j var p d cov p d , cov p d , tt t j j 1r . 20.10 In words, price dividend ratios can only vary if they forecast changing divi dend growth or if they forecast changing returns. To derive 20.10 from 20.8 ,multiplybothsidesby pt dt E pt dt andtakeexpectations. Notice that both terms on the right hand side of 20.10 are the numerators of exponentially weighted long run regression coefficients. This is a powerful equation. At first glance, it would seem a reasonable approximation that returns are unforecastable the random walk hypoth esis and that dividend growth is not forecastable either. But if this were the case, the price dividend ratio would have to be a constant. Thus the fact that the price dividend ratio varies at all means that either dividend growth or returns must be forecastable that the world is not i.i.d. At a simple level, Table 20.1 includes regressions of long horizon divi dend growth on dividend price ratios to match the return regressions. The coefficients in the dividend growth case are much smaller, typically one standard error from zero, and the R2 are tiny. Worse, the signs are wrong in Table 20.1. To the extent that a high price dividend ratio forecasts any change in dividends, it seems to forecast a small decline in dividends! Having seen equation 20.10 , one is hungry for estimates. Table 20.3 presents some, taken from Cochrane 1991a . As one might suspect from Table 20.1, Table 20.3 shows that in the past almost all variation in price dividend ratios is due to changing return forecasts. The elements of the decomposition in 20.10 do not have to be between 0 and 100 . For example, 34, 138 occurs because high prices seem to forecast lower real dividend growth though this number is not statistically j 1 j 1",
        "400 20. Expected Returns in the Time Series and Cross Section Table 20.3. Real Std. error Nominal Std. error Variance decomposition of value weighted NYSE price dividend ratio Dividends Returns 34 138 10 32 30 85 41 19 Table entries are the percent of the variance of the price dividend ratio attributable to dividend and return forecasts,100 cov pt dt, 15 j 1 d var pt dt j 1 t j and similarly for returns. significant . Therefore they must and do forecast really low returns, and returns must account for more than 100 of price dividend variation. This observation solidifies one s belief in price dividend ratio forecasts of returns. Yes, the statistical evidence that price dividend ratios forecast returns is weak, and many return forecasting variables have been tried and discarded, so selection bias is a big worry in forecasting regressions. But the price dividend ratio or price earning, market book, etc. has a special status since it must forecast something. To believe that the price dividend ratio is stationary and varies, but does not forecast returns, you have to believe that the price dividend ratio does forecast dividends. Given this choice and Table 20.1, it seems a much firmer conclusion that it forecasts returns. It is nonetheless an uncomfortable fact that almost all variation in price dividend ratios is due to variation in expected excess returns. How nice it would be if high prices reflected expectations of higher future cash flows. Alas, that seems not to be the case. If not, it would be nice if high prices reflected lower interest rates. Again, that seems not to be the case. High prices reflect low risk premia, lower expected excess returns. Campbell s Return Decomposition Campbell 1991 provides a similar decomposition for unexpected returns, r E r E E j d jr . 20.11 tt 1ttt 1 t j t j j 0 j 1 A positive shock to returns must come from a positive shock to forecast dividend growth, or from a negative shock to forecast returns. Since a positive shock to time t dividends is directly paid as a return the first sum starts at j 0 , Campbell finds some fraction of return variation",
        "20.1. Time Series Predictability 401 is due to current dividends. However, once again, the bulk of index return variation comes from shocks to future returns, i.e., discount rates. To derive 20.11 , start with the approximate identity 20.8 , and move it back one period, j 0 j 0 Pulling rt over to the left hand side, you obtain 20.11 . Problem 3 at the end of the chapter guides you through an alternative and more constructive derivation. Cross Section So far, we have concentrated on the index. One can apply the same anal ysis to firms. What causes the variation in price dividend ratios, or, better book market ratios since dividends can be zero across firms, or over time for a given firm? Vuolteenaho 1999 applies the same sort of analysis to indi vidual stock data. He finds that as much as half of the variation in individual firm book market ratios reflects expectations of future cashflows. Much of the expected cashflow variation is idiosyncratic, while the expected return variation is common, which is why variation in the index book market ratio, like variation in the index dividend price ratio, is almost all due to varying expected excess returns. Bubbles In deriving the exact and linearized present value identities, I assumed an extra condition that the price dividend ratio does not explode. Without that condition, and taking expectations of both sides, the exact identity reads pt 1 dt 1 const. Now take innovations of both sides, j dt j rt j . P j t E R 1 D lim E t k t k j t j Pt j R 1 D , 20.12 D t t t k t k D t j j 1 k 1 and the linearized identity reads pt dt const. Et k 1 0 Et Et 1 j dt j rt j . j 1 j 1 dt j rt j Et limj pt j dt j . j 20.13",
        "402 20. Expected Returns in the Time Series and Cross Section As you can see, the limits in the right hand sides of 20.12 and 20.13 are zero if the price dividend ratio is stationary, or even bounded. For these terms not to be zero, the price dividend ratio must be expected to grow explosively, and faster than R or  1. Especially in the linearized form 20.13 you can see that stationary r, d implies stationary p d if the last term is zero, and p d is not stationary if the last term is not zero. Thus, you might want to rule out these terms just based on the view that price dividend ratios do not and are not expected to explode in this way. You can also invoke economic theory to rule them out. The last terms must be zero in an equilibrium of infinitely lived agents or altruistically linked generations. If wealth explodes, optimizing long lived agents will consume more. Technically, this limiting condition is a first order condition for opti mality just like the period to period first order condition. The presence of the last term also presents an arbitrage opportunity in complete markets, as you can short a security whose price contains the last term, buy the dividends separately, and eat the difference right away. On the other hand, there are economic theories that permit the limiting terms overlapping generations models, and they capture the interest ing possibility of rational bubbles that many observers think they see in markets, and that have sparked a huge literature and a lot of controversy. An investor holds a security with a rational bubble not for any divi dends, but on the expectation that someone else will pay even more for that security in the future. This does seem to capture the psychology of some investors from the alleged, see Garber 2000 tulip bubble of 17th century Holland to the dot com bubble of the millennial United States why else would anyone buy Cisco Systems at a price earnings ratio of 217 and market capitalization 10 times that of General Motors in early 2000? A rational bubble imposes a little discipline on this centuries old description, however, by insisting that the person who is expected to buy the security in the future also makes the same calculation. He must expect the price to rise even further. Continuing recursively, the price in a rational bubble must be expected to rise forever. A Ponzi scheme, in which everyone knows the game will end at some time, cannot rationally get off the ground. The expectation that prices will grow at more than a required rate of return forever does not mean that sample paths do so. For example, consider the bubble process Pt 1 RPt, 1, prob PtR 1 , PtR 1 prob PtR  1 . PtR 1 Figure 20.2 plots a realization of this process with  1.2. This process yields an expected return R, and the dashed line graphs this expectation as of the first date. Its price is positive though it never pays dividends. It",
        "20.1. Time Series Predictability 403 Figure 20.2. Sample path from a simple bubble process. The solid line gives a price realization. The dashed line gives the expected value of prices as of time zero, i.e., p0 R t . repeatedly grows with a high return  R for a while and then bursts back to one. The expected price always grows, though almost all sample paths do not do so. Infinity is a long time. It is really hard to believe that prices will rise forever. The solar system will end at some point; any look at the geological and evolutionary history of the earth suggests that our species will die out a lot sooner than that. Thus, the infinity in the bubble must really be a parable for a really long time. But then the rational part of the bubble pops it must hinge on the expectation that someone will be around to hold the bag; to buy a security without the expectation of dividends or further price increases. The forever part of usual present value formulas is not similarly worrying because 99.99 of the value comes from the first few hundred years of dividends. Empirically, bubbles do not appear to be the reason for historical price dividend ratio variation. First, price dividend ratios do seem stationary. Craine 1993 runs a unit root test with this conclusion. Even if statistical tests are not decisive, as is expected for a slow moving series or a series such as that plotted in Figure 20.2, it is hard to believe that price dividend ratios can explode rather than revert back to their four century average level of about 20 to 25. Second, Table 20.3 shows that return and dividend forecastability terms add up to 100 of the variance of price dividend ratios. In a bubble, we would expect price variation not",
        "404 20. Expected Returns in the Time Series and Cross Section matched by any variation in expected returns or dividends, as is the case in Figure 20.2. I close with a warning: The word bubble is widely used to mean very different things. Some people seem to mean any large movement in prices. Others mean large movements in prices that do correspond to low or perhaps negative expected excess returns I think this is what Shiller 2000 has in mind , rather than a violation of the terminal condition, but these expected returns are somehow disconnected from the rest of the economy. A Simple Model for Digesting Predictability To unite the various predictability and return observations, I construct a simple VAR representation for returns, price growth, dividend growth, dividend price ratio. I start only with a slow moving expected return and unforecastable dividends. This specification implies that d p ratios reveal expected returns. This specification implies return forecastability. To believe in a lower predictability of returns, you must either believe that dividend growth really is predictable, or that the d p ratio is really much more persistent than it appears to be. This specification shows that small but persistent changes in expected returns add up to large price changes. We have isolated two important features of the long horizon forecast phenomenon: dividend price ratios are highly persistent, and dividend growth is essentially unforecastable. Starting with these two facts, a simple VAR representation can tie together many of the predictability and volatility phenomena. Start by specifying a slow moving state variable xt that drives expected returns, and unforecastable dividend growth, xt bxt 1 t, rt 1 xt r t 1, dt 1 d t 1. 20.14 20.15 20.16 All variables are de meaned logs. The term structure models of Chapter 19 were of this form. From this specification, using the linearized present value identity and return, we can derive a VAR representation for prices, returns, dividends,",
        "20.1. Time Series Predictability 405 and the dividend price ratio ignoring constants ,  dt 1 pt 1 b dt pt t 1 , 20.17 1 b  rt 1 1 b dt pt dt 1 pt 1 1 b dt pt dt 1 1 t 1 , 20.19 t 1 , 20.18 dt pt Et j 1 Etrt j Et dt j xt 1 b 1 b dt 1 d t 1. I derive each of these equations, I look at data to come up with parameter values, and then I use this system to digest predictability. Dividend priceratio: Usingtheapproximatepresent valueidentity 20.9 ,we can find the dividend price ratio . 20.21 Equation 20.17 follows. Equation 20.21 makes precise my comment that the dividend price ratio reveals expected returns xt . Obviously, the feature that the dividend price ratio is exactly proportional to the expected return xt does not generalize. If dividend growth is also forecastable, then the dividend price ratio is a combination of dividend growth and return fore casts. Actual return forecasting exercises can often benefit from cleaning up the dividend price ratio to focus on the implied return forecast. Returns: Since we know where the dividend price ratio and dividends are going, we can figure out where returns are going. Use the return linearization this is equivalent to 20.7 20.20 j 1 1 b Rt 1 1 Pt 1 Dt 1 Pt , Dt 1 Dt Dt rt 1  pt 1 dt 1 dt 1 dt pt dt . Now, plug in from 20.17 and 20.16 to get 20.18 . Prices: Write pt 1 pt dt 1 pt 1 dt pt dt 1 dt . Then, plugging in from 20.17 and 20.16 , we get 20.19 . 20.22 20.23",
        "406 20. Expected Returns in the Time Series and Cross Section Parameters We can back out parameters from the reduced form return d p VAR. Any two equations carry all the information of this system. Table 20.4 presents some estimates. I report both the more intuitive coefficients of returns on the actual d p ratio, denoted a, D P and the coefficients on the log d p ratio, denoted a, which is a more useful specification for our transformations. The two line up; a coefficient of 5 on Dt Pt implies a coefficient of 5 D P 0.25 on Dt Pt D P . You can see that the parameters depend somewhat on the sample. In particular, the dramatic returns of the late 1990s, despite low dividend yields, cut the postwar return forecast coefficients in half and the overall sample estimate by about one third. That dramatic decline in the d p ratio also induces a very high apparent persistence in the d p ratio, rising to a 0.97 estimate in the 48 98 sample. Faced with an apparent trend in the data, an autoregression estimates a root near unity. With these estimates in mind, given the considerations outlined below, I make calculations using parameters Table 20.4. Estimates of log excess return and log dividend price ratio regressions, using annual CRSP data b   r  dp  r , dp 0.9, 0.96, 15, 12.5, 0.7. 20.24 Sample a 27 98 0.16 48 98 0.14 27 92 0.28 48 92 0.27 a , D P b  r 4.7 0.92 19.2 4.0 0.97 15.0 6.7 0.82 19.0 6.2 0.87 14.5  dp  r ,dp 15.2 0.72 12.6 0.71 15.0 0.69 12.4 0.67 r is the difference between the log value weighted return and the log treasury bill rate. The estimates are of the system and rt 1 a dt pt rt 1, d t 1 pt 1 b d t pt dp t 1 , r a,D P Dt  . t 1 Pt t 1",
        "20.1. Time Series Predictability 407 From these parameters, we can find the underlying parameters of 20.14 20.16 . I comment on each one below as it becomes useful. From 20.17 From 20.18 ,    dp 1 b 1.7.      2 r 22 dp 2 r,dp 10.82,  d , dp  r , dp   2 dp , 20.25 20.26 d r dp and hence  d,dp r dp r dp 0.139.         d Now we are ready to use 20.17 20.20 in order to integrate predictability issues. The Size of the Return Forecasting Coefficient. Does the magnitude of the estimated return predictability make sense? Given the statistical uncertainties, do other facts guide us to higher or lower predictability? The coefficient of the one year excess return on the dividend price ratio in Table 20.1 is about 5, and the estimates in Table 20.4 vary from 4 to 6 depending on the sample. These values are surprisingly large. For example, a naive investor might think that dividend yields move one for one with returns; if they pay more dividends, you get more money. This logic implies a coefficient of 1. Before predictability, we would have explained that high dividend yield means that prices are low now in anticipation of lower future dividends, leaving the expected return unchanged. This logic implies a coefficient of 0. Now we recognize the possibility of time varying expected returns, but does it make sense that expected returns move even more than dividend yields? Return forecastability follows from the fact that dividends are not forecastable, and that the dividend price ratio is highly but not completely persistent. We see this in the calculated coefficients of prices and returns on the dividend price ratio in 20.18 and 20.19 . We derived rt 1 1 b dt pt rt 1, pt 1 1 b dt pt pt 1. To transform units to regressions on D P, multiply by 25, e.g., rt 1 1 bDt rt 1. D P Pt",
        "408 20. Expected Returns in the Time Series and Cross Section Suppose the d p ratio were not persistent at all, b 0. Then both return and price growth coefficients should be 1 in logs or about 25 in levels! If the D P ratio is one percentage point above its average, we must forecast enough of a rise in prices to restore the D P ratio to its average in one year. The average D P ratio is about 4 , though, so prices and hence returns must rise by 25 to change the D P ratio by one percentage point. d D P D P d P P. Suppose instead that the d p ratio were completely persistent, i.e., a random walk with b 1. Then the return coefficient is 1  0.04, and about 1.0 in levels, while the price coefficient is 0. If the d p ratio is one percent above average and expected to stay there, and dividends are not forecastable, then prices must not be forecast to change either. The return is one percentage point higher, because you get the higher dividends as well. Thus, the naive investor who expects dividend yield to move one for one with expected returns not only implicitly assumes that dividends are not forecastable which turns out to be true but also that the d p ratio will stay put forever. A persistence parameter b 0.90 implies price and return regression coefficients of 1 b 0.10, 1 b 1 0.96 0.90 0.14 20.27 or about 2.5 and 3.4 on D P. If the dividend yield is one percentage point high, and is expected to be 0.9 percentage points high one year from now, then prices must be expected to increase by P D 0.1 2.5 percentage points in the next year. The return gets the additional dividend as well as the expected price change. This, fundamentally, is how unforecastable dividend growth together with persistent D P imply that expected returns move more than one for one with the dividend yield. Now, we can turn to the central question: how much return forecasta bility should we believe? The calculations of equation 20.27 are a little below most of the estimates in Table 20.1 and Table 20.4, which suggest coefficients on D P of 4 6. In the sample, a high price seems to forecast lower dividend growth. This is the wrong sign, which is hard to believe. To believe in this much return forecastability without such perverse divi dend growth forecastability, we have to lower the persistence coefficient. For example, a persistence coefficient b 0.8 implies a return coefficient 1 b 1 0.96 0.8 0.23 or 0.23 25 5.75 on D P. However, given the slow movement of D P seen in Figure 20.1 and the fact that autore gression estimates are downward biased, it is hard to believe that D P ratios really do revert that much more quickly. It seems more sensible to believe b 0.9 and hence that return predictability is in fact something like 0.14, or",
        "20.1. Time Series Predictability 409 roughly 3.4 on D P. This value is equal to the estimate in the 48 98 sample, though distinctly lower than in some earlier samples. Going in the other direction, statistical uncertainty, the recent runup in stocks despite low dividend yields, and the dramatic portfolio implications of time varying returns for investors whose risks or risk aversion do not change over time all lead one to consider even lower return predictability. As we see from these calculations though, there are only two ways to make sense of lower predictability. You have to believe that high prices really do forecast higher dividend growth, or you have to believe that dividend price ratios are substantially more persistent than b 0.9. Much more persistent d p is a tough road to follow, since D P ratios already move very slowly. They basically change sign once a generation; high in the 50 s, low in the 60 s, high in the mid 70 s, and decreasing ever since see Figure 20.1. Can this be a sample of unusually fast D P movement? As a quantitative example, suppose the D P ratio had an AR 1 coefficient of 0.96 in annual data. This means a half life of ln 0.5 ln 0.96 17 years. In this case, the price coefficient would be 1 b 1 0.96 1 D P 0.04 and the return coefficient would be 1 b 1 0.962 2 D P 0.04 A one percentage point higher d p ratio means that prices must rise 1 percentage point next year, so returns must be about 2 percentage points higher. A two for one movement of expected returns with the dividend yield thus seems about the lower bound for return predictability. The only other option is to believe that dividend growth really is fore castable. New economy advocates believe that this time, prices really are rising on advance news of dividend growth, even though prices have not forecast dividend growth in the past. This would be wonderful if it were true. However, you have to face the fact that every variation of the market D P in the past was not followed by unusual dividend growth. You have to believe that our data were generated from a very unlucky sample. Persistence, Price Volatility, and Expected Returns From the dividend price ratio equation 20.17 we can find the volatility of the dividend price ratio and relate it to the volatility and persistence of expected returns:  dt pt 1  xt . 1 b",
        "410 20. Expected Returns in the Time Series and Cross Section With b 0.9, 1 1 b 1 1 0.96 0.9 7.4. Thus, the persistence of expected returns means that a small expected return variation translates into a large price variation. A one percentage point change in expected returns with persistence b 0.9 corresponds to a 7.4 increase in price. The Gordon growth model is a classic and even simpler way to see this point. With constant dividend growth g and return r, the present value identity becomes P D. r g A price dividend ratio of 25 means r g 0.04. Then, a one percentage point permanent change in expected return translates into a 25 percentage point change in price! This is an overstatement, since expected returns are not this persistent, but it allows you clearly to see the point. This point also shows that small market imperfections in expected returns can translate into substantial market imperfections in prices, if those expected return changes are persistent. We know markets cannot be perfectly efficient Grossman and Stiglitz 1980 . If they were per fectly efficient, there would be no traders around to make them efficient. Especially where short sales or arbitrage are constrained by market fric tions, prices of similar assets can be substantially different, while the expected returns of those assets are almost the same. For example the closed end fund puzzle Thompson 1978 noted that baskets of securities sold for substantial price discounts relative to the sum of the individual securities. Even if we concede this as an anomaly, it is a small difference in expected returns. The price differentials persist for a long time. You cannot short the closed end funds to buy the securities and keep that short position on for years. Mean Reversion Long Run Regressions and Variance Ratios The first important evidence of long run forecastability in the stock market did not come from regressions of returns on d p ratios, but rather from clever ways of looking at the long run univariate properties of returns. Fama I introduce long horizon return regressions and variance ratios. I show that they are related: each one picks up a string of small negative return autocorrelations. I show though that the direct evidence for mean reversion and Sharpe ratios that rise with horizon is weak.",
        "20.1. Time Series Predictability Table 20.7. 1947 1996 logs 1  rk k 15.6 k 0.10 0.29 0.30 0.30 0.17 415 Sharpe k 1947 1996 levels 0.51 0.46 0.41 3 5 7 17.1 17.9 16.8 21.9 29.3 Mean reversion in postwar data Horizon k years 2 3 5 7 10 15.6 0.18 0.36 10 39.8 0.25 0.37 14.9 13.0 13.9 15.0 0.44 0.46 1 2  rk k k 0.13 0.33 0.30 0.25 0.13 Sharpe k 0.50 0.51 0.55 0.48 0.41 Mean Reversion and Forecastability I reconcile large forecastability from d p ratios with a small mean reversion. I calculate the univariate return process implied by the simple VAR, and find that it displays little mean reversion. I show that if dividend shocks are uncorrelated with expected return shocks, there must be some mean reversion. If we rule out the small positive correlation between dividend and expected return shocks in our sample, we get a slightly higher estimate of univariate mean reversion. I tie the strong negative correlation between return and d p shocks to an essentially zero correlation between expected return and dividend growth shocks. How is it possible that variables such as the dividend price ratio forecast returns strongly, but there seems to be little evidence for mean reversion in stock returns? To answer this question, we have to connect the d p regressions and the mean reversion statistics. Forecastability from variables such as the dividend price ratios is related to, but does not necessarily imply, mean reversion. Campbell 1991 emphasizes this point. Mean reversion, is about the univariate properties of thereturnseries,forecastsofrt j basedon rt,rt 1,rt 2,... .Predictabilityis aboutthemultivariateproperties,forecastsofrt j basedon xt,xt 1,xt 2,... aswellas rt,rt 1,rt 2,... .Variables xt j canforecastrt 1,while rt j failto forecast rt 1 . As a simple example, suppose that returns are i.i.d., but you get to see tomorrow s newspaper. You forecast returns very well with xt rt 1, but lagged returns do not forecast returns at all. To examine the relationship between d p forecasts and mean reversion, continue with the VAR representation built up from a slowly moving",
        "416 20. Expected Returns in the Time Series and Cross Section expected return and unforecastable dividend growth, 20.14 20.20 . We want to find the univariate return process implied by this VAR: what would happen if you took infinite data from the system and ran a regression of returns on lagged returns? The answer, derived below, is of the form rt 1 Lt. 20.33 1 bL This is just the kind of process that can display slow mean reversion or momentum. The moving average representation is rt t  b t 1 b  b t 2 b2  b t 3 b3  b t 4 . 20.34 Thus, if  b, a positive return shock sets off a long string of small negative returns, which cumulatively bring the value back towards where it started. If  b, a positive shock sets off a string of small positive returns, which add momentum to the original increase in value. Now, what value of  does our VAR predict? Is there a sensible structure of the VAR that generates substantial predictability but little mean reversion? The general formula, derived below, is that  solves 2q, 20.35 1 2 and hence, 1 b2 2 d 1 2 2 dp 2  b  d,dp  b2 d 2 dp 1 b  d,dp  q q2 1. If returns are not predictable in this system if   0 so  dp 0, then 20.35 specializes to b  b, so returns in 20.33 are not autocorrelated. Sensibly enough, no predictability implies no mean reversion. Case 2: Constant Dividend Growth Next, suppose that dividend growth is constant;  d 0 and variation in expected returns is the only reason that ex post returns vary at all. In this Case 1: No Predictability 1 2 1 b2 .",
        "20.1. Time Series Predictability 417 case, 20.35 specializes quickly to 1 2 1 2 , and thus  . These parameters imply a substantial amount of mean reversion.  b in 20.34 is then 0.96 0.90 0.06, so that each year j after a shock returns come back by 6 bj percent of the original shock. The cumulative impact is that value ends up at 1  1 b 1 0.96 1 0.9 0.4 or only 40 of the original shock. Case 3: Dividend Growth Uncorrelated With Expected Return Shocks Pure variation in expected returns is of course not realistic. Dividends do vary. If we add dividend growth uncorrelated with expected return shocks  dp , d 0, 20.35 specializes to 1 2 1 b2 1 2  1  2q; 20.36  b b2 d  . b2 d 2 dp In this case, b  . There will be some mean reversion in returns this model cannot generate  b. However, the mean reversion in returns will be lower than with constant dividend growth, because dividend growth obscures the information in ex post returns about time varying expected returns. How much lower depends on the parameters. Using the parameters 20.24 , I find that 20.36 implies  q q2 1 0.928. Our baseline VAR with no correlation between dividend growth and expected return shocks thus generates a univariate return process that is slightly on the mean reversion edge of uncorrelated. The long run response to a shock is 1  1 0.928 0.72. 1 b 1 0.9 This is a lot less mean reversion than 0.4, but still somewhat more mean reversion than we see in direct estimates such as Tables 20.5 20.7. This case is an important baseline worth stressing. If expected returns are positively correlated, realized returns are negatively autocorrelated. If unchanged expected dividends are discounted at a higher rate, today s price falls. You",
        "418 20. Expected Returns in the Time Series and Cross Section can see this most easily by just looking at the return or its linearization, 20.22 , rt 1 dt 1  dt 1 pt 1 dt pt . 20.37 The d p ratio is proportional to expected returns. Thus, the second term on the right hand side implies that a positive shock to expected returns, uncor related with dividend growth, lowers actual returns. A little more deeply, look at the return innovation identity 20.11 , r E r E E j d jr . tt 1ttt 1 t j t j j 0 j 1 Ifexpectedreturns Et Et 1 about current or future dividends, then rt Et 1rt decreases. This is the point to remark on a curious feature of the return dividend price VAR: the negative correlation between ex post return shocks and dividend price ratio shocks. All the estimates were around 0.7. At first glance such a strong correlation between VAR residuals seems strange. At second glance, it is expected. From 20.37 you can see that a positive innovation to the dividend price ratio will correspond to a negative return innovation, unless a striking dividend correlation gets in the way. More deeply, you can see the point in 20.38 . Quantitatively, from 20.18 , the return shock is related to the dividend growth shock and the expected return shock by j 1 jrt j increase,withnoconcurrentnews  1 b Thus, a zero correlation between the underlying dividend growth and expected return shocks,  d ,  0, implies a negative covariance between return shocks and expected return shocks, r d  d dp. 20.38 2  . the parameters  dp 12.5,  r 15, we obtain  1 b  r, The correlation is a perfect 1 if there are no dividend growth shocks. At  r ,   r , dp    1 b     dp   0.96 12.5 15 0.8. The slight 0.1 positive correlation between dividend growth and expected return shocks in 20.26 results or, actually, results from a slightly lower 0.7 specification for the correlation of return and d p shocks.",
        "20.1. Time Series Predictability 419 The strong negative correlation between return shocks and expected return shocks, expected from a low correlation between dividend growth shocks and expected return shocks, is crucial to the finding that returns are not particularly correlated despite predictability. Consider what would hap pen if the correlation  r , dp  r ,  were zero. The expected return xt is slow moving. If it is high now, it has been high for a while, and there has likely been a series of good past returns. But it also will remain high for a while, leading to a period of high future returns. This is momentum, positive return autocorrelation, the opposite of mean reversion. Case 4: Dividend Growth Shocks Positively Correlated With Expected Return Shocks As we have seen, the VAR with no correlation between expected return and dividend growth shocks cannot deliver uncorrelated returns or positive momentum correlation patterns. At best, volatile dividend growth can obscure an underlying negative correlation pattern. However, looking at 20.37 or 20.38 , you can see that adding dividend growth shocks posi tively correlated with expected return shocks could give us uncorrelated or positively correlated returns. The estimates in Table 20.4 and 20.24 implied a slight positive correla tionofdividendgrowthandexpectedreturnshocks,d 0.14in 20.26 . If we use that estimate in 20.35 , we recover an estimate  0.923, 1  1 b 0.77. This  is quite close to b 0.9, and the small mean reversion is more closely consistent with the direct estimates in Tables 20.5 20.7. Recall that point estimates as in Table 20.1 showed that a high d p ratio forecast slightly higher dividends the wrong sign. This point estimate means that shocks to the d p ratio and expected returns are positively corre lated with shocks to expected dividend growth. If you generalize the VAR to allow such shocks, along with a richer specification allowing additional lags and variables, you find that VARs give point estimates with slight but very small mean reversion. See Cochrane 1994a for a plot. The estimated univariate process has slight mean reversion, with an impulse response end ing up at about 0.8 of its starting value, and no different from the direct estimate. Can we generate unforecastable returns in this system? To do so, we have to increase further the correlation between expected return shocks and dividend growth. Equating 20.35 to 1 b 2 b and solving for  d , dp , we obtain  d,dp 1 b  b  dp 1 b 2  b  d 0.51.",
        "420 20. Expected Returns in the Time Series and Cross Section This is possible, but not likely. Any positive correlation between divi dend growth and expected return shocks strikes me as suspect. If anything, I would expect that since expected returns rise in bad times when risk or risk aversion increases, we should see a positive shock to expected returns associated with a negative shock to current or future dividend growth. Similarly, if we are going to allow dividend price ratios to fore cast dividend growth, a high dividend price ratio should forecast lower dividends. Tying together all these thoughts, I think it is reasonable to impose zero dividend forecastability and zero correlation between dividend growth and expected return shocks. This specification means that returns are really less forecastable than they seem in some samples. As we have seen, b 0.9 and no dividend forecastability means that the coefficient of return on D P is really about 3.4 rather than 5 or 6. This specification means that expected returns really account for 100 rather than 130 of the price dividend variance. However, it also means that univariate mean reversion is slightly stronger than it seems in our sample. This section started with the possibility that the implied mean reversion from a multivariate system could be a lot larger than that revealed by direct estimates. Instead, we end up by reconciling strong predictability and slight mean reversion. How to Find the Univariate Return Representation This section ties up one technical loose end how to derive equation 20.33 . To find the implied univariate representation, we have to start with the VAR and find a representation rt 1 a L t 20.39 in which the a L is invertible. The Wold decomposition theorem tells us that there is a unique moving invertible moving average representation in whichthet aretheone step aheadforecasterrorshocks,i.e.,theerrorsin a regression model a L rt 1 t 1. Thus, if you find any invertible moving average representation, you know you have the right one. We cannot do this by simply manipulating the systems starting with 20.14 , because they are expressed in terms of multivariate shocks, errors in regressions that include x. There are three fundamental representations of a time series: its Wold moving average representation, its autocorrelation function, and its spectral density. To find the univariate representation 20.39 , you either calculate the autocorrelations E rt rt j from 20.14 and then try to recognize what process has that autocorrelation pattern, or you calculate the spectral density and try to recognize what process has that spectral density.",
        "422 20. Expected Returns in the Time Series and Cross Section numerically inverse Fourier transforming the spectral density of returns. To find the univariate, invertible moving average representation from the spectral density, you have to factor the spectral density Srr z a z a z 1 , where a z is a polynomial with roots outside the unit circle, a z 1 1z 1 2z i 1. Then, since a L is invertible, rt a L t  2 1 is the univariate representation of the return process. The autocorrelations and spectral densities are directly revealing: a string of small negative autocorrelations or a dip in the spectral density near frequency zero correspond to mean reversion; positive autocorrelations or a spectral density higher at frequency zero than elsewhere corresponds to momentum. Multivariate Mean Reversion We are left with a troubling set of facts: high price dividend ratios strongly forecast low returns, yet high past returns do not seem to forecast low subsequent returns. Surely, there must be some sense in which high prices forecast lower subsequent returns? The resolution must involve dividends or earnings, book value, or a similar divisor for prices . A price rise with no change in dividends results in lower subsequent returns. A price rise that comes with a dividend rise does not result in lower subsequent returns. A high return combines dividend news and price dividend news, and so obscures the lower expected return message. In a more time series language, instead of looking at the response to a univariate return shock, a return that was unanticipated based on lagged returns, let us look at the responses to multivariate shocks, a return that was unanticipated based on lagged returns and dividends. This is easy to do in our simple VAR. We can simulate 20.17 20.20 forward and trace the responses to a dividend growth shock and an expected return d p ratio shock. Figures 20.4 and 20.5 present the results of this calculation. Cochrane 1994a presents a corresponding calculation using an unrestricted VAR, and the results are very similar. Start with Figure 20.4. The negative expected return shock raises prices and the p d ratio immediately. We can identify such a shock in the data as a return shock with no contemporaneous movement in dividends. The p d ratio then reverts to its mean. Dividends are not forecastable, so they show  I calculate the responses to multivariate rather than univariate shocks. In a multivariate system you can isolate expected return shocks and divi dend growth shocks. The price response to expected return shocks is entirely stationary.",
        "424 20. Expected Returns in the Time Series and Cross Section no immediate or eventual response to the expected return shock. Prices show a long and complete reversion back to the level of dividends. This shock looks a lot like a negative yield shock to bonds: such a shock raises prices now so that bonds end up at the same maturity value despite a smaller expected return. The cumulative return mean reverts even more than prices. For given prices, dividends are now smaller smaller d p so returns deviate from their mean by more than price growth. The cumulative return ends up below its previously expected value. Compare this value response to the univariate value response, which we calculated above, and ends up at about 0.8 of its initial response. The dividend shock shown in Figure 20.5 raises prices and cumulative returns immediately and proportionally to dividends, so the price dividend ratio does not change. Expected returns or the discount rate, reflected in any slope of the value line, do not change. If the world were i.i.d., this is the only kind of shock we would see, and dividend price ratios would always be constant. Figures 20.4 and 20.5 plot the responses to typical, one standard deviation shocks. Thus you can see that actual returns are typically about half dividend shocks and half expected return shocks. That is why returns alone are a poor indicator of expected returns. In sum, at last we can see some rather dramatic mean reversion. Good past returns by themselves are not a reliable signal of lower subsequent returns, because they contain substantial dividend growth noise. Good returns that do not include good dividends isolate an expected return shock. This does signal low subsequent returns. It sets off a completely transitory variation in prices. Cointegration and Short vs. Long Run Volatility You might think that the facts about predictability depend on the exact structure of the VAR, including parameter estimates. In fact, most of what we have learned about predictability and mean reversion comes down to a few facts: the dividend price ratio, returns, and dividend growth are all If d p, p, and d are stationary, then the long run variance of d and p must be the same, long run movements in d and p must be perfectly correlated, and d and p must end up in the same place after any shock. Thus, the patterns of predictability, volatility, and univariate, and multivariate mean reversion really all just stem from these facts, the persistence of d p and the near unforecastability of d.",
        "20.1. Time Series Predictability 425 stationary; dividend growth is not or is at best weakly forecastable, and dividend growth varies less than returns. These facts imply that the dividend and price responses to each shock are eventually equal in Figures 20.4 and 20.5. If d p, p, and d are stationary, then d and p must end up in the same place following a shock. The responses of a stationary variable d p must die out. If dividends are not forecastable, then it must be the case that prices do all the adjustment following a price shock that does not affect dividends. Stationary d p, p, and d also implies that the variance of long horizon p must equal the variance of long horizon d: lim 1var pt k pt lim 1var dt k dt , 20.41 k k k k and the correlation of long run price and dividend growth must approach one. These facts follow from the fact that the variance ratio of a stationary variable must approach zero, and d p is stationary. Intuitively, long run price growth cannot be more volatile than long run dividend growth, or the long run p d ratio would not be stationary. Now, if dividend growth is not forecastable, its long run volatility is the same as its short run volatility its variance ratio is one. Short run price growth is more volatile than short run dividend growth, so we conclude that prices must be mean reverting; their variance ratio must be below one. Quantitatively, this observation supports the magnitude of univariate mean reversion that we have found so far. Dividend growth has a short run, and thus long run, standard deviation of about 10 per year, while returns and prices have a standard deviation of about 15 per year. Thus, prices must have a long run variance ratio of about 2 3 2, or a long run response to univariate shocks of 2 3. The work of Lettau and Ludvigson 2001b suggests that we may get much more dramatic implications by including consumption data. The ratio of stock market values to consumption should also be stationary; if wealth were to explode people would surely consume more and vice versa. The ratio of dividends to aggregate consumption should also be stationary. Consump tion growth seems independent at all horizons, and consumption growth is very stable, with roughly 1 annual standard deviation. For example, Lettau and Ludvigson 2001b find that none of the variables that forecast returns in Table 20.2 including d p and a consumption to wealth ratio forecast consumption growth at any horizon. These facts suggest that aggregate dividends are forecastable, by the consumption dividend ratio, and strongly so the long run volatility of aggregate dividend growth must be the 1 volatility of consumption growth, not the 10 short run volatility of dividend growth.",
        "426 20. Expected Returns in the Time Series and Cross Section These facts also suggest that almost all of the 15 or more variation in annual stock market wealth must be transitory the long run volatility of stock market value must be no more than the 1 consumption growth volatility! However, total market value is not the same thing as price, price is not the same thing as cumulated return, and aggregate dividends are not the same thing as the dividend concept we have used so far dividends paid to a dollar investment with dividends consumed , or dividends paid to a dollar investment with dividends reinvested. Lettau and Ludvigson show that the consumption wealth ratio does forecast returns, but no one has yet worked out the mean reversion implications of this fact. My statements about the implications of stationary d p, d, p, r are developed in detail in Cochrane 1994b . They are special cases of the representation theorems for cointegrated variables developed by Engle and Granger 1987 . A regression of a difference like p on a ratio like p d is called the error correction representation of a cointegrated system. Error correction regressions have subtly and dramatically changed almost all empirical work in finance and macroeconomics. The vast majority of the successful return forecasting regressions in this section, both time series and cross section, are error correction regressions of one sort or another. Corporate finance is being redone with regressions of growth rates on ratios, as is macroeconomic forecasting. For example, the consumption GDP ratio is a powerful forecaster of GDP growth. Bonds The venerable expectations model of the term structure specifies that long term bond yields are equal to the average of expected future short term bond yields. As with the CAPM and random walk, the expectations model was the workhorse of empirical finance for a generation. And as with those other views, a new round of research has significantly modified the traditional view. Table 20.8 calculates the average return on bonds of different maturi ties. The expectations hypothesis seems to do pretty well. Average holding period returns do not seem very different across bond maturities, despite The expectations model of the term structure works well on average and for horizons of four years or greater. At the one year horizon, however, a forward rate one percentage point higher than the spot rate seems entirely to indicate a one percentage point higher expected excess return rather than a one percentage point rise in future interest rates.",
        "20.3. Summary and Interpretation 451 at least plausible, and the effects have been expected by theorists for a generation. To make this point come to life, think concretely about what you have to do to take advantage of the value or predictability strategies. You have to buy stocks or long term bonds at the bottom, when stock prices are low after a long and depressing bear market; in the bottom of a recession or financial panic; a time when long term bond prices and corporate bond prices are unusually low. This is a time when few people have the guts the risk tolerance or the wallet to buy risky stocks or risky long term bonds. Looking across stocks rather than over time, you have to invest in value companies, dogs by any standards. These are companies with years of poor past returns, years of poor sales, companies on the edge of bankruptcy, far off of any list of popular stocks to buy. Then, you have to sell stocks and long term bonds in good times, when stock prices are high relative to dividends, earnings, and other multiples, when the yield curve is flat or inverted so that long term bond prices are high. You have to sell the popular growth stocks with good past returns, good sales, and earnings growth. I am going on a bit here to counter the widespread impression, best crystallized by Shiller 2000 that high price earnings ratios must signal irrational exuberance. Perhaps, but is it just a coincidence that this exu berance comes at the top of an unprecedented economic expansion, a time when the average investor is surely feeling less risk averse than ever, and willing to hold stocks despite historically low risk premia? I do not know the answer, but the rational explanation is surely not totally impossible! Is it just a coincidence that we are finding premia just where a generation of theo rists said we ought to in recessions, credit crunches, bad labor markets, investment opportunity set variables, and so forth? This line of explanation for the foreign exchange puzzle is still a bit farther off, though there are recent attempts to make economic sense of the puzzle. See Engel s 1996 survey; Atkeson, Alvarez, and Kehoe 1999 is a recent example. At a verbal level, the strategy leads you to invest in countries with high interest rates. High interest rates are often a sign of monetary instability or other economic trouble, and thus may mean that the investments may be more exposed to the risks of global financial stress or a global recession than are investments in the bonds of countries with low interest rates, who are typically enjoying better times. Overall, the new view of finance amounts to a profound change. We have to get used to the fact that most returns and price variation come from variation in risk premia, not variation in expected cash flows, interest rates, etc. Most interesting variation in priced risk comes from nonmar ket factors. These are easy to say, but profoundly change our view of the world.",
        "454 20. Expected Returns in the Time Series and Cross Section Apply Et Et 1 to both sides, rt Et 1rt Et Et 1 dt  Et Et 1 pt dt . 20.44 Use the price dividend identity and iterate forward to obtain 20.11 . 4. Find the univariate representation and mean reversion statistics for prices implied by the simple VAR and the three dividend examples. 5. Find the univariate return representation from a general return fore casting VAR, rt 1 axt rt 1, xt 1 bxt xt 1. Find the correlation between return and x shocks necessary to generate uncorrelated returns. 6. Show that stationary xt yt , xt , yt imply that xt and yt must have the same limiting variance of kth differences as k , and that long run differences must become perfectly correlated. Start by showing that the long run variance limk var xt k xt k for any stationary variable must be zero. Apply that fact to xt yt . 7. Compute the long horizon regression coefficients and R2 in the VAR 20.14 20.20 . Show that the R2 do indeed rise with horizon. Do coefficients and R 2 rise forever, or do they turn around at some point?"
    ]
}